{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb2d3d2",
   "metadata": {},
   "source": [
    "---\n",
    "title: Supervised methods - Classification\n",
    "teaching: 60\n",
    "exercises: 0\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions \n",
    "\n",
    "- What is classification, and how does it differ from regression?\n",
    "- How can we use Scikit-Learn to train and evaluate a classification model?\n",
    "- Why and how do we split data into training and testing sets for classification?\n",
    "- What are hyperparameters, and how do they affect model performance?\n",
    "- When should I standardize my data, and when is it safe to skip this step?\n",
    "  \n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Define supervised classification and explain how it differs from (supervised) regression.\n",
    "- Load and inspect the penguins dataset, selecting appropriate numeric features and labels.\n",
    "- Create train/test splits for classification and explain why stratification and shuffling matter.\n",
    "- Train and evaluate a decision tree classifier using Scikit-Learn.\n",
    "- Describe what a hyperparameter is and explore how changing `max_depth` affects a decision tree.\n",
    "- Visualize decision boundaries for a simple classifier to build intuition about how it separates classes.\n",
    "- Standardize features and train an SVM classifier, comparing its performance to the decision tree.\n",
    "- Explain which types of models benefit from feature scaling (standardization) and which generally do not.\n",
    "- Recognize signs of overfitting in classification models and relate them to model complexity and data quality.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "# Classification\n",
    "\n",
    "Classification is a supervised method to recognise and group data objects into a pre-determined categories. Where regression uses labelled observations to predict a continuous numerical value, classification predicts a discrete categorical fit to a class. Classification in ML leverages a wide range of algorithms to classify a set of data/datasets into their respective categories.\n",
    "\n",
    "In this episode we are going to introduce the concept of supervised classification by classifying penguin data into different species of penguins using Scikit-Learn.\n",
    "\n",
    "## The penguins dataset\n",
    "We're going to be using the penguins dataset of Allison Horst, published [here](https://github.com/allisonhorst/palmerpenguins), The dataset contains 344 size measurements for three penguin species (Chinstrap, Gentoo and Adélie) observed on three islands in the Palmer Archipelago, Antarctica.\n",
    "\n",
    "![*Artwork by @allison_horst*](https://raw.githubusercontent.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/main/episodes/fig/palmer_penguins.png)\n",
    "\n",
    "The physical attributes measured are flipper length, beak length, beak width, body mass, and sex.\n",
    "![*Artwork by @allison_horst*](https://raw.githubusercontent.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/main/episodes/fig/culmen_depth.png)\n",
    "\n",
    "In other words, the dataset contains 344 rows with 7 features i.e. 5 physical attributes, species and the island where the observations were made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "dataset = sns.load_dataset('penguins')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc5d29",
   "metadata": {},
   "source": [
    "Our aim is to develop a classification model that will predict the species of a penguin based upon measurements of those variables.\n",
    "\n",
    "As a rule of thumb for ML/DL modelling, it is best to start with a simple model and progressively add complexity in order to meet our desired classification performance.\n",
    "\n",
    "For this lesson we will limit our dataset to only numerical values such as bill_length, bill_depth, flipper_length, and body_mass while we attempt to classify species.\n",
    "\n",
    "The above table contains multiple categorical objects such as species. If we attempt to include the other categorical fields, island and sex, we might hinder classification performance due to the complexity of the data.\n",
    "\n",
    "### Preprocessing our data\n",
    "\n",
    "Lets do some pre-processing on our dataset and specify our `X` features and `y` labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data we need\n",
    "feature_names = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "dataset.dropna(subset=feature_names, inplace=True)\n",
    "\n",
    "class_names = dataset['species'].unique()\n",
    "\n",
    "X = dataset[feature_names]\n",
    "y = dataset['species']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b68cb",
   "metadata": {},
   "source": [
    "Having extracted our features `X` and labels `y`, we can now split the data using the `train_test_split` function.\n",
    "\n",
    "## Training-testing split\n",
    "When undertaking any machine learning project, it's important to be able to evaluate how well your model works. \n",
    "\n",
    "Rather than evaluating this manually we can instead set aside some of our training data, usually 20% of our training data, and use these as a testing dataset. We then train on the remaining 80% and use the testing dataset to evaluate the accuracy of our trained model. \n",
    "\n",
    "We lose a bit of training data in the process, But we can now easily evaluate the performance of our model. With more advanced test-train split techniques we can even recover this lost training data!\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: callout\n",
    "\n",
    "\n",
    "## Why do we do this?\n",
    "It's important to do this early, and to do all of your work with the training dataset - this avoids any risk of you introducing bias to the model based on your own manual observations of data in the testing set (afterall, we want the model to make the decisions about parameters!). This can also highlight when you are over-fitting on your training data.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "How we split the data into training and testing sets is also extremely important. We need to make sure that our training data is representitive of both our test data and actual data. \n",
    "\n",
    "For classification problems this means we should ensure that each class of interest is represented proportionately in both training and testing sets. For regression problems we should ensure that our training and test sets cover the range of feature values that we wish to predict.\n",
    "\n",
    "In the previous regression episode we created the penguin training data by taking the first 146 samples our the dataset. Unfortunately the penguin data is sorted by species and so our training data only considered one type of penguin and thus was not representitive of the actual data we tried to fit. We could have avoided this issue by randomly shuffling our penguin samples before splitting the data.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: callout\n",
    "\n",
    "\n",
    "## When not to shuffle your data\n",
    "Sometimes your data is dependant on it's ordering, such as time-series data where past values influence future predictions. Creating train-test splits for this can be tricky at first glance, but fortunately there are existing techniques to tackle this (often called stratification): See [Scikit-Learn](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators) for more information.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    " We specify the fraction of data to use as test data, and the function randomly shuffles our data prior to splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a180d",
   "metadata": {},
   "source": [
    "We'll use `X_train` and `y_train` to develop our model, and only look at `X_test` and `y_test` when it's time to evaluate its performance.\n",
    "\n",
    "### Visualising the data\n",
    "In order to better understand how a model might classify this data, we can first take a look at the data visually, to see what patterns we might identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86245d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig01 = sns.scatterplot(X_train, x=feature_names[0], y=feature_names[1], hue=dataset['species'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9962a",
   "metadata": {},
   "source": [
    "As there are four measurements for each penguin, we need quite a few plots to visualise all four dimensions against each other. Here is a handy Seaborn function to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79818371",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset, hue=\"species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369fc73b",
   "metadata": {},
   "source": [
    "We can see that penguins from each species form fairly distinct spatial clusters in these plots, so that you could draw lines between those clusters to delineate each species. This is effectively what many classification algorithms do. They use the training data to delineate the observation space, in this case the 4 measurement dimensions, into classes. When given a new observation, the model finds which of those class areas the new observation falls in to.\n",
    "\n",
    "\n",
    "## Classification using a decision tree\n",
    "We'll first apply a decision tree classifier to the data. Decisions trees are conceptually similar to flow diagrams (or more precisely for the biologists: dichotomous keys). They split the classification problem into a binary tree of comparisons, at each step comparing a measurement to a value, and moving left or right down the tree until a classification is reached.\n",
    "\n",
    "![Decision tree for classifying penguins](https://raw.githubusercontent.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/main/episodes/fig/decision_tree_example.png)\n",
    "\n",
    "\n",
    "Training and using a decision tree in Scikit-Learn is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc83aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ede1b",
   "metadata": {},
   "source": [
    "::::::::::::::::::::::::::::::::::::: callout\n",
    "\n",
    "\n",
    "## Hyper-parameters: parameters that tune a model\n",
    "'Max Depth' is an example of a *hyper-parameter* for the decision tree model. Where models use the parameters of an observation to predict a result, hyper-parameters are used to tune how a model works. Each model you encounter will have its own set of hyper-parameters, each of which affects model behaviour and performance in a different way. The process of adjusting hyper-parameters in order to improve model performance is called hyper-parameter tuning.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "We can conveniently check how our model did with the .score() function, which will make predictions and report what proportion of them were accurate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a73661",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_score = clf.score(X_test, y_test)\n",
    "print(clf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24130c17",
   "metadata": {},
   "source": [
    "Our model reports an accuracy of ~98% on the test data! We can also look at the decision tree that was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e8c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 10))\n",
    "plot_tree(clf, class_names=class_names, feature_names=feature_names, filled=True, ax=fig.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db20def",
   "metadata": {},
   "source": [
    "The first first question (`depth=1`) splits the training data into \"Adelie\" and \"Gentoo\" categories using the criteria `flipper_length_mm <= 206.5`, and the next two questions (`depth=2`) split the \"Adelie\" and \"Gentoo\" categories into \"Adelie & Chinstrap\" and \"Gentoo & Chinstrap\" predictions. \n",
    "\n",
    "\n",
    "\n",
    "<!-- We can see from this that there's some very tortuous logic being used to tease out every single observation in the training set. For example, the single purple Gentoo node at the bottom of the tree. If we truncated that branch to the second level (Chinstrap), we'd have a little inaccuracy, a total of 9 non-Chinstraps in with 48 Chinstraps, but a less convoluted model.\n",
    "\n",
    "The tortuous logic, such as the bottom purple Gentoo node, is a clear indication that this model has been over-fitted. It has developed a very complex delineation of the classification space in order to match every single observation, which will likely lead to poor results for new observations.\n",
    "\n",
    "We can see that rather than clean lines between species, the decision tree produces orthogonal regions as each decision only considers a single parameter. Again, we can see that the model is over-fitting as the decision space is far more complex than needed, with regions that only select a single point. -->\n",
    "\n",
    "### Visualising the classification space\n",
    "We can visualise the classification space (decision tree boundaries) to get a more intuitive feel for what it is doing.Note that our 2D plot can only show two parameters at a time, so we will quickly visualise by training a new model on only 2 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be441552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "f1 = feature_names[0]\n",
    "f2 = feature_names[3]\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "clf.fit(X_train[[f1, f2]], y_train)\n",
    "\n",
    "d = DecisionBoundaryDisplay.from_estimator(clf, X_train[[f1, f2]])\n",
    "\n",
    "sns.scatterplot(X_train, x=f1, y=f2, hue=y_train, palette=\"husl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a21854c",
   "metadata": {},
   "source": [
    "## Tuning the `max_depth` hyperparameter\n",
    "\n",
    "Our decision tree using a `max_depth=2` is fairly simple and there are still some incorrect predictions in our final classifications. Let's try varying the `max_depth` hyperparameter to see if we can improve our model predictions.\n",
    "\n",
    "<!-- We can reduce the over-fitting of our decision tree model by limiting its depth, forcing it to use less decisions to produce a classification, and resulting in a simpler decision space. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "max_depths = [1, 2, 3, 4, 5]\n",
    "\n",
    "accuracy = []\n",
    "for i, d in enumerate(max_depths):\n",
    "    clf = DecisionTreeClassifier(max_depth=d)\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "\n",
    "    accuracy.append((d, acc))\n",
    "\n",
    "acc_df = pd.DataFrame(accuracy, columns=['depth', 'accuracy'])\n",
    "\n",
    "sns.lineplot(acc_df, x='depth', y='accuracy')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67bfbb7",
   "metadata": {},
   "source": [
    "Here we can see that a `max_depth=2` performs slightly better on the test data than those with `max_depth > 2`. This can seem counter intuitive, as surely more questions should be able to better split up our categories and thus give better predictions?\n",
    "\n",
    "Let's reuse our fitting and plotting codes from above to inspect a decision tree that has `max_depth=5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb364a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "plot_tree(clf, class_names=class_names, feature_names=feature_names, filled=True, ax=fig.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0818a",
   "metadata": {},
   "source": [
    "It looks like our decision tree has split up the training data into the correct penguin categories and more accurately than the `max_depth=2` model did, however it used some very specific questions to split up the penguins into the correct categories. Let's try visualising the classification space for a more intuitive understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f61d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = feature_names[0]\n",
    "f2 = feature_names[3]\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=5)\n",
    "clf.fit(X_train[[f1, f2]], y_train)\n",
    "\n",
    "d = DecisionBoundaryDisplay.from_estimator(clf, X_train[[f1, f2]])\n",
    "\n",
    "sns.scatterplot(X_train, x=f1, y=f2, hue=y_train, palette='husl')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4fedb4",
   "metadata": {},
   "source": [
    "Earlier we saw that the `max_depth=2` model split the data into 3 simple bounding boxes, whereas for `max_depth=5` we see the model has created some very specific classification boundaries to correctly classify every point in the training data.\n",
    "\n",
    "This is a classic case of over-fitting - our model has produced extremely specific parameters that work for the training data but are not representitive of our test data. Sometimes simplicity is better!\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: challenge\n",
    "\n",
    "## Exercise: Adding noise to the training data\n",
    "\n",
    "We observed that this data doesn't seem prone to overfitting effects. Why might this be? There are at least two factors contributing to these results:\n",
    "\n",
    "1. We only have 4 predictors. With so few predictors, there are only so many unique tree structures that can be tested/formed. This makes overfitting less likely.  \n",
    "2. Our data is sourced from a Python library, and has been cleaned/vetted. Real-world data typically has more noise.\n",
    "\n",
    "Let's try adding a small amount of noise to the data using the code below. How does this impact the ideal setting for the tree depth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) LOAD DATA (if not loaded already)\n",
    "import seaborn as sns\n",
    "dataset = sns.load_dataset(\"penguins\")\n",
    "dataset.head()\n",
    "\n",
    "# 2) Extract the data we need and drop NaNs (if not done already)\n",
    "feature_names = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "dataset.dropna(subset=feature_names, inplace=True)\n",
    "class_names = dataset[\"species\"].unique()\n",
    "X = dataset[feature_names]\n",
    "y = dataset[\"species\"]\n",
    "\n",
    "# 3) ADD RANDOM NOISE TO X\n",
    "import numpy as np\n",
    "\n",
    "stds = X.std(axis=0).to_numpy()\n",
    "\n",
    "# Generate noise and scale it\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "noise = np.random.normal(0, 1, X.shape)  # sample numbers from normal distribution\n",
    "scaled_noise = noise * stds  # noise up to 1 standard deviation\n",
    "X_noisy = X + scaled_noise\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig01 = sns.scatterplot(X, x=feature_names[0], y=feature_names[1], hue=dataset[\"species\"])\n",
    "plt.show()\n",
    "fig02 = sns.scatterplot(X_noisy, x=feature_names[0], y=feature_names[1], hue=dataset[\"species\"])\n",
    "plt.show()\n",
    "\n",
    "# 4) TRAIN/TEST SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=0, stratify=y\n",
    "# )\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_noisy, y, test_size=0.2, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "# 5) HYPERPARAMETER TUNING\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "\n",
    "max_depths = list(range(1, 200))\n",
    "accuracy = []\n",
    "for d in max_depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=d)\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    accuracy.append((d, acc))\n",
    "\n",
    "acc_df = pd.DataFrame(accuracy, columns=[\"depth\", \"accuracy\"])\n",
    "\n",
    "sns.lineplot(acc_df, x=\"depth\", y=\"accuracy\")\n",
    "plt.xlabel(\"Tree depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a60ae1",
   "metadata": {},
   "source": [
    ":::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "## Classification using support vector machines\n",
    "Next, we'll look at another commonly used classification algorithm, and see how it compares. Support Vector Machines (SVM) work in a way that is conceptually similar to your own intuition when first looking at the data. They devise a set of hyperplanes that delineate the parameter space, such that each region contains ideally only observations from one class, and the boundaries fall between classes. One of the core strengths of Support Vector Machines (SVMs) is their ability to handle non-linear relationships between features by transforming the data into a higher-dimensional space. This transformation allows SVMs to find a linear boundary/hyperplane in this new space, which corresponds to a non-linear boundary in the original space.\n",
    "\n",
    "**What are the \"trainable parameters\" in an SVM?** \n",
    "For a linear SVM, the trainable parameters are:\n",
    "\n",
    "- Weight vector: A vector that defines the orientation of the hyperplane. Its size is equal to the number of features in X.\n",
    "- Bias: A scalar value that shifts the hyperplane to maximize the margin.\n",
    "  \n",
    "### When to Choose SVM Over Decision Tree\n",
    "\n",
    "1. **High-Dimensional Data**:\n",
    "   - **Why SVM**: SVMs excel in high-dimensional spaces because the kernel trick allows them to separate classes even in complex feature spaces without explicitly mapping the data.\n",
    "   - **Why Not Decision Tree**: Decision trees struggle with high-dimensional data as the number of potential splits grows exponentially, leading to overfitting or underperformance.\n",
    "\n",
    "2. **Accuracy over Interpretbaility**:\n",
    "   - **Why SVM**: SVMs are often considered black-box models, focusing on accuracy rather than interpretability.\n",
    "   - **Why Not Decision Tree**: Decision trees are typically interpretable, making them better if you need to explain your model.\n",
    "\n",
    "\n",
    "### Standardizing data\n",
    "Unlike decision trees, SVMs require an additional pre-processing step for our data. We need to standardize or \"z-score\" it. Our raw data has parameters with different magnitudes such as bill length measured in 10's of mm's, whereas body mass is measured in 1000's of grams. If we trained an SVM directly on this data, it would only consider the parameter with the greatest variance (body mass). \n",
    "\n",
    "Standarizing maps each parameter to a new range so that it has a mean of 0 and a standard deviation of 1. This places all features on the same playing field, and allows SVM to reveal the most accurate decision boundaries.\n",
    "\n",
    "#### When to Standardize Your Data: A Broader Overview\n",
    "\n",
    "Standardization is an essential preprocessing step for many machine learning models, particularly those that rely on **distance-based calculations** to make predictions or extract features. These models are sensitive to the scale of the input features because their mathematical foundations involve distances, magnitudes, or directions in the feature space. Without standardization, features with larger ranges can dominate the calculations, leading to suboptimal results. However, not all models require standardization; some, like decision trees, operate on thresholds and are unaffected by feature scaling. Here's a breakdown of when to standardize, explicitly explaining the role of distance-based calculations in each case.\n",
    "  \n",
    "##### When to Standardize: Models That Use Distance-Based Calculations\n",
    "\n",
    "1. **Support Vector Machines (SVMs)**: SVMs calculate the distance of data points to a hyperplane and aim to maximize the margin (the distance between the hyperplane and the nearest points, called support vectors).\n",
    "\n",
    "2. **k-Nearest Neighbors (k-NN)**: k-NN determines class or value predictions based on the distance between a query point and its k-nearest neighbors in the feature space.\n",
    "\n",
    "3. **Logistic Regression with Regularization**: Regularization terms (e.g., L1 or L2 penalties) involve calculating the magnitude (distance) of the parameter vector to reduce overfitting and encourage simplicity.\n",
    "\n",
    "4. **Principal Component Analysis (PCA)**: PCA identifies principal components by calculating the Euclidean distance from data points to the axes representing the highest variance directions in the feature space.\n",
    "\n",
    "5. **Neural Networks (NNs)**: Neural networks rely on gradient-based optimization to learn weights. If input features have vastly different scales, gradients can become unstable, slowing down training or causing convergence issues. Standardizing or normalizing (scaling from 0 to 1) features ensures that all inputs contribute equally to the optimization process.\n",
    "\n",
    "6. **Linear Regression (for Interpreting Many Coefficients)**: While linear regression itself doesn’t rely on distance-based calculations, standardization is crucial when interpreting coefficients because it ensures that all features are on the same scale, making their relative importance directly comparable. Without standardization, coefficients in linear regression reflect the relationship between the dependent variable and a feature in the units of that feature, making it difficult to compare features with different scales (e.g., height in centimeters vs. weight in kilograms).\n",
    "\n",
    "##### When to Skip Standardization: Models That Don’t Use Distance-Based Calculations\n",
    "\n",
    "1. **Decision Trees**: Decision trees split data based on thresholds, independent of feature scales, without relying on any distance-based calculations.\n",
    "\n",
    "2. **Random Forests**: Random forests aggregate decisions from multiple trees, which also use thresholds rather than distance-based metrics.\n",
    "\n",
    "3. **Gradient Boosted Trees**: Gradient boosting optimizes decision trees sequentially, focusing on residuals and splits rather than distance measures.\n",
    "\n",
    "By understanding whether a model relies on distance-based calculations (or benefits from standardized features for interpretability), you can decide whether standardization is necessary, ensuring that your preprocessing pipeline is well-suited to the algorithm you’re using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea26ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "scalar = preprocessing.StandardScaler()\n",
    "scalar.fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(scalar.transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scalar.transform(X_test), columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a1169",
   "metadata": {},
   "source": [
    "Note that we fit the scalar to our training data - we then use this same pre-trained scalar to transform our testing data.\n",
    "\n",
    "With this scaled data, training the models works exactly the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85380ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "SVM = svm.SVC(kernel='poly', degree=3, C=1.5)\n",
    "SVM.fit(X_train_scaled, y_train)\n",
    "\n",
    "svm_score = SVM.score(X_test_scaled, y_test)\n",
    "print(\"Decision tree score is \", clf_score)\n",
    "print(\"SVM score is \", svm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc6576",
   "metadata": {},
   "source": [
    "We can again visualise the decision space produced, also using only two parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = X_train_scaled[[feature_names[0], feature_names[1]]]\n",
    "\n",
    "SVM = svm.SVC(kernel='poly', degree=3, C=1.5)\n",
    "SVM.fit(x2, y_train)\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(SVM, x2) #, ax=ax\n",
    "sns.scatterplot(x2, x=feature_names[0], y=feature_names[1], hue=dataset['species'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbce978",
   "metadata": {},
   "source": [
    "**SVM parameters**: \n",
    "\n",
    "- **`kernel`**: Specifies how the SVM transforms the data to find patterns; start with `'rbf'` for most cases, `'linear'` for high-dimensional data, or `'poly'` for polynomial relationships.\n",
    "  - Linear Kernel: Directly computes the dot product between input vectors; best for linearly separable data and high-dimensional spaces, offering simplicity and efficiency.\n",
    "  - Poly Kernel: Computes polynomial relationships of features, allowing for flexible decision boundaries; ideal for data with polynomial patterns.\n",
    "  - RBF (Radial Basis Function) Kernel: Uses a Gaussian function to create highly flexible decision boundaries; effective for non-linear, complex data. \n",
    "- **`degree`**: Sets the complexity of the polynomial kernel; use `degree=3` for cubic relationships, and avoid going higher unless you have lots of data.\n",
    "- **`C`**: Balances smoothness of the decision boundary and misclassifications; start with `C=1`, increase for tighter boundaries, decrease to prevent overfitting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "While this SVM model performs slightly worse than our decision tree (95.6% vs. 98.5%), it's likely that the non-linear boundaries will perform better when exposed to more and more real data, as decision trees are prone to overfitting and requires complex linear models to reproduce simple non-linear boundaries. It's important to pick a model that is appropriate for your problem and data trends!\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints \n",
    "\n",
    "- Classification requires labelled data (is supervised)\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
