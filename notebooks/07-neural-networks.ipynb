{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c50ff8",
   "metadata": {},
   "source": [
    "---\n",
    "title: Neural Networks\n",
    "teaching: 20\n",
    "exercises: 30\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions \n",
    "\n",
    "- What are Neural Networks?\n",
    "- How can we classify images using a neural network?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Understand the basic architecture of a perceptron.\n",
    "- Be able to create a perceptron to encode a simple function.\n",
    "- Understand that layers of perceptrons allow non-linear separable problems to be solved.\n",
    "- Train a multi-layer perceptron using Scikit-Learn.\n",
    "- Evaluate the accuracy of a multi-layer perceptron using real input data.\n",
    "- Understand that cross validation allows the entire data set to be used in the training process.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "# Neural networks\n",
    "\n",
    "Neural networks are a machine learning method inspired by how the human brain works. They are particularly good at pattern recognition and classification tasks, often using images as inputs. They are a well-established machine learning technique, having been around since the 1950s, but they've gone through several iterations to overcome limitations in previous generations. Using state-of-the-art neural networks is often referred to as 'deep learning'.\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "Perceptrons are the building blocks of neural networks. They are an artificial version of a single neuron in the brain. They typically have one or more inputs and a single output. Each input will be multiplied by a weight and the value of all the weighted inputs are then summed together. Finally, the summed value is put through an activation function which decides if the neuron \"fires\" a signal. In some cases, this activation function is simply a threshold step function which outputs zero below a certain input and one above it. Other designs of neurons use other activation functions, but typically they have an output between zero and one and are still step-like in their nature.\n",
    "\n",
    "![A diagram of a perceptron](https://raw.githubusercontent.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/main/episodes/fig/perceptron.svg)\n",
    "\n",
    "### Coding a perceptron\n",
    "\n",
    "Below is an example of a perceptron written as a Python function. The function takes three parameters: `Inputs` is a list of input values, `Weights` is a list of weight values and `Threshold` is the activation threshold.\n",
    "\n",
    "First we multiply each input by the corresponding weight. To do this quickly and concisely, we will use the numpy multiply function which can multiply each item in a list by a corresponding item in another list.\n",
    "\n",
    "We then take the sum of all the inputs multiplied by their weights. Finally, if this value is less than the activation threshold, we output zero, otherwise we output a one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506fdad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def perceptron(inputs, weights, threshold):\n",
    "\n",
    "    assert len(inputs) == len(weights)\n",
    "\n",
    "    # multiply the inputs and weights\n",
    "    values = np.multiply(inputs,weights)\n",
    "\n",
    "    # sum the results\n",
    "    total = sum(values)\n",
    "\n",
    "    # decide if we should activate the perceptron\n",
    "    if total < threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f7da6",
   "metadata": {},
   "source": [
    "### Computing with a perceptron\n",
    "\n",
    "A single perceptron can perform basic linear classification problems such as computing the logical AND, OR, and NOT functions.\n",
    "\n",
    "OR\n",
    "\n",
    "| Input 1 | Input 2 | Output |\n",
    "| --------|---------|--------|\n",
    "| 0       |0        |0       |\n",
    "| 0       |1        |1       |\n",
    "| 1       |0        |1       |\n",
    "| 1       |1        |1       |\n",
    "\n",
    "AND\n",
    "\n",
    "| Input 1 | Input 2 | Output |\n",
    "| --------|---------|--------|\n",
    "| 0       |0        |0       |\n",
    "| 0       |1        |0       |\n",
    "| 1       |0        |0       |\n",
    "| 1       |1        |1       |\n",
    "\n",
    "\n",
    "NOT\n",
    "\n",
    "| Input 1 |Output |\n",
    "| --------|--------|\n",
    "| 0       |1       |\n",
    "| 1       |0       |\n",
    "\n",
    "\n",
    "We can get a single perceptron to compute each of these functions\n",
    "\n",
    "OR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[0.0,0.0],[1.0,0.0],[0.0,1.0],[1.0,1.0]]\n",
    "for input in inputs:\n",
    "    print(input,perceptron(input, [0.5,0.5], 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c95a3",
   "metadata": {},
   "source": [
    "AND:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20937da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[0.0,0.0],[1.0,0.0],[0.0,1.0],[1.0,1.0]]\n",
    "for input in inputs:\n",
    "    print(input,perceptron(input, [0.5,0.5], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37341c4c",
   "metadata": {},
   "source": [
    "NOT:\n",
    "\n",
    "The NOT function only has a single input. To make it work in the perceptron we need to introduce a bias term which is always the same value. In this example it is the second input. It has a weight of 1.0 while the weight on the real input is -1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[0.0,1.0],[1.0,1.0]]\n",
    "for input in inputs:\n",
    "    print(input,perceptron(input, [-1.0,1.0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a4df37",
   "metadata": {},
   "source": [
    "A perceptron can be trained to compute any function which has linear separability. A simple training algorithm called the perceptron learning algorithm can be used to do this and Scikit-Learn has its own implementation of it. We are going to skip over the perceptron learning algorithm and move straight onto more powerful techniques. If you want to learn more about it see [this page](https://computing.dcu.ie/~humphrys/Notes/Neural/single.neural.html) from Dublin City University.\n",
    "\n",
    "\n",
    "\n",
    "### Perceptron limitations\n",
    "\n",
    "A single perceptron cannot solve any function that is not linearly separable, meaning that we need to be able to divide the classes of inputs and outputs with a straight line. A common example of this is the XOR function shown below:\n",
    "\n",
    "| Input 1 | Input 2 | Output |\n",
    "| --------|---------|--------|\n",
    "| 0       |0        |0       |\n",
    "| 0       |1        |1       |\n",
    "| 1       |0        |1       |\n",
    "| 1       |1        |0       |\n",
    "\n",
    "(Make a graph of this)\n",
    "\n",
    "This function outputs a zero when all its inputs are one or zero and its not possible to separate with a straight line. This is known as linear separability. When this limitation was discovered in the 1960s it effectively halted development of neural networks for over a decade in a period known as the \"AI Winter\".\n",
    "\n",
    "\n",
    "## Multi-layer perceptrons\n",
    "\n",
    "A single perceptron cannot be used to solve a non-linearly separable function. For that, we need to use multiple perceptrons and typically multiple layers of perceptrons. They are formed of networks of artificial neurons which each take one or more inputs and typically have a single output. The neurons are connected together in networks of 10s to 1000s of neurons. Typically, networks are connected in layers with an input layer, middle or hidden layer (or layers), and finally an output layer.\n",
    "\n",
    "![A multi-layer perceptron](https://raw.githubusercontent.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/main/episodes/fig/multilayer_perceptron.svg)\n",
    "\n",
    "### Training multi-layer perceptrons\n",
    "\n",
    "Multi-layer perceptrons need to be trained by showing them a set of training data and measuring the error between the network's predicted output and the true value. Training takes an iterative approach that improves the network a little each time a new training example is presented. There are a number of training algorithms available for a neural network today, but we are going to use one of the best established and well known, the backpropagation algorithm. This algorithm is called back propagation because it takes the error calculated between an output of the network and the true value and takes it back through the network to update the weights. If you want to read more about back propagation, please see [this chapter](http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf) from the book \"Neural Networks - A Systematic Introduction\".\n",
    "\n",
    "### Multi-layer perceptrons in Scikit-Learn\n",
    "\n",
    "We are going to build a multi-layer perceptron for recognising handwriting from images. Scikit-Learn includes some example handwriting data from the [MNIST data set](http://yann.lecun.com/exdb/mnist/), which is a dataset containing 70,000 images of hand-written digits. Each image is 28x28 pixels in size (784 pixels in total) and is represented in grayscale with values between zero for fully black and 255 for fully white. This means we will need 784 perceptrons in our input layer, each taking the input of one pixel and 10 perceptrons in our output layer to represent each digit we might classify. If trained correctly, only the perceptron in the output layer will \"fire\" to represent the contents of the image (but this is a massive oversimplification!).\n",
    "\n",
    "We can import this dataset from `sklearn.datasets` then load it into memory by calling the `fetch_openml` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fe167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as skl_data\n",
    "data, labels = skl_data.fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448724b",
   "metadata": {},
   "source": [
    "This creates two arrays of data, one called `data` which contains the image data and the other `labels` that contains the labels for those images which will tell us which digit is in the image. A common convention is to call the data `X` and the labels `y`.\n",
    "\n",
    "As neural networks typically want to work with data that ranges between 0 and 1.0 we need to normalise our data to this range. Python has a shortcut which lets us divide the entire data array by 255 and store the result, we can simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3925a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c704e",
   "metadata": {},
   "source": [
    "This is instead of writing a loop ourselves to divide every pixel by 255. Although the final result is the same and will take about the same amount of computation (possibly a little less, it might do some clever optimisations).\n",
    "\n",
    "Now we need to initialise a neural network. Scikit-Learn has an entire library for this (`sklearn.neural_network`) and the `MLPClassifier` class handles multi-layer perceptrons. This network takes a few parameters including the size of the hidden layer, the maximum number of training iterations we're going to allow, the exact algorithm to use, whether or not we'd like verbose output about what the training is doing, and the initial state of the random number generator.\n",
    "\n",
    "In scikit-learn's `MLPClassifier`, the `hidden_layer_sizes` parameter specifies the number and size of hidden layers in the neural network. For example, `hidden_layer_sizes=(50,)` creates a single hidden layer with 50 neurons, while `(100, 50)` creates two hidden layers with 100 and 50 neurons, respectively. Itâ€™s important to include the trailing comma for a single hidden layer (e.g., `(50,)`) because without it, `(50)` would be interpreted as an integer, not a tuple, and cause an error. The example, `MLPClassifier(hidden_layer_sizes=(50,), max_iter=50, verbose=1, random_state=1)`, builds a neural network with one hidden layer containing 50 neurons, runs for a maximum of 50 iterations, logs training progress, and ensures reproducibility with `random_state=1`.\n",
    "\n",
    "The max_iter parameter in MLPClassifier specifies the maximum number of iterations, not epochs. Since MLPClassifier uses stochastic gradient descent (or its variants), each iteration processes a small random subset of the data (a batch), and the full dataset may not be seen in a single iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29953fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neural_network as skl_nn\n",
    "mlp = skl_nn.MLPClassifier(hidden_layer_sizes=(50,), max_iter=50, verbose=1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc5db57",
   "metadata": {},
   "source": [
    "We now have a neural network but we have not trained it yet. Before training, we will split our dataset into two parts: a training set which we will use to train the classifier and a test set which we will use to see how well the training is working. By using different data for the two, we can avoid 'over-fitting', which is the creation of models which do not \"generalise\" or work with data other than their training data.\n",
    "\n",
    "Typically, the majority of the data will be used as training data (70-90%), to help avoid overfitting. Let us see how big our dataset is to decide how many samples we want to train with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e20586",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3410e63c",
   "metadata": {},
   "source": [
    "This tells us we have 70,000 rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b822f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "<bound method NDFrame.describe of        pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  pixel781  pixel782  pixel783  pixel784\n",
    "0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
    "1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
    "2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
    "3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
    "4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
    "...       ...     ...     ...     ...     ...     ...     ...     ...     ...      ...  ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...\n",
    "69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
    "69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
    "69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
    "69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
    "69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
    "\n",
    "[70000 rows x 784 columns]>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e563c",
   "metadata": {},
   "source": [
    "python\n",
    "{: .output}\n",
    "\n",
    "Let us take 90% of the data for training and 10% for testing, so we will use the first 63,000 samples in the dataset as the training data and the last 7,000 as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming `data` is your feature matrix and `labels` is your target vector\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.values,        # Features\n",
    "    labels.values,      # Labels\n",
    "    test_size=0.1,      # Reserve 10% of data for testing\n",
    "    random_state=42     # For reproducibility\n",
    ")\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5941171",
   "metadata": {},
   "source": [
    "Now lets train the network. This line will take about one minute to run. We do this by calling the `fit` function inside the `mlp` class instance. This needs two arguments: the data itself, and the labels showing what class each item should be classified to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6703a6",
   "metadata": {},
   "source": [
    "Finally, we will score the accuracy of our network against both the original training data and the test data. If the training had converged to the point where each iteration of training was not improving the accuracy, then the accuracy of the training data should be 1.0 (100%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbf8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score\", mlp.score(X_train, y_train))\n",
    "print(\"Testing set score\", mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55126f94",
   "metadata": {},
   "source": [
    "### Prediction using a multi-layer perceptron\n",
    "\n",
    "Now that we have trained a multi-layer perceptron, we can give it some input data and ask it to perform a prediction. In this case, our input data is a 28x28 pixel image, which can also be represented as a 784-element list of data. The output will be a number between 0 and 9 telling us which digit the network thinks we have supplied. The `predict` function in the `MLPClassifier` class can be used to make a prediction. Lets use the first digit from our test set as an example.\n",
    "\n",
    "Before we can pass it to the predictor, we need to extract one of the digits from the test set. We can use `iloc` on the dataframe to get hold of the first element in the test set. In order to present it to the predictor, we have to turn it into a numpy array which has the dimensions of 1x784 instead of 28x28. We can then call the `predict` function with this array as our parameter. This will return an array of predictions (as it could have been given multiple inputs), the first element of this will be the predicted digit. You may get a warning stating \"X does not have valid feature names\", this is because we didn't encode feature names into our X (digit images) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_digit = X_test[0].reshape(1,784) # current shape is (784,)\n",
    "test_digit_prediction = mlp.predict(test_digit)[0]\n",
    "print(\"Predicted value\",test_digit_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782da22",
   "metadata": {},
   "source": [
    "We can now verify if the prediction is correct by looking at the corresponding item in the `labels_test` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f6d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual value\",y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1624e97",
   "metadata": {},
   "source": [
    "This should be the same value which is being predicted.\n",
    "\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: challenge\n",
    "\n",
    "\n",
    "## Changing the learning parameters\n",
    "There are several parameters which control the training of the data. One of these is called the learning rate. Increasing this can reduce how many learning iterations we need. But if this is too large you can end up overshooting.\n",
    "Try tweaking this parameter by adding the parameter `learning_rate_init` with a default value of 0.001. Try increasing it to around 0.1.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: challenge\n",
    "\n",
    "\n",
    "## Using your own handwriting\n",
    "Create an image using Microsoft Paint, the GNU Image Manipulation Project (GIMP) or [jspaint](https://jspaint.app/). The image needs to be grayscale and 28 x 28 pixels.\n",
    "\n",
    "Try to draw a digit (0-9) in the image and save it into your code directory.\n",
    "\n",
    "The code below loads the image (called digit.png, change to whatever your file is called) using the OpenCV library. Some Anaconda installations need this installed either through the package manager or by running the command: `conda install -c conda-forge opencv ` from the anaconda terminal.\n",
    "\n",
    "OpenCV assumes that images are 3 channel red, green, blue and we have to convert to one channel grayscale with `cvtColor`.\n",
    "\n",
    "We also need to normalise the image by dividing each pixel by 255.\n",
    "\n",
    "To verify the image, we can plot it by using OpenCV's `imshow` function (we could also use Matplotlib's `matshow` function).\n",
    "\n",
    "To check what digit it is, we can pass it into `mlp.predict`, but we have to convert it from a 28x28 array to a one dimensional 784-byte long array with the `reshape` function.\n",
    "\n",
    "Did it correctly classify your hand(mouse) writing? Try a few images.\n",
    "If you have time try drawing images on a touch screen or taking a photo of something you have really written by hand. Remember that you will have to resize it to be 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89bddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "digit = cv2.imread(\"digit.png\")\n",
    "digit_gray = cv2.cvtColor(digit, cv2.COLOR_BGR2GRAY)\n",
    "digit_norm = digit_gray/255.0\n",
    "cv2.imshow(\"Normalised Digit\",digit_norm)\n",
    "print(\"Your digit is\",mlp.predict(digit_norm.reshape(1,784)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b22b3",
   "metadata": {},
   "source": [
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "### Confusion matrix\n",
    "\n",
    "We now know what percentage of images were correctly classified, but we don't know anything about the distribution of correct predictions across our different classes (the digits 0 to 9 in this case). A more powerful technique is known as a confusion matrix. Here we draw a grid with each class along both the x and y axis. The x axis is the actual number of items in each class and the y axis is the predicted number. In a perfect classifier, there will be a diagonal line of values across the grid moving from the top left to bottom right corresponding to the number in each class, and all other cells will be zero. If any cell outside of the diagonal is non-zero then it indicates a miss-classification. Scikit-Learn has a function called `confusion_matrix` in the `sklearn.metrics` class which can display a confusion matrix for us. It will need two inputs: arrays showing how many items were in each class for both the real data and the classifications. We already have the real data in the labels_test array, but we need to build it for the classifications by classifying each image (in the same order as the real data) and storing the result in another array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117195a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all test set predictions\n",
    "y_test_pred = mlp.predict(X_test)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb4ab10",
   "metadata": {},
   "source": [
    "The `ConfusionMatrixDisplay` class in the `sklearn.metrics` package can create a graphical representation of a confusion matrix with colour coding to highlight how many items are in each cell. This colour coding can be useful when working with very large numbers of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaff52a",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Previously we split the data into training and test sets. But what if the test set includes important features we want to train on that happen to be missing in the training set? We are throwing away part of our data to use it in the testing set.\n",
    "\n",
    "Cross-validation runs the training/testing multiple times but splits the data in a different way each time. This means all of the data gets used both for training and testing. We can use multiple iterations of training with different data in each set to eventually include the entire dataset.\n",
    "\n",
    "example list\n",
    "\n",
    "[1,2,3,4,5,6,7,8]\n",
    "\n",
    "train = 1,2,3,4,5,6\n",
    "test = 7,8\n",
    "\n",
    "train = 1,2,3,4,7,8\n",
    "test = 5,6\n",
    "\n",
    "train = 1,2,5,6,7,8\n",
    "test = 3,4\n",
    "\n",
    "train = 3,4,5,6,7,8\n",
    "test = 1,2\n",
    "\n",
    "(generate an image of this)\n",
    "\n",
    "### Cross-validation code example\n",
    "\n",
    "The `sklearn.model_selection` module provides support for doing k-fold cross validation in Scikit-Learn. It can automatically partition our data for cross validation.\n",
    "\n",
    "Import this and call it `skl_msel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as skl_msel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d1ba7",
   "metadata": {},
   "source": [
    "Now we can choose how many ways we would like to split our data (three or four are common choices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56891a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = skl_msel.KFold(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cfd7cd",
   "metadata": {},
   "source": [
    "Now we can loop through our data and test on each combination. The `kfold.split` function returns two variables and we will have our for loop work through both of them. The train variable will contain a list of which items (by index number) we are currently using to train and the test one will contain the list of which items we are going to test on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e583b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (train, test) in kfold.split(data):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a739c98",
   "metadata": {},
   "source": [
    "Now inside the loop, we can select the data with `data_train = data.iloc[train]` and `labels_train = labels.iloc[train]`. In some versions of Python/Pandas/Scikit-Learn, you might be able to use `data_train = data[train]` and `labels_train = labels[train]`. This is a useful Python shorthand which will use the list of indices from `train` to select which items from `data` and `labels` we use. We can repeat this process with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data.iloc[train]\n",
    "    labels_train = labels.iloc[train]\n",
    "\n",
    "    data_test = data.iloc[test]\n",
    "    labels_test = labels.iloc[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d5b3e",
   "metadata": {},
   "source": [
    "Finally, we need to train the classifier with the selected training data and then score it against the test data. The scores for each set of test data should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(data_train,labels_train)\n",
    "    print(\"Testing set score\", mlp.score(data_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b3398c",
   "metadata": {},
   "source": [
    "Once we have established that the cross validation was ok, we can go ahead and train using the entire dataset by doing `mlp.fit(data,labels)`.\n",
    "\n",
    "Here is the entire example program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ac171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as skl_data\n",
    "import sklearn.neural_network as skl_nn\n",
    "import sklearn.model_selection as skl_msel\n",
    "\n",
    "data, labels = skl_data.fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "data = data / 255.0\n",
    "\n",
    "mlp = skl_nn.MLPClassifier(hidden_layer_sizes=(50,), max_iter=50, random_state=1)\n",
    "\n",
    "kfold = skl_msel.KFold(4)\n",
    "\n",
    "for (train, test) in kfold.split(data):\n",
    "    data_train = data.iloc[train]\n",
    "    labels_train = labels.iloc[train]\n",
    "\n",
    "    data_test = data.iloc[test]\n",
    "    labels_test = labels.iloc[test]\n",
    "    mlp.fit(data_train,labels_train)\n",
    "    print(\"Training set score\", mlp.score(data_train, labels_train))\n",
    "    print(\"Testing set score\", mlp.score(data_test, labels_test))\n",
    "mlp.fit(data,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80216c00",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "\n",
    "Deep learning usually refers to newer neural network architectures which use a special type of network known as a 'convolutional network'. Typically, these have many layers and thousands of neurons. They are very good at tasks such as image recognition but take a long time to train and run. They are often used with GPUs (Graphical Processing Units) which are good at executing multiple operations simultaneously. It is very common to use cloud computing or high performance computing systems with multiple GPUs attached.\n",
    "\n",
    "Scikit-Learn is not really setup for deep learning. We will have to rely on other libraries. Common choices include Google's TensorFlow, Keras, (Py)Torch or Darknet. There is, however, an interface layer between sklearn and tensorflow called skflow. A short example of using this layer can be found at [https://www.kdnuggets.com/2016/02/scikit-flow-easy-deep-learning-tensorflow-scikit-learn.html](https://www.kdnuggets.com/2016/02/scikit-flow-easy-deep-learning-tensorflow-scikit-learn.html).\n",
    "\n",
    "### Cloud APIs\n",
    "\n",
    "Google, Microsoft, Amazon, and many other companys now have cloud based Application Programming Interfaces (APIs) where you can upload an image and have them return you the result. Most of these services rely on a large pre-trained (and often proprietary) neural network.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: challenge\n",
    "\n",
    "\n",
    "## Exercise: Try cloud image classification\n",
    "Take a photo with your phone camera or find an image online of a common daily scene.\n",
    "Upload it to Google's Vision AI at https://cloud.google.com/vision/\n",
    "How many objects has it correctly classified? How many did it incorrectly classify?\n",
    "Try the same image with Microsoft's Computer Vision API at https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/\n",
    "Does it do any better/worse than Google?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "{% include links.md %}\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints \n",
    "\n",
    "- Perceptrons are artificial neurons which build neural networks.\n",
    "- A perceptron takes multiple inputs, multiplies each by a weight value and sums the weighted inputs. It then applies an activation function to the sum.\n",
    "- A single perceptron can solve simple functions which are linearly separable.\n",
    "- Multiple perceptrons can be combined to form a neural network which can solve functions that aren't linearly separable.\n",
    "- We can train a whole neural network with the back propagation algorithm. Scikit-learn includes an implementation of this algorithm.\n",
    "- Training a neural network requires some training data to show the network examples of what to learn.\n",
    "- To validate our training we split the training data into a training set and a test set.\n",
    "- To ensure the whole dataset can be used in training and testing we can train multiple times with different subsets of the data acting as training/testing data. This is called cross validation.\n",
    "- Deep learning neural networks are a very powerful modern machine learning technique. Scikit-Learn does not support these but other libraries like Tensorflow do.\n",
    "- Several companies now offer cloud APIs where we can train neural networks on powerful computers.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
