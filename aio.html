<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Introduction to Machine Learning with Scikit-Learn: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png">
<link rel="manifest" href="favicons/incubator/site.webmanifest">
<link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Beginner-friendly introduction to machine learning in Python using scikit-learn." src="assets/images/incubator-logo.svg"><span class="badge text-bg-info">
          <abbr title="This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-circle" style="border-radius: 5px"></i>
              Beta
            </a>
            <span class="visually-hidden">This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Beginner-friendly introduction to machine learning in Python using scikit-learn." src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Introduction to Machine Learning with Scikit-Learn
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Introduction to Machine Learning with Scikit-Learn
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to Machine Learning with Scikit-Learn
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-regression.html">2. Supervised methods - Regression</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-classification.html">3. Supervised methods - Classification</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-ensemble-methods.html">4. Ensemble methods</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-clustering.html">5. Unsupervised methods - Clustering</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-dimensionality-reduction.html">6. Unsupervised methods - Dimensionality reduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-neural-networks.html">7. Neural Networks</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="08-ethics.html">8. Ethics and the Implications of Machine Learning</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush10">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading10">
        <a href="09-learn-more.html">9. Find out more</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-introduction"><p>Content from <a href="01-introduction.html">Introduction</a></p>
<hr>
<p>Last updated on 2025-11-07 |

        <a href="https://github.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/edit/main/episodes/01-introduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is machine learning?</li>
<li>What are some useful machine learning techniques?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Gain an overview of what machine learning is and the techniques
available.</li>
<li>Understand how machine learning, deep learning, and artificial
intelligence differ.</li>
<li>Be aware of some caveats when using machine mearning.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="what-is-machine-learning">What is machine learning?<a class="anchor" aria-label="anchor" href="#what-is-machine-learning"></a>
</h1>
<p>Machine learning is a set of techniques that enable computers to use
data to improve their performance in a given task. This is similar in
concept to how humans learn to make predictions based upon previous
experience and knowledge. Machine learning is “data-driven”, meaning
that it uses the underlying statistics of a set of data to achieve a
task.</p>
<p>Machine learning encompasses a wide range of tasks and activities,
but broadly speaking it can be used to: find trends in a dataset,
classify data into groups or categories, make predictions based upon
data, and even “learn” how to interact with an environment when provided
with goals to achieve.</p>
<div class="section level3">
<h3 id="artificial-intelligence-vs-machine-learning">Artificial intelligence vs machine learning<a class="anchor" aria-label="anchor" href="#artificial-intelligence-vs-machine-learning"></a>
</h3>
<p>The term machine learning (ML) is often mentioned alongside
artificial intelligence (AI) and deep learning (DL). Deep learning is a
subset of machine learning, and machine learning is a subset of
artificial intelligence.</p>
<p>AI is increasingly being used as a catch-all term to describe things
that encompass ML and DL systems - from simple email spam filters, to
more complex image recognition systems, to large language models such as
ChatGPT. The more specific term “Artificial General Intelligence” (AGI)
is used to describe a system possessing a “general intelligence” that
can be applied to solve a diverse range of problems, often mimicking the
behaviour of intelligent biological systems. Modern attempts at AGI are
getting close to fooling humans, but while there have been great
advances in AI research, human-like intelligence is only possible in a
few specialist areas.</p>
<p>ML refers to techniques where a computer can “learn” patterns in
data, usually by being shown many training examples. While ML algorithms
can learn to solve specific problems, or multiple similar problems, they
are not considered to possess a general intelligence. ML algorithms
often need hundreds or thousands of examples to learn a task and are
confined to activities such as simple classifications. A human-like
system could learn much quicker than this, and potentially learn from a
single example by using it’s knowledge of many other problems.</p>
<p>DL is a particular field of machine learning where algorithms called
neural networks are used to create highly complex systems. Large
collections of neural networks are able to learn from vast quantities of
data. Deep learning can be used to solve a wide range of problems, but
it can also require huge amounts of input data and computational
resources to train.</p>
<p>The image below shows the relationships between artificial
intelligence, machine learning and deep learning.</p>
<p><img src="fig/introduction/AI_ML_DL_differences.png" alt="An infographic showing some of the relationships between AI, ML, and DL" class="figure">
The image above is by Tukijaaliwa, CC BY-SA 4.0, via Wikimedia Commons,
original source</p>
</div>
<div class="section level3">
<h3 id="machine-learning-in-our-daily-lives">Machine learning in our daily lives<a class="anchor" aria-label="anchor" href="#machine-learning-in-our-daily-lives"></a>
</h3>
<p>Machine learning has quickly become an important technology and is
now frequently used to perform services we encounter in our daily lives.
Here are just a few examples:</p>
<ul>
<li>Banks look for trends in transaction data to detect outliers that
may be fraudulent</li>
<li>Email inboxes use text to decide whether an email is spam or not,
and adjust their rules based upon how we flag emails</li>
<li>Travel apps use live and historic data to estimate traffic, travel
times, and journey routes</li>
<li>Retail companies and streaming services use data to recommend new
content we might like based upon our demographic and historical
preferences</li>
<li>Image, object, and pattern recognition is used to identify humans
and vehicles, capture text, generate subtitles, and much more</li>
<li>Self-driving cars and robots use object detection and performance
feedback to improve their interaction with the world</li>
</ul>
<div id="where-else-have-you-encountered-machine-learning-already" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="where-else-have-you-encountered-machine-learning-already" class="callout-inner">
<h3 class="callout-title">Where else have you encountered machine
learning already?</h3>
<div class="callout-content">
<p>Now that we have explored machine learning in a bit more detail,
discuss with the person next to you: 1. Where else have I seen machine
learning in use? 2. What kind of input data does that machine learning
system use to make predictions/classifications? 3. Is there any evidence
that your interaction with the system contributes to further training?
4. Do you have any examples of the system failing?</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="limitations-of-machine-learning">Limitations of machine learning<a class="anchor" aria-label="anchor" href="#limitations-of-machine-learning"></a>
</h3>
<p>Like any other systems machine learning has limitations, caveats, and
“gotchas” to be aware of that may impact the accuracy and performance of
a machine learning system.</p>
<div class="section level4">
<h4 id="garbage-in-garbage-out">Garbage in = garbage out<a class="anchor" aria-label="anchor" href="#garbage-in-garbage-out"></a>
</h4>
<p>There is a classic expression in computer science: “garbage in =
garbage out”. This means that if the input data we use is garbage then
the ouput will be too. If, for example, we try to use a machine learning
system to find a link between two unlinked variables then it may well
manage to produce a model attempting this, but the output will be
meaningless.</p>
</div>
<div class="section level4">
<h4 id="biases-due-to-training-data">Biases due to training data<a class="anchor" aria-label="anchor" href="#biases-due-to-training-data"></a>
</h4>
<p>The performance of a ML system depends on the breadth and quality of
input data used to train it. If the input data contains biases or blind
spots then these will be reflected in the ML system. For example, if we
collect data on public transport use from only high socioeconomic areas,
the resulting input data may be biased due to a range of factors that
may increase the likelihood of people from those areas using private
transport vs public options.</p>
</div>
<div class="section level4">
<h4 id="extrapolation">Extrapolation<a class="anchor" aria-label="anchor" href="#extrapolation"></a>
</h4>
<p>We can only make reliable predictions about data which is in the same
range as our training data. If we try to extrapolate beyond the
boundaries of the training data we cannot be confident in our results.
As we shall see some algorithms are better suited (or less suited) to
extrapolation than others.</p>
</div>
<div class="section level4">
<h4 id="over-fitting">Over fitting<a class="anchor" aria-label="anchor" href="#over-fitting"></a>
</h4>
<p>Sometimes ML algorithms become over-trained and subsequently don’t
perform well when presented with real data. It’s important to consider
how many rounds of training a ML system has recieved and whether or not
it may have become over-trained.</p>
</div>
<div class="section level4">
<h4 id="inability-to-explain-answers">Inability to explain answers<a class="anchor" aria-label="anchor" href="#inability-to-explain-answers"></a>
</h4>
<p>Machine learning techniques will return an answer based on the input
data and model parameters even if that answer is wrong. Most systems are
unable to explain the logic used to arrive at that answer. This can make
detecting and diagnosing problems difficult.</p>
</div>
</div>
</div>
<div class="section level1">
<h1 id="getting-started-with-scikit-learn">Getting started with Scikit-Learn<a class="anchor" aria-label="anchor" href="#getting-started-with-scikit-learn"></a>
</h1>
<div class="section level3">
<h3 id="about-scikit-learn">About Scikit-Learn<a class="anchor" aria-label="anchor" href="#about-scikit-learn"></a>
</h3>
<p><a href="http://github.com/scikit-learn/scikit-learn" class="external-link">Scikit-Learn</a> is a
python package designed to give access to well-known machine learning
algorithms within Python code, through a clean application programming
interface (API). It has been built by hundreds of contributors from
around the world, and is used across industry and academia.</p>
<p>Scikit-Learn is built upon Python’s <a href="http://numpy.org" class="external-link">NumPy
(Numerical Python)</a> and <a href="http://scipy.org" class="external-link">SciPy (Scientific
Python)</a> libraries, which enable efficient in-core numerical and
scientific computation within Python. As such, Scikit-Learn is not
specifically designed for extremely large datasets, though there is <a href="https://github.com/ogrisel/parallel_ml_tutorial" class="external-link">some work</a> in
this area. For this introduction to ML we are going to stick to
processing small to medium datasets with Scikit-Learn, without the need
for a graphical processing unit (GPU).</p>
<p>Like any other Python package, we can import Scikit-Learn and check
the package version using the following Python commands:</p>
<pre><code>import sklearn
print('scikit-learn:', sklearn.__version__)</code></pre>
<p>{: .language-python}</p>
</div>
<div class="section level3">
<h3 id="representation-of-data-in-scikit-learn">Representation of Data in Scikit-learn<a class="anchor" aria-label="anchor" href="#representation-of-data-in-scikit-learn"></a>
</h3>
<p>Machine learning is about creating models from data: for that reason,
we’ll start by discussing how data can be represented in order to be
understood by the computer.</p>
<p>Most machine learning algorithms implemented in scikit-learn expect
data to be stored in a two-dimensional array or matrix. The arrays can
be either numpy arrays, or in some cases scipy.sparse matrices. The size
of the array is expected to be [n_samples, n_features]</p>
<p>We typically have a “Features Matrix” (usually referred to as the
code variable <code>X</code>) which are the “features” data we wish to
train on.</p>
<ul>
<li>n_samples: The number of samples. A sample can be a document, a
picture, a sound, a video, an astronomical object, a row in database or
CSV file, or whatever you can describe with a fixed set of quantitative
traits.</li>
<li>n_features: The number of features (variables) that can be used to
describe each item in a quantitative manner. Features are generally
real-valued, but may be boolean or discrete-valued in some cases.</li>
</ul>
<p>If we want our ML models to make predictions or classifications, we
also provide “labels” as our expected “answers/results”. The model will
then be trained on the input features to try and match our provided
labels. This is done by providing a “Target Array” (usually referred to
as the code variable <code>y</code>) which contains the “labels or
values” that we wish to predict using the features data.</p>
<p><img src="fig/introduction/sklearn_input.png" alt="Types of Machine Learning" class="figure"> Figure from the <a href="https://github.com/jakevdp/PythonDataScienceHandbook" class="external-link">Python Data
Science Handbook</a></p>
</div>
</div>
<div class="section level1">
<h1 id="what-will-we-cover-today">What will we cover today?<a class="anchor" aria-label="anchor" href="#what-will-we-cover-today"></a>
</h1>
<p>This lesson will introduce you to some of the key concepts and
sub-domains of ML such as supervised learning, unsupervised learning,
and neural networks.</p>
<p>The figure below provides a nice overview of some of the sub-domains
of ML and the techniques used within each sub-domain. We recommend
checking out the Scikit-Learn <a href="https://scikit-learn.org/stable/index.html" class="external-link">webpage</a> for
additional examples of the topics we will cover in this lesson. We will
cover topics highlighted in blue: classical learning techniques such as
regression, classification, clustering, and dimension reduction, as well
as ensemble methods and a brief introduction to neural networks using
perceptrons.</p>
<p><img src="fig/introduction/ML_summary.png" alt="Types of Machine Learning" class="figure"><a href="https://vas3k.com/blog/machine_learning/" class="external-link">Image from Vasily
Zubarev via their blog</a> with modifications in blue to denote lesson
content.</p>
<p>{% include links.md %}</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Machine learning is a set of tools and techniques that use data to
make predictions.</li>
<li>Artificial intelligence is a broader term that refers to making
computers show human-like intelligence.</li>
<li>Deep learning is a subset of machine learning.</li>
<li>All machine learning systems have limitations to be aware of.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div></section><section id="aio-02-regression"><p>Content from <a href="02-regression.html">Supervised methods - Regression</a></p>
<hr>
<p>Last updated on 2025-11-07 |

        <a href="https://github.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/edit/main/episodes/02-regression.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is supervised learning?</li>
<li>What is regression?</li>
<li>How can I model data and make predictions using regression
methods?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Apply linear regression with Scikit-Learn to create a model.</li>
<li>Measure the error between a regression model and input data.</li>
<li>Analyse and assess the accuracy of a linear model using
Scikit-Learn’s metrics library.</li>
<li>Understand how more complex models can be built with non-linear
equations.</li>
<li>Apply polynomial modelling to non-linear data using
Scikit-Learn.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="supervised-learning">Supervised learning<a class="anchor" aria-label="anchor" href="#supervised-learning"></a>
</h1>
<p>Classical machine learning is often divided into two categories –
supervised and unsupervised learning.</p>
<p>For the case of supervised learning we act as a “supervisor” or
“teacher” for our ML algorithms by providing the algorithm with
“labelled data” that contains example answers of what we wish the
algorithm to achieve.</p>
<p>For instance, if we wish to train our algorithm to distinguish
between images of cats and dogs, we would provide our algorithm with
images that have already been labelled as “cat” or “dog” so that it can
learn from these examples. If we wished to train our algorithm to
predict house prices over time we would provide our algorithm with
example data of datetime values that are “labelled” with house
prices.</p>
<p>Supervised learning is split up into two further categories:
classification and regression. For classification the labelled data is
discrete, such as the “cat” or “dog” example, whereas for regression the
labelled data is continuous, such as the house price example.</p>
<p>In this episode we will explore how we can use regression to build a
“model” that can be used to make predictions.</p>
</div>
<div class="section level1">
<h1 id="regression">Regression<a class="anchor" aria-label="anchor" href="#regression"></a>
</h1>
<p>Regression is a statistical technique that relates a dependent
variable (a label or target variable in ML terms) to one or more
independent variables (features in ML terms). A regression model
attempts to describe this relation by fitting the data as closely as
possible according to mathematical criteria. This model can then be used
to predict new labelled values by inputting the independent variables
into it. For example, if we create a house price model we can then feed
in any datetime value we wish, and get a new house price value
prediction.</p>
<p>Regression can be as simple as drawing a “line of best fit” through
data points, known as linear regression, or more complex models such as
polynomial regression, and is used routinely around the world in both
industry and research. You may have already used regression in the past
without knowing that it is also considered a machine learning
technique!</p>
<figure><img src="fig/regression_example.png" alt="Example of linear and polynomial regressions" class="figure mx-auto d-block"><div class="figcaption">Example of linear and polynomial
regressions</div>
</figure><div class="section level2">
<h2 id="linear-regression-using-scikit-learn">Linear regression using Scikit-Learn<a class="anchor" aria-label="anchor" href="#linear-regression-using-scikit-learn"></a>
</h2>
<p>We’ve had a lot of theory so time to start some actual coding! Let’s
create a regression model on some penguin data available through the
Python plotting library <a href="https://seaborn.pydata.org/" class="external-link">Seaborn</a>.</p>
<p>Let’s start by loading in and examining the penguin dataset, which
containing a few hundred samples and a number of features and
labels.</p>
<pre><code># !pip install seaborn if import fails, run this first
import seaborn as sns

dataset = sns.load_dataset("penguins")
print(dataset.shape)
dataset.head()</code></pre>
<p>{: .language-python}</p>
<p>We can see that we have seven columns in total: 4 continuous
(numerical) columns named <code>bill_length_mm</code>,
<code>bill_depth_mm</code>, <code>flipper_length_mm</code>, and
<code>body_mass_g</code>; and 3 discrete (categorical) columns named
<code>species</code>, <code>island</code>, and <code>sex</code>. We can
also see from a quick inspection of the first 5 samples that we have
some missing data in the form of <code>NaN</code> values. Missing data
is a fairly common occurrence in real-life data, so let’s go ahead and
remove any rows that contain <code>NaN</code> values:</p>
<pre><code><span><span class="fu">dataset.dropna</span><span class="op">(</span>inplace<span class="op">=</span><span class="va">True</span><span class="op">)</span></span>
<span><span class="fu">dataset.head</span><span class="op">(</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<p>In this scenario we will train a linear regression model using
<code>body_mass_g</code> as our feature data and
<code>bill_depth_mm</code> as our label data. We will train our model on
a subset of the data by slicing the first 146 samples of our cleaned
data.</p>
<p>In machine learning we often train our models on a subset of data,
for reasons we will explain later in this lesson, so let us extract a
subset of data to work on by slicing the first 146 samples of our
cleaned data and extracting our feature and label data:</p>
<pre><code>import matplotlib.pyplot as plt

train_data = dataset[:146] # first 146 rows

x_train = train_data["body_mass_g"]
y_train = train_data["bill_depth_mm"]

plt.scatter(x_train, y_train)
plt.xlabel("mass g")
plt.ylabel("depth mm")
plt.show()</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/penguin_regression.png" alt="Comparison of the regressions of our dataset" class="figure mx-auto d-block"><div class="figcaption">Comparison of the regressions of our
dataset</div>
</figure><p>In this regression example we will create a Linear Regression model
that will try to predict <code>y</code> values based upon <code>x</code>
values.</p>
<p>In machine learning terminology: we will use our <code>x</code>
feature (variable) and <code>y</code> labels(“answers”) to train our
Linear Regression model to predict <code>y</code> values when provided
with <code>x</code> values.</p>
<p>The mathematical equation for a linear fit is <code>y = mx + c</code>
where <code>y</code> is our label data, <code>x</code> is our input
feature(s), <code>m</code> represents the slope of the linear fit, and
<code>c</code> represents the intercept with the y-axis.</p>
<p>A typical ML workflow is as following:</p>
<ul>
<li>Decide on a model to use model (also known as an estimator)</li>
<li>Tweak your data into the required format for your model</li>
<li>Define and train your model on the input data</li>
<li>Predict some values using the trained model</li>
<li>Check the accuracy of the prediction, and visualise the result</li>
</ul>
<p>We have already decided to use a linear regression model, so we’ll
now pre-process our data into a format that Scikit-Learn can use.</p>
<p>Let’s check our current x/y types and shapes. ~~~
print(type(x_train)) print(type(y_train)) print(x_train.shape)
print(y_train.shape) ~~~ {: .language-python}</p>
<pre><code>import numpy as np

# sklearn requires a 2D array, so lets reshape our 1D arrays from (N) to (N,).
x_train = np.array(x_train).reshape(-1, 1)
y_train = np.array(y_train).reshape(-1, 1)
print(x_train.shape)
print(y_train.shape)</code></pre>
<p>{: .language-python}</p>
<p>Next we’ll define a model, and train it on the pre-processed data.
We’ll also inspect the trained model parameters m and c:</p>
<pre><code>from sklearn.linear_model import LinearRegression

# Define our estimator/model
model = LinearRegression(fit_intercept=True)

# train our estimator/model using our data
lin_regress = model.fit(x_train,y_train)

# inspect the trained estimator/model parameters
m = lin_regress.coef_
c = lin_regress.intercept_
print("linear coefs=", m, c)</code></pre>
<p>{: .language-python}</p>
<p>Now we can make predictions using our trained model, and calculate
the Root Mean Squared Error (RMSE) of our predictions:</p>
<pre><code>import math
from sklearn.metrics import mean_squared_error

# Predict some values using our trained estimator/model.
# In this case we predict our input data to evaluate accuracy!
y_train_pred = lin_regress.predict(x_train)

# calculated a RMS error as a quality of fit metric
error = math.sqrt(mean_squared_error(y_train, y_train_pred))
print("train RMSE =", error)</code></pre>
<p>{: .language-python}</p>
<p>Finally, we’ll plot our input data, our linear fit, and our
predictions:</p>
<pre><code><span><span class="fu">plt.scatter</span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span>, label<span class="op">=</span><span class="st">"input"</span><span class="op">)</span></span>
<span><span class="fu">plt.plot</span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train_pred</span>, <span class="st">"-"</span>, label<span class="op">=</span><span class="st">"fit"</span><span class="op">)</span></span>
<span><span class="fu">plt.plot</span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train_pred</span>, <span class="st">"rx"</span>, label<span class="op">=</span><span class="st">"predictions"</span><span class="op">)</span></span>
<span><span class="fu">plt.xlabel</span><span class="op">(</span><span class="st">"body_mass_g"</span><span class="op">)</span></span>
<span><span class="fu">plt.ylabel</span><span class="op">(</span><span class="st">"bill_depth_mm"</span><span class="op">)</span></span>
<span><span class="fu">plt.legend</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="fu">plt.show</span><span class="op">(</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/regress_penguin_lin.png" alt="Comparison of the regressions of our dataset" class="figure mx-auto d-block"><div class="figcaption">Comparison of the regressions of our
dataset</div>
</figure><p>Congratulations! We’ve now created our first machine-learning model
of the lesson and we can now make predictions of
<code>bill_depth_mm</code> for any <code>body_mass_g</code> values that
we pass into our model.</p>
<p>Let’s provide the model with all of the penguin samples and see how
our model performs on the full dataset:</p>
<pre><code># Extract remaining observations for testing

test_data = dataset[146:] # row 147 -&gt; end
x_test = test_data["body_mass_g"] # lowercase x since there is only one predictor
y_test = test_data["bill_depth_mm"] # lowercase y since there is only one target variable

# sklearn requires a 2D array, so lets reshape our 1D arrays from (N) to (N,).
x_test = np.array(x_test).reshape(-1, 1)
y_test = np.array(y_test).reshape(-1, 1)

# Predict values using our trained estimator/model from earlier
y_test_pred = lin_regress.predict(x_test)

# calculated a RMSE error for all data
error_all = math.sqrt(mean_squared_error(y_test, y_test_pred))
print("test RMSE =", error)</code></pre>
<p>{: .language-python}</p>
<p>Our RMSE for predictions on all penguin samples is far larger than
before, so let’s visually inspect the situation:</p>
<pre><code><span><span class="fu">plt.scatter</span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span>, label<span class="op">=</span><span class="st">"train"</span><span class="op">)</span></span>
<span><span class="fu">plt.scatter</span><span class="op">(</span><span class="va">x_test</span>, <span class="va">y_test</span>, label<span class="op">=</span><span class="st">"test"</span><span class="op">)</span></span>
<span><span class="fu">plt.plot</span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train_pred</span>, <span class="st">"-"</span>, label<span class="op">=</span><span class="st">"fit"</span><span class="op">)</span></span>
<span><span class="co"># plt.plot(x_train, y_train_pred, "rx", label="predictions")</span></span>
<span><span class="fu">plt.xlabel</span><span class="op">(</span><span class="st">"body_mass_g"</span><span class="op">)</span></span>
<span><span class="fu">plt.ylabel</span><span class="op">(</span><span class="st">"bill_depth_mm"</span><span class="op">)</span></span>
<span><span class="fu">plt.legend</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="fu">plt.show</span><span class="op">(</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/penguin_regression_all.png" alt="Comparison of the regressions of our dataset" class="figure mx-auto d-block"><div class="figcaption">Comparison of the regressions of our
dataset</div>
</figure><p>Oh dear. It looks like our linear regression fits okay for our subset
of the penguin data, and a few additional samples, but there appears to
be a cluster of points that are poorly predicted by our model. Even if
we re-trained our model using all samples it looks unlikely that our
model would perform much better due to the two-cluster nature of our
dataset.</p>
<div id="this-is-a-classic-machine-learning-scenario-known-as-overffitting" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="this-is-a-classic-machine-learning-scenario-known-as-overffitting" class="callout-inner">
<h3 class="callout-title">This is a classic machine learning scenario
known as overffitting</h3>
<div class="callout-content">
<p>We have trained our model on a specific set of data, and our model
has learnt to reproduce those specific answers at the expense of
creating a more generally-applicable model. Overfitting is the ML
equivalent of learning an exam papers mark scheme off by heart, rather
than understanding and answering the questions. Overfitting is
especially prevalent when you have (A) limited data, and/or (B)
complicated/large models with lots of trainable parameters (e..g, neural
nets).</p>
</div>
</div>
</div>
<p>In this episode we chose to create a regression model for
<code>bill_depth_mm</code> versus <code>body_mass_g</code> predictions
without understanding our penguin dataset. While we proved we
<em>can</em> make a model by doing this we also saw that the model is
flawed due to complexity in the data that we did not account for.</p>
<p>At least two interpretrations of these results:</p>
<ul>
<li>Bill depth really does simply decrease with increasing body mass,
and we just missed the larger story by zooming in on a subset of the
data</li>
<li>Bill depth generally increases with body mass, but with another
covariate producing somewhat unique distributions/clusters across this
axis.</li>
</ul>
<p>Let’s assume for a moment that we only have access to the two
variables, body mass and bill depth. In this scenario, we may want a
model that captures the global trend of bill depth decreasing with body
mass. For this, we need to revisit how we split our data into train/test
sets. Sklearn provides a tool to make it easy to split into these
subsets using random shuffling of observations.</p>
<pre><code>from sklearn.model_selection import train_test_split
x = dataset['body_mass_g']
y = dataset['bill_depth_mm']

# # sklearn requires a 2D array, so lets reshape our 1D arrays from (N) to (N,).
x = np.array(x).reshape(-1, 1)
y = np.array(y).reshape(-1, 1)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)</code></pre>
<p>{: .language-python}</p>
<div class="section level3">
<h3 id="exercise-try-to-re-implement-our-univariate-regression-model-using-these-new-traintest-sets-">Exercise: Try to re-implement our univariate regression model using
these new train/test sets.<a class="anchor" aria-label="anchor" href="#exercise-try-to-re-implement-our-univariate-regression-model-using-these-new-traintest-sets-"></a>
</h3>
<p>Follow these steps:</p>
<ol style="list-style-type: decimal">
<li>Define your estimator model</li>
<li>Train the model using .fit()</li>
<li>Get predictions from the model using .predict</li>
<li>Calculate RMSE for train/test</li>
<li>Plot scatter plot of train/test data, with line of best fit</li>
</ol>
<pre><code>from sklearn.linear_model import LinearRegression

# Define our estimator/model
model = LinearRegression(fit_intercept=True)

# train our estimator/model using our data
lin_regress = model.fit(x_train, y_train)

# get preds and calculated a RMS error for train data
y_train_pred = lin_regress.predict(x_train)
train_error = math.sqrt(mean_squared_error(y_train, y_train_pred))
print("train RMSE =", train_error)

# get preds and calculated a RMS error for test data
y_test_pred = lin_regress.predict(x_test)
test_error = math.sqrt(mean_squared_error(y_test, y_test_pred))
print("test RMSE =", test_error)

# scatter plot
plt.scatter(x_train, y_train, label="train")
plt.scatter(x_test, y_test, label="test")
plt.plot(x_train, y_train_pred, "-", label="fit")
# plt.plot(x_train, y_train_pred, "rx", label="predictions")
plt.xlabel("body_mass_g")
plt.ylabel("bill_depth_mm")
plt.legend()
plt.show()</code></pre>
<p>{: .language-python}</p>
<p><strong>Quick follow-up</strong>: Interpret the results of your
model. Is it accurate? What does it say about the relationship between
body mass and bill depth? Is this a “good” model?</p>
</div>
</div>
<div class="section level2">
<h2 id="repeating-the-regression-with-different-estimators">Repeating the regression with different estimators<a class="anchor" aria-label="anchor" href="#repeating-the-regression-with-different-estimators"></a>
</h2>
<p>The goal of this lesson isn’t to build a generalisable
<code>bill_depth_mm</code> versus <code>body_mass_g</code> model for the
penguin dataset - the goal is to give you some hands-on experience
building machine learning models with scikit-learn. So let’s repeat the
above but this time using a polynomial function.</p>
<p>Polynomial functions are non-linear functions that are commonly-used
to model data. Mathematically they have <code>N</code> degrees of
freedom and they take the following form
<code>y = a + bx + cx^2 + dx^3 ... + mx^N</code>. If we have a
polynomial of degree <code>N=1</code> we once again return to a linear
equation <code>y = a + bx</code> or as it is more commonly written
<code>y = mx + c</code>.</p>
<p>We’ll follow the same workflow from before: * Decide on a model to
use model (also known as an estimator) * Tweak your data into the
required format for your model * Define and train your model on the
input data * Predict some values using the trained model * Check the
accuracy of the prediction, and visualise the result</p>
<p>We’ve decided to use a Polynomial estimator, so now let’s tweak our
dataset into the required format. For polynomial estimators in
Scikit-Learn this is done in two steps. First we pre-process our input
data <code>x_train</code> into a polynomial representation using the
<code>PolynomialFeatures</code> function. Then we can create our
polynomial regressions using the <code>LinearRegression().fit()</code>
function as before, but this time using the polynomial representation of
our <code>x_train</code>.</p>
<pre><code>from sklearn.preprocessing import PolynomialFeatures

# create a polynomial representation of our training data
poly_features = PolynomialFeatures(degree=2)
x_train_poly = poly_features.fit_transform(x_train)
x_test_poly = poly_features.transform(x_test)</code></pre>
<p>{: .language-python}</p>
<div id="we-convert-a-non-linear-problem-into-a-linear-one" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="we-convert-a-non-linear-problem-into-a-linear-one" class="callout-inner">
<h3 class="callout-title">We convert a non-linear problem into a linear
one</h3>
<div class="callout-content">
<p>By converting our input feature data into a polynomial representation
we can now solve our non-linear problem using linear techniques. This is
a common occurence in machine learning as linear problems are far easier
computationally to solve. We can treat this as just another
pre-processing step to manipulate our features into a ML-ready
format.</p>
</div>
</div>
</div>
<p>We are now ready to create and train our model using our polynomial
feature data.</p>
<pre><code><span><span class="co"># Define our estimator/model(s) and train our model</span></span>
<span><span class="va">poly_regress</span> <span class="op">=</span> <span class="fu">LinearRegression</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="fu">poly_regress.fit</span><span class="op">(</span><span class="va">x_train_poly</span>, <span class="va">y_train</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<p>We can now make predictions on train/test sets, and calculate
RMSE</p>
<pre><code><span><span class="co"># Predictions</span></span>
<span><span class="va">y_train_pred</span> <span class="op">=</span> <span class="fu">poly_regress.predict</span><span class="op">(</span><span class="va">x_train_poly</span><span class="op">)</span></span>
<span><span class="va">y_test_pred</span> <span class="op">=</span> <span class="fu">poly_regress.predict</span><span class="op">(</span><span class="va">x_test_poly</span><span class="op">)</span></span>
<span></span>
<span><span class="va">poly_train_error</span> <span class="op">=</span> <span class="fu">math.sqrt</span><span class="op">(</span><span class="fu">mean_squared_error</span><span class="op">(</span><span class="va">y_train_pred</span>, <span class="va">y_train</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="st">"poly train error ="</span>, <span class="va">poly_train_error</span><span class="op">)</span></span>
<span></span>
<span><span class="va">poly_test_error</span> <span class="op">=</span> <span class="fu">math.sqrt</span><span class="op">(</span><span class="fu">mean_squared_error</span><span class="op">(</span><span class="va">y_test_pred</span>, <span class="va">y_test</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="st">"poly train error ="</span>, <span class="va">poly_test_error</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<p>Finally, let’s visualise our model fit on our training data and full
dataset. ~~~ # Scatter plots for train and test data
plt.scatter(x_train, y_train, label=‘Train’, color=‘blue’, alpha=0.6)
plt.scatter(x_test, y_test, label=‘Test’, color=‘red’, alpha=0.6)</p>
</div>
</div>
<div class="section level1">
<h1 id="plot-the-model-fit">Plot the model fit<a class="anchor" aria-label="anchor" href="#plot-the-model-fit"></a>
</h1>
<p>x_range = np.linspace(min(x), max(x), 500).reshape(-1, 1)
y_range_pred = poly_regress.predict(poly_features.transform(x_range))
plt.plot(x_range, y_range_pred, label=‘Polynomial Model Fit’,
color=‘green’, linewidth=2)</p>
</div>
<div class="section level1">
<h1 id="labels-and-legend">Labels and legend<a class="anchor" aria-label="anchor" href="#labels-and-legend"></a>
</h1>
<p>plt.xlabel(“mass g”) plt.ylabel(“depth mm”) plt.title(‘Polynomial
Regression with Training and Testing Data’) plt.legend()</p>
<pre><code>{: .language-python}


![Comparison of the regressions of our dataset](fig/penguin_regression_poly.png)

::::::::::::::::::::::::::::::::::::: challenge


## Exercise: Vary your polynomial degree to try and improve fitting
Adjust the `degree=3` input variable for the `PolynomialFeatures` function to change the degree of polynomial fit. Can you improve the RMSE of your model?

::::::::::::::::::::::::::::::::::::::::::::::::

## Zooming back out: the importance of EDA
While polynomial regression may help to a degree here, it isn't an ideal solution. Whenever you see multiple distict clusters in your data, you should ask yourself what hidden variable might be causing additional clusters to appear, and move on to exploring multivariable models (models with more than one input feature). When we investigate additional features from our data, we are able to see the larger picture that describes how input variables relate to whatever target variable we are interested in.

When you are doing any kind of modeling work, it is critical to spend your first few hours/days/weeks simply exploring the data. This means:
- Investigate pairwise relationships between "predictors" (X)
- Investigate correlation between predictors
- Plot distributions of each variable
- Check for outliers
- Check for NaNs
</code></pre>
</div>
<div class="section level1">
<h1 id="create-the-pairs-plot">Create the pairs plot<a class="anchor" aria-label="anchor" href="#create-the-pairs-plot"></a>
</h1>
<p>sns.pairplot(dataset, vars=[“body_mass_g”, “bill_depth_mm”],
hue=“species”, diag_kind=“kde”, markers=[“o”, “s”, “D”]) plt.show() ~~~
{: .language-python}</p>
<p>Let’s try a model that includes penguin species as a predictor.</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load the penguins dataset
# (replace this with your own dataset import if needed)
from seaborn import load_dataset
dataset = load_dataset('penguins')

# Drop rows with missing values in relevant columns
dataset = dataset.dropna(subset=['body_mass_g', 'bill_depth_mm', 'species'])

# Define predictors and target
X = dataset[['body_mass_g', 'species']]
y = dataset['bill_depth_mm']</code></pre>
<p>{: .language-python}</p>
<p>Since the species column is coded as a string, we need to convert it
into a numerical format before we can use it in a machine learning
model. To do this, we apply dummy coding (also called one-hot encoding),
which creates new binary columns for each species category (e.g.,
species_Adelie, species_Chinstrap, species_Gentoo). Each row gets a 1 in
the column that matches its species and 0 in the others.</p>
<p>By default, we drop the first category to avoid
multicollinearity—this means the omitted category serves as the
reference group when interpreting model coefficients. ~~~ # One-hot
encode species (drop_first avoids multicollinearity) X =
pd.get_dummies(X, columns=[‘species’], drop_first=True) ~~~ {:
.language-python}</p>
<p>We can than train/fit and evaluate our model as usual. ~~~ #
Train/test split x_train, x_test, y_train, y_test = train_test_split(X,
y, test_size=0.2, random_state=0)</p>
</div>
<div class="section level1">
<h1 id="fit-a-linear-regression-model">Fit a linear regression model<a class="anchor" aria-label="anchor" href="#fit-a-linear-regression-model"></a>
</h1>
<p>model = LinearRegression() model.fit(x_train, y_train)</p>
</div>
<div class="section level1">
<h1 id="predict-and-evaluate">Predict and evaluate<a class="anchor" aria-label="anchor" href="#predict-and-evaluate"></a>
</h1>
<p>y_pred = model.predict(x_test) rmse = mean_squared_error(y_test,
y_pred) print(f”RMSE with species as a predictor: {rmse:.2f}“)</p>
</div>
<div class="section level1">
<h1 id="optional-view-learned-coefficients">Optional: view learned coefficients<a class="anchor" aria-label="anchor" href="#optional-view-learned-coefficients"></a>
</h1>
<p>coefficients = pd.Series(model.coef_, index=X.columns)
print(“coefficients:”) print(coefficients) ~~~ {: .language-python}</p>
<p>{% include links.md %}</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Scikit-Learn is a Python library with lots of useful machine
learning functions.</li>
<li>Scikit-Learn includes a linear regression function.</li>
<li>Scikit-Learn can perform polynomial regressions to model non-linear
data.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div></section><section id="aio-03-classification"><p>Content from <a href="03-classification.html">Supervised methods - Classification</a></p>
<hr>
<p>Last updated on 2025-11-07 |

        <a href="https://github.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/edit/main/episodes/03-classification.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can I classify data into known categories?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Use two different supervised methods to classify data.</li>
<li>Learn about the concept of hyper-parameters.</li>
<li>Learn to validate and ?cross-validate? models</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="classification">Classification<a class="anchor" aria-label="anchor" href="#classification"></a>
</h1>
<p>Classification is a supervised method to recognise and group data
objects into a pre-determined categories. Where regression uses labelled
observations to predict a continuous numerical value, classification
predicts a discrete categorical fit to a class. Classification in ML
leverages a wide range of algorithms to classify a set of data/datasets
into their respective categories.</p>
<p>In this episode we are going to introduce the concept of supervised
classification by classifying penguin data into different species of
penguins using Scikit-Learn.</p>
<div class="section level2">
<h2 id="the-penguins-dataset">The penguins dataset<a class="anchor" aria-label="anchor" href="#the-penguins-dataset"></a>
</h2>
<p>We’re going to be using the penguins dataset of Allison Horst,
published <a href="https://github.com/allisonhorst/palmerpenguins" class="external-link">here</a>, The
dataset contains 344 size measurements for three penguin species
(Chinstrap, Gentoo and Adélie) observed on three islands in the Palmer
Archipelago, Antarctica.</p>
<figure><img src="fig/palmer_penguins.png" alt="Artwork by @allison_horst" class="figure mx-auto d-block"><div class="figcaption"><em>Artwork by <span class="citation">@allison_horst</span></em></div>
</figure><p>The physical attributes measured are flipper length, beak length,
beak width, body mass, and sex. <img src="fig/culmen_depth.png" alt="Artwork by @allison_horst" class="figure"></p>
<p>In other words, the dataset contains 344 rows with 7 features i.e. 5
physical attributes, species and the island where the observations were
made.</p>
<pre><code>import seaborn as sns

dataset = sns.load_dataset('penguins')
dataset.head()</code></pre>
<p>{: .language-python}</p>
<p>Our aim is to develop a classification model that will predict the
species of a penguin based upon measurements of those variables.</p>
<p>As a rule of thumb for ML/DL modelling, it is best to start with a
simple model and progressively add complexity in order to meet our
desired classification performance.</p>
<p>For this lesson we will limit our dataset to only numerical values
such as bill_length, bill_depth, flipper_length, and body_mass while we
attempt to classify species.</p>
<p>The above table contains multiple categorical objects such as
species. If we attempt to include the other categorical fields, island
and sex, we might hinder classification performance due to the
complexity of the data.</p>
<div class="section level3">
<h3 id="preprocessing-our-data">Preprocessing our data<a class="anchor" aria-label="anchor" href="#preprocessing-our-data"></a>
</h3>
<p>Lets do some pre-processing on our dataset and specify our
<code>X</code> features and <code>y</code> labels:</p>
<pre><code># Extract the data we need
feature_names = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
dataset.dropna(subset=feature_names, inplace=True)

class_names = dataset['species'].unique()

X = dataset[feature_names]
y = dataset['species']</code></pre>
<p>{: .language-python}</p>
<p>Having extracted our features <code>X</code> and labels
<code>y</code>, we can now split the data using the
<code>train_test_split</code> function.</p>
</div>
</div>
<div class="section level2">
<h2 id="training-testing-split">Training-testing split<a class="anchor" aria-label="anchor" href="#training-testing-split"></a>
</h2>
<p>When undertaking any machine learning project, it’s important to be
able to evaluate how well your model works.</p>
<p>Rather than evaluating this manually we can instead set aside some of
our training data, usually 20% of our training data, and use these as a
testing dataset. We then train on the remaining 80% and use the testing
dataset to evaluate the accuracy of our trained model.</p>
<p>We lose a bit of training data in the process, But we can now easily
evaluate the performance of our model. With more advanced test-train
split techniques we can even recover this lost training data!</p>
<div id="why-do-we-do-this" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="why-do-we-do-this" class="callout-inner">
<h3 class="callout-title">Why do we do this?</h3>
<div class="callout-content">
<p>It’s important to do this early, and to do all of your work with the
training dataset - this avoids any risk of you introducing bias to the
model based on your own manual observations of data in the testing set
(afterall, we want the model to make the decisions about parameters!).
This can also highlight when you are over-fitting on your training
data.</p>
</div>
</div>
</div>
<p>How we split the data into training and testing sets is also
extremely important. We need to make sure that our training data is
representitive of both our test data and actual data.</p>
<p>For classification problems this means we should ensure that each
class of interest is represented proportionately in both training and
testing sets. For regression problems we should ensure that our training
and test sets cover the range of feature values that we wish to
predict.</p>
<p>In the previous regression episode we created the penguin training
data by taking the first 146 samples our the dataset. Unfortunately the
penguin data is sorted by species and so our training data only
considered one type of penguin and thus was not representitive of the
actual data we tried to fit. We could have avoided this issue by
randomly shuffling our penguin samples before splitting the data.</p>
<div id="when-not-to-shuffle-your-data" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="when-not-to-shuffle-your-data" class="callout-inner">
<h3 class="callout-title">When not to shuffle your data</h3>
<div class="callout-content">
<p>Sometimes your data is dependant on it’s ordering, such as
time-series data where past values influence future predictions.
Creating train-test splits for this can be tricky at first glance, but
fortunately there are existing techniques to tackle this (often called
stratification): See <a href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators" class="external-link">Scikit-Learn</a>
for more information.</p>
</div>
</div>
</div>
<p>We specify the fraction of data to use as test data, and the function
randomly shuffles our data prior to splitting:</p>
<pre><code>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)</code></pre>
<p>{: .language-python}</p>
<p>We’ll use <code>X_train</code> and <code>y_train</code> to develop
our model, and only look at <code>X_test</code> and <code>y_test</code>
when it’s time to evaluate its performance.</p>
<div class="section level3">
<h3 id="visualising-the-data">Visualising the data<a class="anchor" aria-label="anchor" href="#visualising-the-data"></a>
</h3>
<p>In order to better understand how a model might classify this data,
we can first take a look at the data visually, to see what patterns we
might identify.</p>
<pre><code>import matplotlib.pyplot as plt

fig01 = sns.scatterplot(X_train, x=feature_names[0], y=feature_names[1], hue=dataset['species'])
plt.show()</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/e3_penguins_vis.png" alt="Visualising the penguins dataset" class="figure mx-auto d-block"><div class="figcaption">Visualising the penguins dataset</div>
</figure><p>As there are four measurements for each penguin, we need quite a few
plots to visualise all four dimensions against each other. Here is a
handy Seaborn function to do so:</p>
<pre><code><span><span class="fu">sns.pairplot</span><span class="op">(</span><span class="va">dataset</span>, hue<span class="op">=</span><span class="st">"species"</span><span class="op">)</span></span>
<span><span class="fu">plt.show</span><span class="op">(</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/pairplot.png" alt="Visualising the penguins dataset" class="figure mx-auto d-block"><div class="figcaption">Visualising the penguins dataset</div>
</figure><p>We can see that penguins from each species form fairly distinct
spatial clusters in these plots, so that you could draw lines between
those clusters to delineate each species. This is effectively what many
classification algorithms do. They use the training data to delineate
the observation space, in this case the 4 measurement dimensions, into
classes. When given a new observation, the model finds which of those
class areas the new observation falls in to.</p>
</div>
</div>
<div class="section level2">
<h2 id="classification-using-a-decision-tree">Classification using a decision tree<a class="anchor" aria-label="anchor" href="#classification-using-a-decision-tree"></a>
</h2>
<p>We’ll first apply a decision tree classifier to the data. Decisions
trees are conceptually similar to flow diagrams (or more precisely for
the biologists: dichotomous keys). They split the classification problem
into a binary tree of comparisons, at each step comparing a measurement
to a value, and moving left or right down the tree until a
classification is reached.</p>
<figure><img src="fig/decision_tree_example.png" alt="Decision tree for classifying penguins" class="figure mx-auto d-block"><div class="figcaption">Decision tree for classifying penguins</div>
</figure><p>Training and using a decision tree in Scikit-Learn is
straightforward: ~~~ from sklearn.tree import DecisionTreeClassifier,
plot_tree</p>
<p>clf = DecisionTreeClassifier(max_depth=2) clf.fit(X_train,
y_train)</p>
<p>clf.predict(X_test) ~~~ {: .language-python}</p>
<div id="hyper-parameters-parameters-that-tune-a-model" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="hyper-parameters-parameters-that-tune-a-model" class="callout-inner">
<h3 class="callout-title">Hyper-parameters: parameters that tune a
model</h3>
<div class="callout-content">
<p>‘Max Depth’ is an example of a <em>hyper-parameter</em> for the
decision tree model. Where models use the parameters of an observation
to predict a result, hyper-parameters are used to tune how a model
works. Each model you encounter will have its own set of
hyper-parameters, each of which affects model behaviour and performance
in a different way. The process of adjusting hyper-parameters in order
to improve model performance is called hyper-parameter tuning.</p>
</div>
</div>
</div>
<p>We can conveniently check how our model did with the .score()
function, which will make predictions and report what proportion of them
were accurate:</p>
<pre><code><span><span class="va">clf_score</span> <span class="op">=</span> <span class="fu">clf.score</span><span class="op">(</span><span class="va">X_test</span>, <span class="va">y_test</span><span class="op">)</span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">clf_score</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<p>Our model reports an accuracy of ~98% on the test data! We can also
look at the decision tree that was generated:</p>
<pre><code>fig = plt.figure(figsize=(12, 10))
plot_tree(clf, class_names=class_names, feature_names=feature_names, filled=True, ax=fig.gca())
plt.show()</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/e3_dt_2.png" alt="Decision tree for classifying penguins" class="figure mx-auto d-block"><div class="figcaption">Decision tree for classifying penguins</div>
</figure><p>The first first question (<code>depth=1</code>) splits the training
data into “Adelie” and “Gentoo” categories using the criteria
<code>flipper_length_mm &lt;= 206.5</code>, and the next two questions
(<code>depth=2</code>) split the “Adelie” and “Gentoo” categories into
“Adelie &amp; Chinstrap” and “Gentoo &amp; Chinstrap” predictions.</p>
<!-- We can see from this that there's some very tortuous logic being used to tease out every single observation in the training set. For example, the single purple Gentoo node at the bottom of the tree. If we truncated that branch to the second level (Chinstrap), we'd have a little inaccuracy, a total of 9 non-Chinstraps in with 48 Chinstraps, but a less convoluted model.

The tortuous logic, such as the bottom purple Gentoo node, is a clear indication that this model has been over-fitted. It has developed a very complex delineation of the classification space in order to match every single observation, which will likely lead to poor results for new observations.

We can see that rather than clean lines between species, the decision tree produces orthogonal regions as each decision only considers a single parameter. Again, we can see that the model is over-fitting as the decision space is far more complex than needed, with regions that only select a single point. -->
<div class="section level3">
<h3 id="visualising-the-classification-space">Visualising the classification space<a class="anchor" aria-label="anchor" href="#visualising-the-classification-space"></a>
</h3>
<p>We can visualise the classification space (decision tree boundaries)
to get a more intuitive feel for what it is doing.Note that our 2D plot
can only show two parameters at a time, so we will quickly visualise by
training a new model on only 2 features:</p>
<pre><code>from sklearn.inspection import DecisionBoundaryDisplay

f1 = feature_names[0]
f2 = feature_names[3]

clf = DecisionTreeClassifier(max_depth=2)
clf.fit(X_train[[f1, f2]], y_train)

d = DecisionBoundaryDisplay.from_estimator(clf, X_train[[f1, f2]])

sns.scatterplot(X_train, x=f1, y=f2, hue=y_train, palette="husl")
plt.show()</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/e3_dt_space_2.png" alt="Classification space for our decision tree" class="figure mx-auto d-block"><div class="figcaption">Classification space for our decision tree</div>
</figure>
</div>
</div>
<div class="section level2">
<h2 id="tuning-the-max_depth-hyperparameter">Tuning the <code>max_depth</code> hyperparameter<a class="anchor" aria-label="anchor" href="#tuning-the-max_depth-hyperparameter"></a>
</h2>
<p>Our decision tree using a <code>max_depth=2</code> is fairly simple
and there are still some incorrect predictions in our final
classifications. Let’s try varying the <code>max_depth</code>
hyperparameter to see if we can improve our model predictions.</p>
<!-- We can reduce the over-fitting of our decision tree model by limiting its depth, forcing it to use less decisions to produce a classification, and resulting in a simpler decision space. -->
<pre><code>import pandas as pd

max_depths = [1, 2, 3, 4, 5]

accuracy = []
for i, d in enumerate(max_depths):
    clf = DecisionTreeClassifier(max_depth=d)
    clf.fit(X_train, y_train)
    acc = clf.score(X_test, y_test)

    accuracy.append((d, acc))

acc_df = pd.DataFrame(accuracy, columns=['depth', 'accuracy'])

sns.lineplot(acc_df, x='depth', y='accuracy')
plt.xlabel('Tree depth')
plt.ylabel('Accuracy')
plt.show()</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/e3_dt_overfit.png" alt="Performance of decision trees of various depths" class="figure mx-auto d-block"><div class="figcaption">Performance of decision trees of various
depths</div>
</figure><p>Here we can see that a <code>max_depth=2</code> performs slightly
better on the test data than those with <code>max_depth &gt; 2</code>.
This can seem counter intuitive, as surely more questions should be able
to better split up our categories and thus give better predictions?</p>
<p>Let’s reuse our fitting and plotting codes from above to inspect a
decision tree that has <code>max_depth=5</code>:</p>
<pre><code>clf = DecisionTreeClassifier(max_depth=5)
clf.fit(X_train, y_train)

fig = plt.figure(figsize=(12, 10))
plot_tree(clf, class_names=class_names, feature_names=feature_names, filled=True, ax=fig.gca())
plt.show()</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/e3_dt_6.png" alt="Simplified decision tree" class="figure mx-auto d-block"><div class="figcaption">Simplified decision tree</div>
</figure><p>It looks like our decision tree has split up the training data into
the correct penguin categories and more accurately than the
<code>max_depth=2</code> model did, however it used some very specific
questions to split up the penguins into the correct categories. Let’s
try visualising the classification space for a more intuitive
understanding: ~~~ f1 = feature_names[0] f2 = feature_names[3]</p>
<p>clf = DecisionTreeClassifier(max_depth=5) clf.fit(X_train[[f1, f2]],
y_train)</p>
<p>d = DecisionBoundaryDisplay.from_estimator(clf, X_train[[f1,
f2]])</p>
<p>sns.scatterplot(X_train, x=f1, y=f2, hue=y_train, palette=‘husl’)
plt.show() ~~~ {: .language-python}</p>
<figure><img src="fig/e3_dt_space_6.png" alt="Classification space of the simplified decision tree" class="figure mx-auto d-block"><div class="figcaption">Classification space of the simplified decision
tree</div>
</figure><p>Earlier we saw that the <code>max_depth=2</code> model split the data
into 3 simple bounding boxes, whereas for <code>max_depth=5</code> we
see the model has created some very specific classification boundaries
to correctly classify every point in the training data.</p>
<p>This is a classic case of over-fitting - our model has produced
extremely specific parameters that work for the training data but are
not representitive of our test data. Sometimes simplicity is better!</p>
<blockquote>
<h2 id="exercise-adding-noise-to-the-training-data">Exercise: Adding
noise to the training data</h2>
<p>We observed that this data doesn’t seem prone to overfitting effects.
Why might this be? There are at least two factors contributing to these
results:</p>
<ol style="list-style-type: decimal">
<li>We only have 4 predictors. With so few predictors, there are only so
many unique tree structures that can be tested/formed. This makes
overfitting less likely.</li>
<li>Our data is sourced from a python library, and has been
cleaned/vetted. Real-world data typically has more noise. Let’s try
adding a small amount of noise to the data using the below code. How
does this impact the ideal setting for depth level?</li>
</ol>
<pre><code># 1) LOAD DATA (if not loaded already)
import seaborn as sns
dataset = sns.load_dataset('penguins')
dataset.head()

# 2) Extract the data we need and drop NaNs (if not done already)
feature_names = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
dataset.dropna(subset=feature_names, inplace=True)
class_names = dataset['species'].unique()
X = dataset[feature_names]
y = dataset['species']

# 3) ADD RANDOM NOISE TO X
import numpy as np
stds = X.std(axis=0).to_numpy()

# Generate noise and scale it
# Set seed for reproducibility
np.random.seed(42)
noise = np.random.normal(0, 1, X.shape) # sample numbers from normal distribution
scaled_noise = noise * stds  # up to 1
X_noisy = X + scaled_noise


import matplotlib.pyplot as plt
fig01 = sns.scatterplot(X, x=feature_names[0], y=feature_names[1], hue=dataset['species'])
plt.show()
fig02 = sns.scatterplot(X_noisy, x=feature_names[0], y=feature_names[1], hue=dataset['species'])
plt.show()

# 4) TRAIN/TEST SPLIT
from sklearn.model_selection import train_test_split
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)
X_train, X_test, y_train, y_test = train_test_split(X_noisy, y, test_size=0.2, random_state=0, stratify=y)

# 5) HYPERPARAM TUNING
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
import matplotlib.pyplot as plt

max_depths = list(range(1,200))
accuracy = []
for d in max_depths:
    clf = DecisionTreeClassifier(max_depth=d)
    clf.fit(X_train, y_train)
    acc = clf.score(X_test, y_test)

    accuracy.append((d, acc))

acc_df = pd.DataFrame(accuracy, columns=['depth', 'accuracy'])

sns.lineplot(acc_df, x='depth', y='accuracy')
plt.xlabel('Tree depth')
plt.ylabel('Accuracy')
plt.show()</code></pre>
<p>{: .language-python}</p>
</blockquote>
</div>
<div class="section level2">
<h2 id="classification-using-support-vector-machines">Classification using support vector machines<a class="anchor" aria-label="anchor" href="#classification-using-support-vector-machines"></a>
</h2>
<p>Next, we’ll look at another commonly used classification algorithm,
and see how it compares. Support Vector Machines (SVM) work in a way
that is conceptually similar to your own intuition when first looking at
the data. They devise a set of hyperplanes that delineate the parameter
space, such that each region contains ideally only observations from one
class, and the boundaries fall between classes. One of the core
strengths of Support Vector Machines (SVMs) is their ability to handle
non-linear relationships between features by transforming the data into
a higher-dimensional space. This transformation allows SVMs to find a
linear boundary/hyperplane in this new space, which corresponds to a
non-linear boundary in the original space.</p>
<p><strong>What are the “trainable parameters” in an SVM?</strong> For a
linear SVM, the trainable parameters are:</p>
<ul>
<li>Weight vector: A vector that defines the orientation of the
hyperplane. Its size is equal to the number of features in X.</li>
<li>Bias: A scalar value that shifts the hyperplane to maximize the
margin.</li>
</ul>
<div class="section level3">
<h3 id="when-to-choose-svm-over-decision-tree">When to Choose SVM Over Decision Tree<a class="anchor" aria-label="anchor" href="#when-to-choose-svm-over-decision-tree"></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<strong>High-Dimensional Data</strong>:
<ul>
<li>
<strong>Why SVM</strong>: SVMs excel in high-dimensional spaces
because the kernel trick allows them to separate classes even in complex
feature spaces without explicitly mapping the data.</li>
<li>
<strong>Why Not Decision Tree</strong>: Decision trees struggle with
high-dimensional data as the number of potential splits grows
exponentially, leading to overfitting or underperformance.</li>
</ul>
</li>
<li>
<strong>Accuracy over Interpretbaility</strong>:
<ul>
<li>
<strong>Why SVM</strong>: SVMs are often considered black-box
models, focusing on accuracy rather than interpretability.</li>
<li>
<strong>Why Not Decision Tree</strong>: Decision trees are typically
interpretable, making them better if you need to explain your
model.</li>
</ul>
</li>
</ol>
</div>
<div class="section level3">
<h3 id="standardizing-data">Standardizing data<a class="anchor" aria-label="anchor" href="#standardizing-data"></a>
</h3>
<p>Unlike decision trees, SVMs require an additional pre-processing step
for our data. We need to standardize or “z-score” it. Our raw data has
parameters with different magnitudes such as bill length measured in
10’s of mm’s, whereas body mass is measured in 1000’s of grams. If we
trained an SVM directly on this data, it would only consider the
parameter with the greatest variance (body mass).</p>
<p>Standarizing maps each parameter to a new range so that it has a mean
of 0 and a standard deviation of 1. This places all features on the same
playing field, and allows SVM to reveal the most accurate decision
boundaries.</p>
<div class="section level4">
<h4 id="when-to-standardize-your-data-a-broader-overview">When to Standardize Your Data: A Broader Overview<a class="anchor" aria-label="anchor" href="#when-to-standardize-your-data-a-broader-overview"></a>
</h4>
<p>Standardization is an essential preprocessing step for many machine
learning models, particularly those that rely on <strong>distance-based
calculations</strong> to make predictions or extract features. These
models are sensitive to the scale of the input features because their
mathematical foundations involve distances, magnitudes, or directions in
the feature space. Without standardization, features with larger ranges
can dominate the calculations, leading to suboptimal results. However,
not all models require standardization; some, like decision trees,
operate on thresholds and are unaffected by feature scaling. Here’s a
breakdown of when to standardize, explicitly explaining the role of
distance-based calculations in each case.</p>
<div class="section level5">
<h5 id="when-to-standardize-models-that-use-distance-based-calculations">When to Standardize: Models That Use Distance-Based
Calculations<a class="anchor" aria-label="anchor" href="#when-to-standardize-models-that-use-distance-based-calculations"></a>
</h5>
<ol style="list-style-type: decimal">
<li><p><strong>Support Vector Machines (SVMs)</strong>: SVMs calculate
the distance of data points to a hyperplane and aim to maximize the
margin (the distance between the hyperplane and the nearest points,
called support vectors).</p></li>
<li><p><strong>k-Nearest Neighbors (k-NN)</strong>: k-NN determines
class or value predictions based on the distance between a query point
and its k-nearest neighbors in the feature space.</p></li>
<li><p><strong>Logistic Regression with Regularization</strong>:
Regularization terms (e.g., L1 or L2 penalties) involve calculating the
magnitude (distance) of the parameter vector to reduce overfitting and
encourage simplicity.</p></li>
<li><p><strong>Principal Component Analysis (PCA)</strong>: PCA
identifies principal components by calculating the Euclidean distance
from data points to the axes representing the highest variance
directions in the feature space.</p></li>
<li><p><strong>Neural Networks (NNs)</strong>: Neural networks rely on
gradient-based optimization to learn weights. If input features have
vastly different scales, gradients can become unstable, slowing down
training or causing convergence issues. Standardizing or normalizing
(scaling from 0 to 1) features ensures that all inputs contribute
equally to the optimization process.</p></li>
<li><p><strong>Linear Regression (for Interpreting Many
Coefficients)</strong>: While linear regression itself doesn’t rely on
distance-based calculations, standardization is crucial when
interpreting coefficients because it ensures that all features are on
the same scale, making their relative importance directly comparable.
Without standardization, coefficients in linear regression reflect the
relationship between the dependent variable and a feature in the units
of that feature, making it difficult to compare features with different
scales (e.g., height in centimeters vs. weight in kilograms).</p></li>
</ol>
</div>
<div class="section level5">
<h5 id="when-to-skip-standardization-models-that-dont-use-distance-based-calculations">When to Skip Standardization: Models That Don’t Use Distance-Based
Calculations<a class="anchor" aria-label="anchor" href="#when-to-skip-standardization-models-that-dont-use-distance-based-calculations"></a>
</h5>
<ol style="list-style-type: decimal">
<li><p><strong>Decision Trees</strong>: Decision trees split data based
on thresholds, independent of feature scales, without relying on any
distance-based calculations.</p></li>
<li><p><strong>Random Forests</strong>: Random forests aggregate
decisions from multiple trees, which also use thresholds rather than
distance-based metrics.</p></li>
<li><p><strong>Gradient Boosted Trees</strong>: Gradient boosting
optimizes decision trees sequentially, focusing on residuals and splits
rather than distance measures.</p></li>
</ol>
<p>By understanding whether a model relies on distance-based
calculations (or benefits from standardized features for
interpretability), you can decide whether standardization is necessary,
ensuring that your preprocessing pipeline is well-suited to the
algorithm you’re using.</p>
<pre><code>from sklearn import preprocessing
import pandas as pd

scalar = preprocessing.StandardScaler()
scalar.fit(X_train)
X_train_scaled = pd.DataFrame(scalar.transform(X_train), columns=X_train.columns, index=X_train.index)
X_test_scaled = pd.DataFrame(scalar.transform(X_test), columns=X_test.columns, index=X_test.index)</code></pre>
<p>{: .language-python}</p>
<p>Note that we fit the scalar to our training data - we then use this
same pre-trained scalar to transform our testing data.</p>
<p>With this scaled data, training the models works exactly the same as
before.</p>
<pre><code>from sklearn import svm

SVM = svm.SVC(kernel='poly', degree=3, C=1.5)
SVM.fit(X_train_scaled, y_train)

svm_score = SVM.score(X_test_scaled, y_test)
print("Decision tree score is ", clf_score)
print("SVM score is ", svm_score)</code></pre>
<p>{: .language-python}</p>
<p>We can again visualise the decision space produced, also using only
two parameters:</p>
<pre><code><span><span class="va">x2</span> <span class="op">=</span> <span class="va">X_train_scaled</span><span class="op">[[</span><span class="va">feature_names</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>, <span class="va">feature_names</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">]</span></span>
<span></span>
<span><span class="va">SVM</span> <span class="op">=</span> <span class="fu">svm.SVC</span><span class="op">(</span>kernel<span class="op">=</span><span class="st">'poly'</span>, degree<span class="op">=</span><span class="fl">3</span>, C<span class="op">=</span><span class="fl">1.5</span><span class="op">)</span></span>
<span><span class="fu">SVM.fit</span><span class="op">(</span><span class="va">x2</span>, <span class="va">y_train</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">DecisionBoundaryDisplay.from_estimator</span><span class="op">(</span><span class="va">SVM</span>, <span class="va">x2</span><span class="op">)</span> <span class="co">#, ax=ax</span></span>
<span><span class="fu">sns.scatterplot</span><span class="op">(</span><span class="va">x2</span>, x<span class="op">=</span><span class="va">feature_names</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>, y<span class="op">=</span><span class="va">feature_names</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, hue<span class="op">=</span><span class="va">dataset</span><span class="op">[</span><span class="st">'species'</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu">plt.show</span><span class="op">(</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<p><strong>SVM parameters</strong>:</p>
<ul>
<li>
<strong><code>kernel</code></strong>: Specifies how the SVM
transforms the data to find patterns; start with <code>'rbf'</code> for
most cases, <code>'linear'</code> for high-dimensional data, or
<code>'poly'</code> for polynomial relationships.
<ul>
<li>Linear Kernel: Directly computes the dot product between input
vectors; best for linearly separable data and high-dimensional spaces,
offering simplicity and efficiency.</li>
<li>Poly Kernel: Computes polynomial relationships of features, allowing
for flexible decision boundaries; ideal for data with polynomial
patterns.</li>
<li>RBF (Radial Basis Function) Kernel: Uses a Gaussian function to
create highly flexible decision boundaries; effective for non-linear,
complex data.</li>
</ul>
</li>
<li>
<strong><code>degree</code></strong>: Sets the complexity of the
polynomial kernel; use <code>degree=3</code> for cubic relationships,
and avoid going higher unless you have lots of data.</li>
<li>
<strong><code>C</code></strong>: Balances smoothness of the decision
boundary and misclassifications; start with <code>C=1</code>, increase
for tighter boundaries, decrease to prevent overfitting.</li>
</ul>
<figure><img src="fig/e3_svc_space.png" alt="Classification space generated by the SVM model" class="figure mx-auto d-block"><div class="figcaption">Classification space generated by the SVM
model</div>
</figure><p>While this SVM model performs slightly worse than our decision tree
(95.6% vs. 98.5%), it’s likely that the non-linear boundaries will
perform better when exposed to more and more real data, as decision
trees are prone to overfitting and requires complex linear models to
reproduce simple non-linear boundaries. It’s important to pick a model
that is appropriate for your problem and data trends!</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Classification requires labelled data (is supervised)</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div>
</div>
</div>
</div></section><section id="aio-04-ensemble-methods"><p>Content from <a href="04-ensemble-methods.html">Ensemble methods</a></p>
<hr>
<p>Last updated on 2025-11-07 |

        <a href="https://github.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/edit/main/episodes/04-ensemble-methods.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are ensemble methods?</li>
<li>What are random forests?</li>
<li>How can we stack estimators in sci-kit learn?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learn about applying ensemble methods in scikit-learn.</li>
<li>Understand why ensemble methods are useful.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="ensemble-methods">Ensemble methods<a class="anchor" aria-label="anchor" href="#ensemble-methods"></a>
</h1>
<p>What’s better than one decision tree? Perhaps two? or three? How
about enough trees to make up a forest? Ensemble methods bundle
individual models together and use each of their outputs to contribute
towards a final consensus for a given problem. Ensemble methods are
based on the mantra that the whole is greater than the sum of the
parts.</p>
<p>Thinking back to the classification episode with decision trees we
quickly stumbled into the problem of overfitting our training data. If
we combine predictions from a series of over/under fitting estimators
then we can often produce a better final prediction than using a single
reliable model - in the same way that humans often hear multiple
opinions on a scenario before deciding a final outcome. Decision trees
and regressions are often very sensitive to training outliers and so are
well suited to be a part of an ensemble.</p>
<p>Ensemble methods are used for a variety of applciations including,
but not limited to, search systems and object detection. We can use any
model/estimator available in sci-kit learn to create an ensemble. There
are three main methods to create ensembles approaches:</p>
<ul>
<li>Stacking</li>
<li>Bagging</li>
<li>Boosting</li>
</ul>
<p>Let’s explore them in a bit more depth.</p>
<div class="section level3">
<h3 id="stacking">Stacking<a class="anchor" aria-label="anchor" href="#stacking"></a>
</h3>
<p>This is where we train a series of different models/estimators on the
same input data in parallel. We then take the output of each model and
pass them into a final decision algorithm/model that makes the final
prediction.</p>
<p>If we trained the same model multiple times on the same data we would
expect very similar answers, and so the emphasis with stacking is to
choose different models that can be used to build up a reliable
concensus. Regression is then typically a good choice for the final
decision-making model.</p>
<figure><img src="fig/stacking.jpeg" alt="Stacking" class="figure mx-auto d-block"><div class="figcaption">Stacking</div>
</figure><p><a href="https://vas3k.com/blog/machine_learning/" class="external-link">Image from Vasily
Zubarev via their blog</a></p>
</div>
<div class="section level3">
<h3 id="bagging-a-k-a-bootstrap-aggregating">Bagging (a.k.a <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" class="external-link">Bootstrap
AGGregatING</a> )<a class="anchor" aria-label="anchor" href="#bagging-a-k-a-bootstrap-aggregating"></a>
</h3>
<p>This is where we use the same model/estimator and fit it on different
subsets of the training data. We can then average the results from each
model to produce a final prediction. The subsets are random and may even
repeat themselves.</p>
<p><strong>How to remember</strong>: The name comes from combining two
ideas, bootstrap (random sampling with replacement) and aggregating
(combining predictions). Imagine putting your data into a “bag,” pulling
out random samples (with replacement), training models on those samples,
and combining their outputs.</p>
<p>The most common example is known as the Random Forest algorithm,
which we’ll take a look at later on. Random Forests are typically used
as a faster, computationally cheaper alternative to Neural Networks,
which is ideal for real-time applications like camera face detection
prompts.</p>
<figure><img src="fig/bagging.jpeg" alt="Stacking" class="figure mx-auto d-block"><div class="figcaption">Stacking</div>
</figure><p><a href="https://vas3k.com/blog/machine_learning/" class="external-link">Image from Vasily
Zubarev via their blog</a></p>
</div>
<div class="section level3">
<h3 id="boosting">Boosting<a class="anchor" aria-label="anchor" href="#boosting"></a>
</h3>
<p>This is where we train a single type of Model/estimator on an initial
dataset, test it’s accuracy, and then subsequently train the same type
of models on poorly predicted samples i.e. each new model pays most
attention to data that were incorrectly predicted by the last one.</p>
<p>Just like for bagging, boosting is trained mostly on subsets, however
in this case these subsets are not randomly generated but are instead
built using poorly estimated predictions. Boosting can produce some very
high accuracies by learning from it’s mistakes, but due to the iterative
nature of these improvements it doesn’t parallelize well unlike the
other ensemble methods. Despite this it can still be a faster, and
computationally cheaper alternative to Neural Networks.</p>
<figure><img src="fig/boosting.jpeg" alt="Stacking" class="figure mx-auto d-block"><div class="figcaption">Stacking</div>
</figure><p><a href="https://vas3k.com/blog/machine_learning/" class="external-link">Image from Vasily
Zubarev via their blog</a></p>
</div>
<div class="section level3">
<h3 id="ensemble-summary">Ensemble summary<a class="anchor" aria-label="anchor" href="#ensemble-summary"></a>
</h3>
<p>Machine learning jargon can often be hard to remember, so here is a
quick summary of the 3 ensemble methods:</p>
<ul>
<li>Stacking - same dataset, different models, trained in parallel</li>
<li>Bagging - different subsets, same models, trained in parallel</li>
<li>Boosting - subsets of bad estimates, same models, trained in
series</li>
</ul>
</div>
<div class="section level3">
<h3 id="which-ensemble-method-is-best">Which ensemble method is best?<a class="anchor" aria-label="anchor" href="#which-ensemble-method-is-best"></a>
</h3>
<table class="table">
<colgroup>
<col width="6%">
<col width="32%">
<col width="29%">
<col width="31%">
</colgroup>
<thead><tr class="header">
<th><strong>Ensemble method</strong></th>
<th><strong>What it does</strong></th>
<th><strong>Best for</strong></th>
<th><strong>Avoid if</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Stacking</strong></td>
<td>Combines predictions from different models trained on the same
dataset using a meta-model.</td>
<td>Leveraging diverse models to improve overall performance.</td>
<td>You need simple and fast models or lack diverse base learners.</td>
</tr>
<tr class="even">
<td><strong>Bagging</strong></td>
<td>Trains the same model on different subsets of the data (via
bootstrapping) and averages their results.</td>
<td>Reducing variance (e.g., overfitting) and stabilizing predictions in
noisy/small datasets.</td>
<td>The problem requires reducing bias or the base model is already
stable (e.g., linear regression).</td>
</tr>
<tr class="odd">
<td><strong>Boosting</strong></td>
<td>Sequentially trains models, focusing on correcting errors made by
previous models.</td>
<td>Capturing complex patterns in large datasets and achieving the
highest possible accuracy.</td>
<td>The dataset is small or noisy, or you lack computational
resources.</td>
</tr>
</tbody>
</table>
</div>
<div class="section level2">
<h2 id="using-bagging-random-forests-for-a-classification-problem">Using Bagging (Random Forests) for a classification problem<a class="anchor" aria-label="anchor" href="#using-bagging-random-forests-for-a-classification-problem"></a>
</h2>
<p>In this session we’ll take another look at the penguins data and
applying one of the most common bagging approaches, random forests, to
try and solve our species classification problem. First we’ll load in
the dataset and define a train and test split.</p>
<pre><code># import libraries
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split

# load penguins data
penguins = sns.load_dataset('penguins')

# prepare and define our data and targets
feature_names = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
penguins.dropna(subset=feature_names, inplace=True)

species_names = penguins['species'].unique()

X = penguins[feature_names]
y = penguins.species

# Split data in training and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5, stratify=y)

print("train size:", X_train.shape)
print("test size", X_test.shape)</code></pre>
<p>{: .language-python}</p>
<p>We’ll now take a look how we can use ensemble methods to perform a
classification task such as identifying penguin species! We’re going to
use a Random forest classifier available in scikit-learn which is a
widely used example of a bagging approach.</p>
<p>Random forests are built on decision trees and can provide another
way to address over-fitting. Rather than classifying based on one single
decision tree (which could overfit the data), an average of results of
many trees can be derived for more robust/accurate estimates compared
against single trees used in the ensemble.</p>
<figure><img src="fig/randomforest.png" alt="Random Forests" class="figure mx-auto d-block"><div class="figcaption">Random Forests</div>
</figure><p><a href="https://commons.wikimedia.org/wiki/File:Random_forest_diagram_complete.png" class="external-link">Image
from Venkatak Jagannath</a></p>
<p>We can now define a random forest estimator and train it using the
penguin training data. We have a similar set of attritbutes to the
DecisionTreeClassifier but with an extra parameter called n_estimators
which is the number of trees in the forest.</p>
<pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import plot_tree

# Define our model
# extra parameter called n_estimators which is number of trees in the forest
# a leaf is a class label at the end of the decision tree
forest = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_leaf=1, random_state=0)

# train our model
forest.fit(X_train, y_train)

# Score our model
print(forest.score(X_test, y_test))</code></pre>
<p>{: .language-python}</p>
<p>You might notice that we have a different value (hopefully increased)
compared with the decision tree classifier used above on the same
training data. Lets plot the first 5 trees in the forest to get an idea
of how this model differs from a single decision tree.</p>
<pre><code>import matplotlib.pyplot as plt

fig, axes = plt.subplots(nrows=1, ncols=5 ,figsize=(12,6))

# plot first 5 trees in forest
for index in range(0, 5):
    plot_tree(forest.estimators_[index],
        class_names=species_names,
        feature_names=feature_names,
        filled=True,
        ax=axes[index])

    axes[index].set_title(f'Tree: {index}')

plt.show()</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/rf_5_trees.png" alt="random forest trees" class="figure mx-auto d-block"><div class="figcaption">random forest trees</div>
</figure><p>We can see the first 5 (of 100) trees that were fitted as part of the
forest.</p>
<p>If we train the random forest estimator using the same two parameters
used to plot the classification space for the decision tree classifier
what do we think the plot will look like?</p>
<pre><code># lets train a random forest for only two features (body mass and bill length)
from sklearn.inspection import DecisionBoundaryDisplay
f1 = feature_names[0]
f2 = feature_names[3]

# plot classification space for body mass and bill length with random forest
forest_2d = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_leaf=1, random_state=0)
forest_2d.fit(X_train[[f1, f2]], y_train)

# Lets plot the decision boundaries made by the model for the two trained features
d = DecisionBoundaryDisplay.from_estimator(forest_2d, X_train[[f1, f2]])

sns.scatterplot(X_train, x=f1, y=f2, hue=y_train, palette="husl")
plt.show()</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/EM_rf_clf_space.png" alt="random forest clf space" class="figure mx-auto d-block"><div class="figcaption">random forest clf space</div>
</figure><p>There is still some overfitting indicated by the regions that contain
only single points but using the same hyper-parameter settings used to
fit the decision tree classifier, we can see that overfitting is
reduced.</p>
</div>
<div class="section level2">
<h2 id="stacking-a-regression-problem">Stacking a regression problem<a class="anchor" aria-label="anchor" href="#stacking-a-regression-problem"></a>
</h2>
<p>We’ve had a look at a bagging approach, but we’ll now take a look at
a stacking approach and apply it to a regression problem. We’ll also
introduce a new dataset to play around with.</p>
<div class="section level3">
<h3 id="california-house-price-prediction">California house price prediction<a class="anchor" aria-label="anchor" href="#california-house-price-prediction"></a>
</h3>
<p>The California housing dataset for regression problems contains 8
training features such as, Median Income, House Age, Average Rooms,
Average Bedrooms etc. for 20,640 properties. The target variable is the
median house value for those 20,640 properties, note that all prices are
in units of $100,000. This toy dataset is available as part of the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html" class="external-link">scikit
learn library</a>. We’ll start by loading the dataset to very briefly
inspect the attributes by printing them out.</p>
<pre><code>import sklearn
from sklearn.datasets import fetch_california_housing

# load the dataset
X, y = fetch_california_housing(return_X_y=True, as_frame=True)

## All price variables are in units of $100,000
print(X.shape)
print(X.head())

print("Housing price as the target: ")

## Target is in units of $100,000
print(y.head())
print(y.shape)</code></pre>
<p>{: .language-python}</p>
<p>For the the purposes of learning how to create and use ensemble
methods and since it is a toy dataset, we will blindly use this dataset
without inspecting it, cleaning or pre-processing it further.</p>
<div id="exercise-investigate-and-visualise-the-dataset" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="exercise-investigate-and-visualise-the-dataset" class="callout-inner">
<h3 class="callout-title">Exercise: Investigate and visualise the
dataset</h3>
<div class="callout-content">
<p>For this episode we simply want to learn how to build and use an
Ensemble rather than actually solve a regression problem. To build up
your skills as an ML practitioner, investigate and visualise this
dataset. What can you say about the dataset itself, and what can you
summarise about about any potential relationships or prediction
problems?</p>
</div>
</div>
</div>
<p>Lets start by splitting the dataset into training and testing
subsets:</p>
<pre><code># split into train and test sets, We are selecting an 80%-20% train-test split.
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)

print(f'train size: {X_train.shape}')
print(f'test size: {X_test.shape}')</code></pre>
<p>{: .language-python}</p>
<p>Lets stack a series of regression models. In the same way the
RandomForest classifier derives a results from a series of trees, we
will combine the results from a series of different models in our stack.
This is done using what’s called an ensemble meta-estimator called a
VotingRegressor.</p>
<p>We’ll apply a Voting regressor to a random forest, gradient boosting
and linear regressor.</p>
<p>Lets stack a series of regression models. In the same way the
RandomForest classifier derives a results from a series of trees, we
will combine the results from a series of different models in our stack.
This is done using what’s called an ensemble meta-estimator called a
VotingRegressor.</p>
<p>We’ll apply a Voting regressor to a random forest, gradient boosting
and linear regressor.</p>
<div id="but-wait-arent-random-forestsdecision-tree-for-classification-problems" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="but-wait-arent-random-forestsdecision-tree-for-classification-problems" class="callout-inner">
<h3 class="callout-title">But wait, aren’t random forests/decision tree
for classification problems?</h3>
<div class="callout-content">
<p>Yes they are, but quite often in machine learning various models can
be used to solve both regression and classification problems.</p>
<p>Decision trees in particular can be used to “predict” specific
numerical values instead of categories, essentially by binning a group
of values into a single value.</p>
<p>This works well for periodic/repeating numerical data. These trees
are extremely sensitive to the data they are trained on, which makes
them a very good model to use as a Random Forest.</p>
</div>
</div>
</div>
<div id="but-wait-again-isnt-a-random-forest-and-a-gradient-boosting-model-an-ensemble-method-instead-of-a-regression-model" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="but-wait-again-isnt-a-random-forest-and-a-gradient-boosting-model-an-ensemble-method-instead-of-a-regression-model" class="callout-inner">
<h3 class="callout-title">But wait again, isn’t a random forest (and a
gradient boosting model) an ensemble method instead of a regression
model?</h3>
<div class="callout-content">
<p>Yes they are, but they can be thought of as one big complex model
used like any other model. The awesome thing about ensemble methods, and
the generalisation of Scikit-Learn models, is that you can put an
ensemble in an ensemble!</p>
</div>
</div>
</div>
<p>A VotingRegressor can train several base estimators on the whole
dataset, and it can take the average of the individual predictions to
form a final prediction.</p>
<pre><code>from sklearn.ensemble import (
    GradientBoostingRegressor,
    RandomForestRegressor,
    VotingRegressor,
)
from sklearn.linear_model import LinearRegression

# Initialize estimators
rf_reg = RandomForestRegressor(random_state=5)
gb_reg = GradientBoostingRegressor(random_state=5)
linear_reg = LinearRegression()
voting_reg = VotingRegressor([("rf", rf_reg), ("gb", gb_reg), ("lr", linear_reg)])

# fit/train voting estimator
voting_reg.fit(X_train, y_train)

# lets also fit/train the individual models for comparison
rf_reg.fit(X_train, y_train)
gb_reg.fit(X_train, y_train)
linear_reg.fit(X_train, y_train)</code></pre>
<p>{: .language-python}</p>
<p>We fit the voting regressor in the same way we would fit a single
model. When the voting regressor is instantiated we pass it a parameter
containing a list of tuples that contain the estimators we wish to
stack: in this case the random forest, gradient boosting and linear
regressors. To get a sense of what this is doing lets predict the first
20 samples in the test portion of the data and plot the results.</p>
<pre><code>import matplotlib.pyplot as plt

# make predictions
X_test_20 = X_test[:20] # first 20 for visualisation

rf_pred = rf_reg.predict(X_test_20)
gb_pred = gb_reg.predict(X_test_20)
linear_pred = linear_reg.predict(X_test_20)
voting_pred = voting_reg.predict(X_test_20)

plt.figure()
plt.plot(gb_pred,  "o", color="black", label="GradientBoostingRegressor")
plt.plot(rf_pred,  "o", color="blue", label="RandomForestRegressor")
plt.plot(linear_pred,  "o", color="green", label="LinearRegression")
plt.plot(voting_pred,  "x", color="red", ms=10, label="VotingRegressor")

plt.tick_params(axis="x", which="both", bottom=False, top=False, labelbottom=False)
plt.ylabel("predicted")
plt.xlabel("training samples")
plt.legend(loc="best")
plt.title("Regressor predictions and their average")

plt.show()</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/house_price_voting_regressor.svg" alt="Regressor predictions and average from stack" class="figure mx-auto d-block"><div class="figcaption">Regressor predictions and average from
stack</div>
</figure><p>Finally, lets see how the average compares against each single
estimator in the stack?</p>
<pre><code>print(f'random forest: {rf_reg.score(X_test, y_test)}')

print(f'gradient boost: {gb_reg.score(X_test, y_test)}')

print(f'linear regression: {linear_reg.score(X_test, y_test)}')

print(f'voting regressor: {voting_reg.score(X_test, y_test)}')</code></pre>
<p>{: .language-python}</p>
<p>Each of our models score between 0.61-0.82, which at the high end is
good, but at the low end is a pretty poor prediction accuracy score. Do
note that the toy datasets are not representative of real world data.
However what we can see is that the stacked result generated by the
voting regressor fits different sub-models and then averages the
individual predictions to form a final prediction. The benefit of this
approach is that, it reduces overfitting and increases generalizability.
Of course, we could try and improve our accuracy score by tweaking with
our indivdual model hyperparameters, using more advaced boosted models
or adjusting our training data features and train-test-split data.</p>
<div id="exercise-stacking-a-classification-problem." class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="exercise-stacking-a-classification-problem." class="callout-inner">
<h3 class="callout-title">Exercise: Stacking a classification
problem.</h3>
<div class="callout-content">
<p>Scikit learn also has method for stacking ensemble classifiers
<code>sklearn.ensemble.VotingClassifier</code> do you think you could
apply a stack to the penguins dataset using a random forest, SVM and
decision tree classifier, or a selection of any other classifier
estimators available in sci-kit learn?</p>
<pre><code>penguins = sns.load_dataset('penguins')

feature_names = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
penguins.dropna(subset=feature_names, inplace=True)

species_names = penguins['species'].unique()

# Define data and targets
X = penguins[feature_names]

y = penguins.species

# Split data in training and test set
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)

print(f'train size: {X_train.shape}')
print(f'test size: {X_test.shape}')</code></pre>
<p>{: .language.python}</p>
<p>The code above loads the penguins data and splits it into test and
training portions. Have a play around with stacking some classifiers
using the <code>sklearn.ensemble.VotingClassifier</code> using the code
comments below as a guide.</p>
<pre><code><span><span class="co"># import classifiers</span></span>
<span></span>
<span><span class="co"># instantiate classifiers</span></span>
<span></span>
<span><span class="co"># fit classifiers</span></span>
<span></span>
<span><span class="co"># instantiate voting classifier and fit data</span></span>
<span></span>
<span><span class="co"># make predictions</span></span>
<span></span>
<span><span class="co"># compare scores</span></span></code></pre>
<p>{: .language.python}</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Ensemble methods can be used to reduce under/over fitting training
data.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div>
</div></section><section id="aio-05-clustering"><p>Content from <a href="05-clustering.html">Unsupervised methods - Clustering</a></p>
<hr>
<p>Last updated on 2025-11-07 |

        <a href="https://github.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/edit/main/episodes/05-clustering.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is unsupervised learning?</li>
<li>How can we use clustering to find data points with similar
attributes?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the difference between supervised and unsupervised
learning</li>
<li>Identify clusters in data using k-means clustering.</li>
<li>Understand the limitations of k-means when clusters overlap.</li>
<li>Use spectral clustering to overcome the limitations of k-means.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="unsupervised-learning">Unsupervised learning<a class="anchor" aria-label="anchor" href="#unsupervised-learning"></a>
</h1>
<p>In episode 2 we learnt about supervised learning. Now it is time to
explore unsupervised learning.</p>
<p>Sometimes we do not have the luxury of using labelled data. This
could be for a number of reasons:</p>
<ul>
<li>We have labelled data, but not enough to accurately train our
model</li>
<li>Our existing labelled data is low-quality or innacurate</li>
<li>It is too time-consuming to (manually) label more data</li>
<li>We have data, but no idea what correlations might exist that we
could model!</li>
</ul>
<p>In this case we need to use unsupervised learning. As the name
suggests, this time we do not “supervise” the ML algorithm by providing
it labels, but instead we let it try to find its own patterns in the
data and report back on any correlations that it might find. You can
think of unsupervised learning as a way to discover labels from the data
itself.</p>
</div>
<div class="section level1">
<h1 id="clustering">Clustering<a class="anchor" aria-label="anchor" href="#clustering"></a>
</h1>
<p>Clustering is the grouping of data points which are similar to each
other. It can be a powerful technique for identifying patterns in data.
Clustering analysis does not usually require any training and is
therefore known as an unsupervised learning technique. Clustering can be
applied quickly due to this lack of training.</p>
<div class="section level2">
<h2 id="applications-of-clustering">Applications of clustering<a class="anchor" aria-label="anchor" href="#applications-of-clustering"></a>
</h2>
<ul>
<li>Looking for trends in data</li>
<li>Reducing the data around a point to just that point (e.g. reducing
colour depth in an image)</li>
<li>Pattern recognition</li>
</ul>
</div>
<div class="section level2">
<h2 id="k-means-clustering">K-means clustering<a class="anchor" aria-label="anchor" href="#k-means-clustering"></a>
</h2>
<p>The k-means clustering algorithm is a simple clustering algorithm
that tries to identify the centre of each cluster. It does this by
searching for a point which minimises the distance between the centre
and all the points in the cluster. The algorithm needs to be told how
many k clusters to look for, but a common technique is to try different
numbers of clusters and combine it with other tests to decide on the
best combination.</p>
<div id="hyper-parameters-again" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="hyper-parameters-again" class="callout-inner">
<h3 class="callout-title">Hyper-parameters again</h3>
<div class="callout-content">
<p>‘K’ is also an exmaple of a <em>hyper-parameter</em> for the k-means
clustering technique. Another example of a hyper-parameter is the
N-degrees of freedom for polynomial regression. Keep an eye out for
others throughout the lesson!</p>
</div>
</div>
</div>
<div class="section level3">
<h3 id="k-means-with-scikit-learn">K-means with Scikit-Learn<a class="anchor" aria-label="anchor" href="#k-means-with-scikit-learn"></a>
</h3>
<p>To perform a k-means clustering with Scikit-Learn we first need to
import the sklearn.cluster module.</p>
<pre><code>import sklearn.cluster as skl_cluster</code></pre>
<p>{: .language-python}</p>
<p>For this example, we’re going to use Scikit-Learn’s built-in ‘random
data blob generator’ instead of using an external dataset. Therefore
we’ll need the <code>sklearn.datasets.samples_generator</code>
module.</p>
<pre><code>import sklearn.datasets as skl_datasets</code></pre>
<p>{: .language-python}</p>
<p>Now lets create some random blobs using the <code>make_blobs</code>
function. The <code>n_samples</code> argument sets how many points we
want to use in all of our blobs while <code>cluster_std</code> sets the
standard deviation of the points. The smaller this value the closer
together they will be. <code>centers</code> sets how many clusters we’d
like. <code>random_state</code> is the initial state of the random
number generator. By specifying this value we’ll get the same results
every time we run the program. If we don’t specify a random state then
we’ll get different points every time we run. This function returns two
things: an array of data points and a list of which cluster each point
belongs to.</p>
<pre><code>import matplotlib.pyplot as plt

def plots_labels(data, labels):
    """
    Visualizes data points with associated labels in a 2D scatter plot.

    Parameters:
    data (ndarray): A 2D NumPy array with shape (n_samples, 2), representing the data points.
    labels (ndarray or list): A 1D array or list of labels corresponding to the data points.

    Returns:
    None: Displays the scatter plot with labels as colors.
    """
    # Extract the x and y coordinates from the data
    tx = data[:, 0]
    ty = data[:, 1]

    # Create a figure with a specified size
    fig = plt.figure(1, figsize=(4, 4))

    # Scatter plot the data points, coloring them by their labels
    plt.scatter(tx, ty, edgecolor='k', c=labels)

    # Display the plot
    plt.show()

def plot_clusters(data, clusters, Kmean):
    """
    Visualizes clustered data points with centroids marked.

    Parameters:
    data (ndarray): A 2D NumPy array with shape (n_samples, 2), representing the data points.
    clusters (ndarray or list): A 1D array or list of cluster assignments for each data point.
    Kmean (KMeans object): The fitted KMeans object containing cluster centers.

    Returns:
    None: Displays the scatter plot with clusters as colors and centroids marked with red crosses.
    """
    # Extract the x and y coordinates from the data
    tx = data[:, 0]
    ty = data[:, 1]

    # Create a figure with a specified size
    fig = plt.figure(1, figsize=(4, 4))

    # Scatter plot the data points, coloring them by their cluster assignment
    # plt.scatter(tx, ty, s=5, linewidth=0, c=clusters)
    plt.scatter(tx, ty, c=clusters, cmap="nipy_spectral", edgecolor='k')

    # Loop through cluster centers and mark them with a red 'x'
    for cluster_x, cluster_y in Kmean.cluster_centers_:
        plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='x')

    # Display the plot
    plt.show()</code></pre>
<p>{: .language-python}</p>
<p>Lets create the clusters.</p>
<pre><code>N_true_clusters = 4
data, cluster_id = skl_datasets.make_blobs(n_samples=400, cluster_std=0.75, centers=N_true_clusters, random_state=1)
plots_labels(data, cluster_id)</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/random_clusters.png" alt="Plot of the random clusters" class="figure mx-auto d-block"><div class="figcaption">Plot of the random clusters</div>
</figure><p>Now that we have some data we can try to identify the clusters using
k-means. First, we need to initialise the KMeans module and tell it how
many clusters to look for. Next, we supply it with some data via the
<code>fit</code> function, in much the same way we did with the
regression functions earlier on. Finally, we run the predict function to
find the clusters.</p>
<pre><code><span><span class="va">N_pred_clusters</span> <span class="op">=</span> <span class="fl">4</span></span>
<span><span class="va">Kmean</span> <span class="op">=</span> <span class="fu">skl_cluster.KMeans</span><span class="op">(</span>n_clusters<span class="op">=</span><span class="va">N_pred_clusters</span><span class="op">)</span></span>
<span><span class="fu">Kmean.fit</span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span><span class="va">clusters</span> <span class="op">=</span> <span class="fu">Kmean.predict</span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<p>The data can now be plotted to show all the points we randomly
generated. To make it clearer which cluster points have been classified
we can set the colours (the c parameter) to use the
<code>clusters</code> list that was returned by the <code>predict</code>
function. The Kmeans algorithm also lets us know where it identified the
centre of each cluster. These are stored as a list called
‘cluster_centers_’ inside the <code>Kmean</code> object. Let’s plot the
points from the clusters, colouring them by the output from the K-means
algorithm, and also plot the centres of each cluster as a red X.</p>
<pre><code><span><span class="fu">plot_clusters</span><span class="op">(</span><span class="va">data</span>, <span class="va">clusters</span>, <span class="va">Kmean</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/random_clusters_centre.png" alt="Plot of the fitted random clusters" class="figure mx-auto d-block"><div class="figcaption">Plot of the fitted random clusters</div>
</figure><p>Here is the code all in a single block.</p>
<pre><code>import sklearn.cluster as skl_cluster
import sklearn.datasets as skl_datasets
import matplotlib.pyplot as plt

data, cluster_id = skl_datasets.make_blobs(n_samples=400, cluster_std=0.75, centers=4, random_state=1)

Kmean = skl_cluster.KMeans(n_clusters=4)
Kmean.fit(data)
clusters = Kmean.predict(data)

plot_clusters(data, clusters, Kmean)</code></pre>
<p>{: .language-python}</p>
<div id="working-in-multiple-dimensions" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="working-in-multiple-dimensions" class="callout-inner">
<h3 class="callout-title">Working in multiple dimensions</h3>
<div class="callout-content">
<p>Although this example shows two dimensions, the kmeans algorithm can
work in more than two. It becomes very difficult to show this visually
once we get beyond 3 dimensions. Its very common in machine learning to
be working with multiple variables and so our classifiers are working in
multi-dimensional spaces.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="assessing-cluster-quality-with-the-silhouette-score">Assessing cluster quality with the silhouette score<a class="anchor" aria-label="anchor" href="#assessing-cluster-quality-with-the-silhouette-score"></a>
</h3>
<p>Evaluating the quality of clusters is a crucial step in clustering
analysis, as it helps determine how well the data points fit into their
assigned clusters. A widely used metric for this purpose is the
<strong>silhouette score</strong>, which measures how similar a data
point is to its own cluster compared to other clusters. The silhouette
score is defined for each data point and ranges from -1 to 1, where:</p>
<ul>
<li>
<strong>1</strong> indicates the data point is well matched to its
cluster and poorly matched to other clusters.</li>
<li>
<strong>0</strong> indicates the data point is on or very close to
the decision boundary between clusters.</li>
<li>
<strong>-1</strong> indicates the data point may have been
misclassified into the wrong cluster.</li>
</ul>
<p>The silhouette score can be averaged across all data points to
provide an overall measure of clustering quality. Additionally,
examining silhouette scores for individual samples can help identify
outliers or problematic clusters.</p>
<p>Here is the Python code to calculate both the overall silhouette
score and the individual sample scores:</p>
<pre><code>from sklearn.metrics import silhouette_score, silhouette_samples

# Calculate the overall silhouette score
overall_silhouette = silhouette_score(data, clusters)
print(f"Overall Silhouette Score: {overall_silhouette:.2f}")

# Calculate silhouette scores for individual samples
sample_silhouettes = silhouette_samples(data, clusters)
sample_silhouettes</code></pre>
<p>{: .language-python}</p>
<div id="exercise-how-many-clusters-should-we-look-for" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="exercise-how-many-clusters-should-we-look-for" class="callout-inner">
<h3 class="callout-title">Exercise: How many clusters should we look
for?</h3>
<div class="callout-content">
<p>Using k-means requires us to specify the number of clusters to
expect. A common strategy to get around this is to vary the number of
clusters we are looking for, and use the silhouette score to select the
most appropriate number of clusters. Use the code below to loop through
searching for between 2 and 10 clusters, generating silhouette plots for
each. Which (if any) of the results look more sensible? What criteria
might you use to select the best one?</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np

def plot_silhouette(data, clusters):
    """
    Calculates and plots silhouette scores for clustering results.

    Parameters:
    - data: array-like of shape (n_samples, n_features)
        Feature matrix of the dataset.
    - clusters: array-like of shape (n_samples,)
        Cluster labels for each sample in the dataset.

    Returns:
    - overall_silhouette: float
        The overall silhouette score for the clustering result.
    """
    # Calculate the overall silhouette score
    overall_silhouette = silhouette_score(data, clusters)
    print(f"Overall Silhouette Score: {overall_silhouette:.2f}")

    # Calculate silhouette scores for individual samples
    sample_silhouettes = silhouette_samples(data, clusters)

    # Plot silhouette values for each cluster
    y_lower = 10
    n_clusters = len(np.unique(clusters))

    for i in range(n_clusters):  # Iterate over each cluster
        cluster_silhouettes = sample_silhouettes[clusters == i]
        cluster_silhouettes.sort()
        cluster_size = len(cluster_silhouettes)
        y_upper = y_lower + cluster_size

        plt.fill_betweenx(
            np.arange(y_lower, y_upper),
            0,
            cluster_silhouettes,
            alpha=0.7
        )
        plt.text(-0.05, y_lower + 0.5 * cluster_size, str(i))
        y_lower = y_upper + 10

    plt.xlabel("Silhouette Coefficient")
    plt.ylabel("Cluster")
    plt.title("Silhouette Analysis")
    # Set x-axis limits
    plt.xlim([-.2, 1])
    plt.show()

    return overall_silhouette
</code></pre>
<p>{: .language-python}</p>
<pre><code>for cluster_count in range(2,11):
    Kmean = skl_cluster.KMeans(n_clusters=cluster_count)
    Kmean.fit(data)
    clusters = Kmean.predict(data)
    plot_silhouette(data, clusters)</code></pre>
<p>{: .language-python}</p>
<div class="section level2">
<h2 id="solution">Solution<a class="anchor" aria-label="anchor" href="#solution"></a>
</h2>
<p>The silouette score, unfortunately, incorrectly identifies N=2 as the
most approprirate clustering configuration in this case (silhouette =
0.73). However, the silhouette score for N=4 (true cluster number) is
very close (silhouette = 0.72). The silhouette can act as a useful guide
in selecting cluster number, but it doesn’t always produce perfect
results. Clustering with different feature sets or exploring different
clustering algorithms may yield better results.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="limitations-of-k-means">Limitations of k-means<a class="anchor" aria-label="anchor" href="#limitations-of-k-means"></a>
</h3>
<ul>
<li>Requires number of clusters to be known in advance</li>
<li>Struggles when clusters have irregular shapes</li>
<li>Will always produce an answer finding the required number of
clusters even if the data isn’t clustered (or clustered in that many
clusters)</li>
<li>Requires linear cluster boundaries</li>
</ul>
<figure><img src="fig/kmeans_concentric_circle.png" alt="An example of kmeans failing on non-linear cluster boundaries" class="figure mx-auto d-block"><div class="figcaption">An example of kmeans failing on non-linear
cluster boundaries</div>
</figure>
</div>
<div class="section level3">
<h3 id="advantages-of-k-means">Advantages of k-means<a class="anchor" aria-label="anchor" href="#advantages-of-k-means"></a>
</h3>
<ul>
<li>Simple algorithm and fast to compute</li>
<li>A good choice as the first thing to try when attempting to cluster
data</li>
<li>Suitable for large datasets due to its low memory and computing
requirements</li>
</ul>
<div id="exercise-k-means-with-overlapping-clusters" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="exercise-k-means-with-overlapping-clusters" class="callout-inner">
<h3 class="callout-title">Exercise: K-Means with overlapping
clusters</h3>
<div class="callout-content">
<p>Adjust the program above to increase the standard deviation of the
blobs (the cluster_std parameter to make_blobs) and increase the number
of samples (n_samples) to 4000. You should start to see the clusters
overlapping. Do the clusters that are identified make sense? Is there
any strange behaviour?</p>
<div class="section level2">
<h2 id="solution-1">Solution<a class="anchor" aria-label="anchor" href="#solution-1"></a>
</h2>
<p>Increasing n_samples to 4000 and cluster_std to 3.0 looks like this:
<img src="fig/kmeans_overlapping_clusters.png" alt="Kmeans attempting to classify overlapping clusters" class="figure"> The straight
line boundaries between clusters look a bit strange.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="spectral-clustering">Spectral clustering<a class="anchor" aria-label="anchor" href="#spectral-clustering"></a>
</h2>
<p>Spectral clustering is a technique that attempts to overcome the
linear boundary problem of k-means clustering. It works by treating
clustering as a graph partitioning problem and looks for nodes in a
graph with a small distance between them. See <a href="https://www.cvl.isy.liu.se/education/graduate/spectral-clustering/SC_course_part1.pdf" class="external-link">this</a>
introduction to spectral clustering if you are interested in more
details about how spectral clustering works.</p>
<p>Here is an example of spectral clustering on two concentric
circles:</p>
<figure><img src="fig/spectral_concentric_circle.png" alt="Spectral clustering on two concentric circles" class="figure mx-auto d-block"><div class="figcaption">Spectral clustering on two concentric
circles</div>
</figure><p>Spectral clustering uses something called a ‘kernel trick’ to
introduce additional dimensions to the data. A common example of this is
trying to cluster one circle within another (concentric circles). A
k-means classifier will fail to do this and will end up effectively
drawing a line which crosses the circles. However spectral clustering
will introduce an additional dimension that effectively moves one of the
circles away from the other in the additional dimension. This does have
the downside of being more computationally expensive than k-means
clustering.</p>
<figure><img src="fig/spectral_concentric_3d.png" alt="Spectral clustering viewed with an extra dimension" class="figure mx-auto d-block"><div class="figcaption">Spectral clustering viewed with an extra
dimension</div>
</figure><div class="section level3">
<h3 id="spectral-clustering-with-scikit-learn">Spectral clustering with Scikit-Learn<a class="anchor" aria-label="anchor" href="#spectral-clustering-with-scikit-learn"></a>
</h3>
<p>Lets try out using Scikit-Learn’s spectral clustering. To make the
concentric circles in the above example we need to use the
<code>make_circles</code> function in the sklearn.datasets module. This
works in a very similar way to the make_blobs function we used earlier
on.</p>
<pre><code>import sklearn.datasets as skl_data

circles, circles_clusters = skl_data.make_circles(n_samples=400, noise=.01, random_state=0)
plots_labels(circles, circles_clusters)</code></pre>
<p>{: .language-python}</p>
<p>The code for calculating the SpectralClustering is very similar to
the kmeans clustering, but instead of using the sklearn.cluster.KMeans
class we use the <code>sklearn.cluster.SpectralClustering</code> class.
~~~ model = skl_cluster.SpectralClustering(n_clusters=2,
affinity=‘nearest_neighbors’, assign_labels=‘kmeans’) ~~~ {:
.language-python}</p>
<p>The SpectralClustering class combines the fit and predict functions
into a single function called fit_predict.</p>
<pre><code><span><span class="va">labels</span> <span class="op">=</span> <span class="fu">model.fit_predict</span><span class="op">(</span><span class="va">circles</span><span class="op">)</span></span>
<span><span class="fu">plots_labels</span><span class="op">(</span><span class="va">circles</span>, <span class="va">labels</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<p>Here is the whole program combined with the kmeans clustering for
comparison. Note that this produces two figures. To view both of them
use the “Inline” graphics terminal inside the Python console instead of
the “Automatic” method which will open a window and only show you one of
the graphs.</p>
<pre><code>import sklearn.cluster as skl_cluster
import sklearn.datasets as skl_data

circles, circles_clusters = skl_data.make_circles(n_samples=400, noise=.01, random_state=0)

# cluster with kmeans
Kmean = skl_cluster.KMeans(n_clusters=2)
Kmean.fit(circles)
clusters = Kmean.predict(circles)

# plot the data, colouring it by cluster
plot_clusters(circles, clusters, Kmean)

# cluster with spectral clustering
model = skl_cluster.SpectralClustering(n_clusters=2, affinity='nearest_neighbors', assign_labels='kmeans')
labels = model.fit_predict(circles)
plots_labels(circles, labels)</code></pre>
<p>{: .language-python}</p>
<p><img src="fig/kmeans_concentric_circle_2.png" alt="Kmeans attempting to cluster the concentric circles" class="figure"><img src="fig/spectral_concentric_circle_2.png" alt="Spectral clustering on the concentric circles" class="figure"></p>
<div id="comparing-k-means-and-spectral-clustering-performance" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="comparing-k-means-and-spectral-clustering-performance" class="callout-inner">
<h3 class="callout-title">Comparing k-means and spectral clustering
performance</h3>
<div class="callout-content">
<p>Modify the program we wrote in the previous exercise to use spectral
clustering instead of k-means and save it as a new file. Time how long
both programs take to run. Add the line <code>import time</code> at the
top of both files as the first line, and get the start time with
<code>start_time = time.time()</code>. End the program by getting the
time again and subtracting the start time from it to get the total run
time. Add <code>end_time = time.time()</code> and
<code>print("Elapsed time:",end_time-start_time,"seconds")</code> to the
end of both files. Compare how long both programs take to run generating
4,000 samples and testing them for between 2 and 10 clusters. How much
did your run times differ? How much do they differ if you increase the
number of samples to 8,000? How long do you think it would take to
compute 800,000 samples (estimate this, it might take a while to run for
real)? ## Solution KMeans version: runtime around 4 seconds (your
computer might be faster/slower) ~~~ import matplotlib.pyplot as plt
import sklearn.cluster as skl_cluster from sklearn.datasets import
make_blobs import time</p>
<p>start_time = time.time() data, cluster_id =
make_blobs(n_samples=4000, cluster_std=3, centers=4, random_state=1)</p>
<p>for cluster_count in range(2,11): Kmean =
skl_cluster.KMeans(n_clusters=cluster_count) Kmean.fit(data) clusters =
Kmean.predict(data)</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="ex">plt.scatter</span><span class="er">(</span><span class="va">data</span><span class="op">[</span>:, 0<span class="op">]</span><span class="ex">,</span> data[:, 1], s=15, linewidth=0, c=clusters<span class="kw">)</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="ex">plt.title</span><span class="er">(</span><span class="ex">str</span><span class="er">(</span><span class="ex">cluster_count</span><span class="kw">)</span><span class="ex">+</span><span class="st">" Clusters"</span><span class="kw">)</span></span></code></pre>
</div>
<p>plt.show()</p>
<p>end_time = time.time() print(“Elapsed time =”, end_time-start_time,
“seconds”) ~~~ {: .language-python}</p>
<p>Spectral version: runtime around 9 seconds (your computer might be
faster/slower) ~~~ import matplotlib.pyplot as plt import
sklearn.cluster as skl_cluster from sklearn.datasets import make_blobs
import time</p>
<p>start_time = time.time() data, cluster_id =
make_blobs(n_samples=4000, cluster_std=3, centers=4, random_state=1)</p>
<p>for cluster_count in range(2,11): model =
skl_cluster.SpectralClustering(n_clusters=cluster_count,
affinity=‘nearest_neighbors’, assign_labels=‘kmeans’) labels =
model.fit_predict(data)</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="ex">plt.scatter</span><span class="er">(</span><span class="va">data</span><span class="op">[</span>:, 0<span class="op">]</span><span class="ex">,</span> data[:, 1], s=15, linewidth=0, c=labels<span class="kw">)</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="ex">plt.title</span><span class="er">(</span><span class="ex">str</span><span class="er">(</span><span class="ex">cluster_count</span><span class="kw">)</span><span class="ex">+</span><span class="st">" Clusters"</span><span class="kw">)</span></span></code></pre>
</div>
<p>plt.show() end_time = time.time() print(“Elapsed time =”,
end_time-start_time, “seconds”) ~~~ {: .language-python}</p>
<p>When the number of points increases to 8000 the runtimes are 24
seconds for the spectral version and 5.6 seconds for kmeans. The runtime
numbers will differ depending on the speed of your computer, but the
relative difference should be similar. For 4000 points kmeans took 4
seconds, while spectral took 9 seconds. A 2.25 fold difference. For 8000
points kmeans took 5.6 seconds, while spectral took 24 seconds. A 4.28
fold difference. Kmeans is 1.4 times slower for double the data, while
spectral is 2.6 times slower. The realative difference is diverging. If
we used 100 times more data we might expect a 100 fold divergence in
execution times. Kmeans might take a few minutes while spectral will
take hours.</p>
</div>
</div>
</div>
<p>{% include links.md %}</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Clustering is a form of unsupervised learning.</li>
<li>Unsupervised learning algorithms don’t need training.</li>
<li>Kmeans is a popular clustering algorithm.</li>
<li>Kmeans is less useful when one cluster exists within another, such
as concentric circles.</li>
<li>Spectral clustering can overcome some of the limitations of
Kmeans.</li>
<li>Spectral clustering is much slower than Kmeans.</li>
<li>Scikit-Learn has functions to create example data.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div>
</div></section><section id="aio-06-dimensionality-reduction"><p>Content from <a href="06-dimensionality-reduction.html">Unsupervised methods - Dimensionality reduction</a></p>
<hr>
<p>Last updated on 2025-11-07 |

        <a href="https://github.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/edit/main/episodes/06-dimensionality-reduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do we apply machine learning techniques to data with higher
dimensions?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Recall that most data is inherently multidimensional.</li>
<li>Understand that reducing the number of dimensions can simplify
modelling and allow classifications to be performed.</li>
<li>Apply Principle Component Analysis (PCA) and t-distributed
Stochastic Neighbor Embedding (t-SNE) to reduce the dimensions of
data.</li>
<li>Evaluate the relative peformance of PCA and t-SNE in reducing data
dimensionality.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="dimensionality-reduction">Dimensionality reduction<a class="anchor" aria-label="anchor" href="#dimensionality-reduction"></a>
</h1>
<p>As seen in the last episode, general clustering algorithms work well
with low-dimensional data. In this episode we see how higher-dimensional
data, such as images of handwritten text or numbers, can be processed
with dimensionality reduction techniques to make the datasets more
accessible for other modelling techniques. The dataset we will be using
is the Scikit-Learn subset of the Modified National Institute of
Standards and Technology (MNIST) dataset.</p>
<figure><img src="fig/MnistExamples.png" alt="MNIST example illustrating all the classes in the dataset" class="figure mx-auto d-block"><div class="figcaption">MNIST example illustrating all the classes in
the dataset</div>
</figure><p>The MNIST dataset contains 70,000 images of handwritten numbers, and
are labelled from 0-9 with the number that each image contains. Each
image is a greyscale and 28x28 pixels in size for a total of 784 pixels
per image. Each pixel can take a value between 0-255 (8bits). When
dealing with a series of images in machine learning we consider each
pixel to be a feature that varies according to each of the sample
images. Our previous penguin dataset only had no more than 7 features to
train with, however even a small 28x28 MNIST image has as much as 784
features (pixels) to work with.</p>
<figure><img src="fig/mnist_30000-letter.png" alt="MNIST example of a single image" class="figure mx-auto d-block"><div class="figcaption">MNIST example of a single image</div>
</figure><p>To make this episode a bit less computationally intensive, the
Scikit-Learn example that we will work with is a smaller sample of 1797
images. Each image is 8x8 in size for a total of 64 pixels per image,
resulting in 64 features for us to work with. The pixels can take a
value between 0-15 (4bits). Let’s retrieve and inspect the Scikit-Learn
dataset with the following code:</p>
<pre><code>
# Let's define these here to avoid repetitive code
import numpy as np
import matplotlib.pyplot as plt

def plots_labels(data, labels):
    """
    Visualizes data points with associated labels in a 2D scatter plot.

    Parameters:
    data (ndarray): A 2D NumPy array with shape (n_samples, 2), representing the data points.
    labels (ndarray or list): A 1D array or list of labels corresponding to the data points.

    Returns:
    None: Displays the scatter plot with labels as colors.
    """
    # Extract the x and y coordinates from the data
    tx = data[:, 0]
    ty = data[:, 1]

    # Create a figure with a specified size
    fig = plt.figure(1, figsize=(4, 4))

    # Scatter plot the data points, coloring them by their labels
    plt.scatter(tx, ty, edgecolor='k', c=labels)

    # Display the plot
    plt.show()

def plot_clusters(data, clusters, Kmean):
    """
    Visualizes clustered data points with centroids marked.

    Parameters:
    data (ndarray): A 2D NumPy array with shape (n_samples, 2), representing the data points.
    clusters (ndarray or list): A 1D array or list of cluster assignments for each data point.
    Kmean (KMeans object): The fitted KMeans object containing cluster centers.

    Returns:
    None: Displays the scatter plot with clusters as colors and centroids marked with red crosses.
    """
    # Extract the x and y coordinates from the data
    tx = data[:, 0]
    ty = data[:, 1]

    # Create a figure with a specified size
    fig = plt.figure(1, figsize=(4, 4))

    # Scatter plot the data points, coloring them by their cluster assignment
    # plt.scatter(tx, ty, s=5, linewidth=0, c=clusters)
    plt.scatter(tx, ty, c=clusters, cmap="nipy_spectral", edgecolor='k')

    # Loop through cluster centers and mark them with a red 'x'
    for cluster_x, cluster_y in Kmean.cluster_centers_:
        plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='x')

    # Display the plot
    plt.show()

def plot_clusters_labels(data, labels):
    """
    Visualizes data points with associated labels using a color map and displays a legend.

    Parameters:
    data (ndarray): A 2D NumPy array with shape (n_samples, 2), representing the data points.
    labels (ndarray or list): A 1D array or list of labels corresponding to the data points.

    Returns:
    None: Displays the scatter plot with labels as colors and a color bar for the label legend.
    """
    # Extract the x and y coordinates from the data
    tx = data[:, 0]
    ty = data[:, 1]

    # Create a figure with a specified size
    fig = plt.figure(1, figsize=(5, 4))

    # Scatter plot the data points, coloring them by their labels and using a colormap
    plt.scatter(tx, ty, c=labels, cmap="nipy_spectral",
                edgecolor='k', label=labels)

    # Add a color bar to show the label legend
    plt.colorbar(boundaries=np.arange(11) - 0.5).set_ticks(np.arange(10))

    # Display the plot
    plt.show()
</code></pre>
<p>{: .language-python}</p>
<p>Next lets load in the digits dataset, ~~~ from sklearn import
datasets</p>
</div>
<div class="section level1">
<h1 id="load-in-dataset-as-a-pandas-dataframe-return-x-and-y">load in dataset as a Pandas Dataframe, return X and Y<a class="anchor" aria-label="anchor" href="#load-in-dataset-as-a-pandas-dataframe-return-x-and-y"></a>
</h1>
<p>features, labels = datasets.load_digits(return_X_y=True,
as_frame=True)</p>
<p>print(features.shape, labels.shape) print(labels) features.head() ~~~
{: .language-python}</p>
<div class="section level2">
<h2 id="preview-data">Preview data<a class="anchor" aria-label="anchor" href="#preview-data"></a>
</h2>
<p>Each image is 8x8 pixels. We can revert the current “flattened” (aka
vectorized) form by using reshape. The -1 tells NumPy to infer the
appropriate number of rows automatically. NumPy does this by dividing
the total number of elements in the original 1D array (image_1D.size) by
8 (the specified number of columns). ~~~ print(features.iloc[0])
image_1D = features.iloc[0] image_2D =
np.array(image_1D).reshape(-1,8)</p>
<p>plt.imshow(image_2D,cmap=“gray_r”) ~~~ {: .language-python}</p>
</div>
<div class="section level2">
<h2 id="our-goal-using-dimensionality-reduction-to-help-with-machine-learning">Our goal: using dimensionality-reduction to help with machine
learning<a class="anchor" aria-label="anchor" href="#our-goal-using-dimensionality-reduction-to-help-with-machine-learning"></a>
</h2>
<p>As humans we are pretty good at object and pattern recognition. We
can look at the images above, inspect the intensity and position pixels
relative to other pixels, and pretty quickly make an accurate guess at
what the image shows. As humans we spends much of our younger lives
learning these spatial relations, and so it stands to reason that
computers can also extract these relations. Let’s see if it is possible
to use unsupervised clustering techniques to pull out relations in our
MNIST dataset of number images.</p>
<div id="exercise-try-to-visually-inspect-the-dataset-and-features-for-correlations" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="exercise-try-to-visually-inspect-the-dataset-and-features-for-correlations" class="callout-inner">
<h3 class="callout-title">Exercise: Try to visually inspect the dataset
and features for correlations</h3>
<div class="callout-content">
<p>As we did for previous datasets, lets visually inspect relationships
between our features/pixels. Try and investigate the following pixels
for relations (written “row_column”): 0_4, 1_4, 2_4, and 3_4.</p>
<div class="section level2">
<h2 id="solution">Solution<a class="anchor" aria-label="anchor" href="#solution"></a>
</h2>
<pre><code>
print(features.iloc[0])
image_1D = features.iloc[0]
image_2D = np.array(image_1D).reshape(-1,8)

plt.imshow(image_2D,cmap="gray_r")
# these points are the pixels we will investigate
# pixels 0,1,2,3 of row 4 of the image
plt.plot([0,1,2,3],[4,4,4,4],"rx")
plt.show()</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/mnist_pairplot_pixels.png" alt="SKLearn image with highlighted pixels" class="figure mx-auto d-block"><div class="figcaption">SKLearn image with highlighted pixels</div>
</figure><pre><code>import seaborn as sns

# make a temporary copy of data for plotting here only
seaborn_data = features

# add labels for pairplot color coding
seaborn_data["labels"] = labels

# make a short list of N features for plotting N*N figures
# 4**2 = 16 plots, whereas 64**2 is over 4000!
feature_subset = []
for i in range(4):
    feature_subset.append("pixel_"+str(i)+"_4")

sns.pairplot(seaborn_data, vars=feature_subset, hue="labels",
             palette=sns.mpl_palette("Spectral", n_colors=10))</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/mnist_pairplot.png" alt="SKLearn image with highlighted pixels" class="figure mx-auto d-block"><div class="figcaption">SKLearn image with highlighted pixels</div>
</figure><p>As we can see the dataset relations are far more complex than our
previous examples. The histograms show that some numbers appear in those
pixel positions more than others, but the
<code>feature_vs_feature</code> plots are quite messy to try and
decipher. There are gaps and patches of colour suggesting that there is
some kind of structure there, but it’s far harder to inspect than the
penguin data. We can’t easily see definitive clusters in our 2D
representations, and we know our clustering algorithms will take a long
time to try and crunch 64 dimensions at once, so let’s see if we can
represent our 64D data in fewer dimensions.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="dimensionality-reduction-with-scikit-learn">Dimensionality reduction with Scikit-Learn<a class="anchor" aria-label="anchor" href="#dimensionality-reduction-with-scikit-learn"></a>
</h1>
<p>We will look at two commonly used techniques for dimensionality
reduction: Principal Component Analysis (PCA) and t-distributed
Stochastic Neighbor Embedding (t-SNE). Both of these techniques are
supported by Scikit-Learn.</p>
<div class="section level3">
<h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)<a class="anchor" aria-label="anchor" href="#principal-component-analysis-pca"></a>
</h3>
<p>PCA allows us to replace our 64 features with a smaller number of
dimensional representations that retain the majority of our
variance/relational data. Using Scikit-Learn lets apply PCA in a
relatively simple way.</p>
<p>For more in depth explanations of PCA please see the following links:
* <a href="https://builtin.com/data-science/step-step-explanation-principal-component-analysis" class="external-link">https://builtin.com/data-science/step-step-explanation-principal-component-analysis</a>
* <a href="https://scikit-learn.org/stable/modules/decomposition.html#pca" class="external-link">https://scikit-learn.org/stable/modules/decomposition.html#pca</a></p>
<p>Let’s apply PCA to the MNIST dataset and retain the two most-major
components:</p>
<pre><code># PCA with 2 components
from sklearn import decomposition
pca = decomposition.PCA(n_components=2)
x_pca = pca.fit_transform(features)

print(x_pca.shape)</code></pre>
<p>{: .language-python}</p>
<p>This returns us an array of 1797x2 where the 2 remaining columns(our
new “features” or “dimensions”) contain vector representations of the
first principle components (column 0) and second principle components
(column 1) for each of the images. We can plot these two new features
against each other:</p>
<pre><code><span><span class="co"># We are passing None becuase it is an unlabelled plot</span></span>
<span><span class="fu">plots_labels</span><span class="op">(</span><span class="va">x_pca</span>, <span class="va">None</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/pca_unlabelled.png" alt="Reduction using PCA" class="figure mx-auto d-block"><div class="figcaption">Reduction using PCA</div>
</figure><p>We now have a 2D representation of our 64D dataset that we can work
with instead. Let’s try some quick K-means clustering on our 2D
representation of the data. Because we already have some knowledge about
our data we can set <code>k=10</code> for the 10 digits present in the
dataset.</p>
<pre><code>import sklearn.cluster as skl_cluster
Kmean = skl_cluster.KMeans(n_clusters=10)
Kmean.fit(x_pca)
clusters = Kmean.predict(x_pca)
plot_clusters(x_pca, clusters, Kmean)</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/pca_clustered.png" alt="Reduction using PCA" class="figure mx-auto d-block"><div class="figcaption">Reduction using PCA</div>
</figure><p>And now we can compare how these clusters look against our actual
image labels by colour coding our first scatter plot:</p>
<pre><code><span><span class="fu">plot_clusters_labels</span><span class="op">(</span><span class="va">x_pca</span>, <span class="va">labels</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/pca_labelled.png" alt="Reduction using PCA" class="figure mx-auto d-block"><div class="figcaption">Reduction using PCA</div>
</figure><p>PCA has done a valiant effort to reduce the dimensionality of our
problem from 64D to 2D while still retaining some of our key structural
information. We can see that the digits
<code>0</code>,<code>1</code>,<code>4</code>, and <code>6</code> cluster
up reasonably well even using a simple k-means test. However it does
look like there is still quite a bit of overlap between the remaining
digits, especially for the digits <code>5</code> and <code>8</code>. The
clustering is from perfect in the largest “blob”, but not a bad effort
from PCA given the substantial dimensionality reduction.</p>
<p>It’s worth noting that PCA does not handle outlier data well
primarily due to global preservation of structural information, and so
we will now look at a more complex form of learning that we can apply to
this problem.</p>
</div>
<div class="section level3">
<h3 id="t-distributed-stochastic-neighbor-embedding-t-sne">t-distributed Stochastic Neighbor Embedding (t-SNE)<a class="anchor" aria-label="anchor" href="#t-distributed-stochastic-neighbor-embedding-t-sne"></a>
</h3>
<p>t-SNE is a powerful example of manifold learning - a
non-deterministic non-linear approach to dimensionality reduction. A
<strong>manifold</strong> is a way to think about complex,
high-dimensional data as if it exists on a simpler, lower-dimensional
shape within that space. Imagine a crumpled piece of paper: while it
exists in 3D space when crumpled, the surface of the paper itself is
inherently 2D. Similarly, in many datasets, the meaningful patterns and
relationships lie along such “lower-dimensional surfaces” within the
high-dimensional space. For example, in image data like MNIST, the raw
pixels (hundreds of dimensions) may seem high-dimensional, but the
actual structure (the shapes of digits) is much simpler, often following
a lower-dimensional manifold. Manifold learning techniques like t-SNE
aim to “uncrumple” the data and flatten it into a lower-dimensional
space, while preserving the relationships between points as much as
possible.</p>
</div>
<div class="section level3">
<h3 id="intuition-for-t-sne">Intuition for t-SNE<a class="anchor" aria-label="anchor" href="#intuition-for-t-sne"></a>
</h3>
<p>t-SNE (<strong>t-distributed Stochastic Neighbor Embedding</strong>)
is a method for visualizing high-dimensional data by mapping it into a
low-dimensional space, typically 2D or 3D, while emphasizing <em>local
relationships</em>. It focuses on keeping nearby points close together,
helping to reveal clusters or patterns that may be hidden in the
original high-dimensional space.</p>
<p><strong>An analogy</strong>: Imagine moving a group of friends from a
large, crowded park into a much smaller garden while trying to keep
people who are chatting with each other close. You won’t care much about
preserving the exact distances between groups from the original
park—your main goal is to keep friends near each other in the smaller
space. Similarly, t-SNE prioritizes preserving these <em>local
connections</em>, while global distances between clusters may be
distorted or not reflect their true relationships. This distortion
happens because t-SNE sacrifices global structure to accurately capture
local neighborhoods. For example: - Two clusters that appear far apart
in the t-SNE plot may actually be closer in the original
high-dimensional space. - Similarly, clusters that appear close together
in the plot might not actually be neighbors in the original space.</p>
<p>As a result, while t-SNE is excellent for discovering <em>local
patterns</em> (e.g., clusters, subgroups), you should be cautious about
interpreting the relative distances between clusters. These are less
reliable and are influenced by how the algorithm optimizes its layout in
the reduced space. It’s best to use t-SNE as a tool to find grouping and
then validate these findings using additional analysis.</p>
<p>For more in depth explanations of t-SNE and manifold learning please
see the following links which also contain som very nice visual examples
of manifold learning in action: * <a href="https://thedatafrog.com/en/articles/visualizing-datasets/" class="external-link">https://thedatafrog.com/en/articles/visualizing-datasets/</a>
* <a href="https://scikit-learn.org/stable/modules/manifold.html" class="external-link">https://scikit-learn.org/stable/modules/manifold.html</a></p>
<p>Scikit-Learn allows us to apply t-SNE in a relatively simple way.
Lets code and apply t-SNE to the MNIST dataset in the same manner that
we did for the PCA example, and reduce the data down from 64D to 2D
again:</p>
<pre><code># t-SNE embedding
from sklearn import manifold

# initialising with "pca" explicitly preserves global structure
tsne = manifold.TSNE(n_components=2, init='pca', random_state = 0)
x_tsne = tsne.fit_transform(features)

plots_labels(x_tsne, None)</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/tsne_unlabelled.png" alt="Reduction using PCA" class="figure mx-auto d-block"><div class="figcaption">Reduction using PCA</div>
</figure><p>It looks like t-SNE has done a much better job of splitting our data
up into clusters using only a 2D representation of the data. Once again,
let’s run a simple k-means clustering on this new 2D representation, and
compare with the actual color-labelled data:</p>
<pre><code><span><span class="va">Kmean</span> <span class="op">=</span> <span class="fu">skl_cluster.KMeans</span><span class="op">(</span>n_clusters<span class="op">=</span><span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">Kmean.fit</span><span class="op">(</span><span class="va">x_tsne</span><span class="op">)</span></span>
<span><span class="va">clusters</span> <span class="op">=</span> <span class="fu">Kmean.predict</span><span class="op">(</span><span class="va">x_tsne</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">plot_clusters</span><span class="op">(</span><span class="va">x_tsne</span>, <span class="va">clusters</span>, <span class="va">Kmean</span><span class="op">)</span></span>
<span><span class="fu">plot_clusters_labels</span><span class="op">(</span><span class="va">x_tsne</span>, <span class="va">labels</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<p><img src="fig/tsne_clustered.png" alt="Reduction using PCA" class="figure"><img src="fig/tsne_labelled.png" alt="Reduction using PCA" class="figure"></p>
<p>It looks like t-SNE has successfully separated out our digits into
accurate clusters using as little as a 2D representation and a simple
k-means clustering algorithm. It has worked so well that you can clearly
see several clusters which can be modelled, whereas for our PCA
representation we needed to rely heavily on the knowledge that we had 10
types of digits to cluster.</p>
<p>Additionally, if we had run k-means on all 64 dimensions this would
likely still be computing away, whereas we have already broken down our
dataset into accurate clusters, with only a handful of outliers and
potential misidentifications (remember, a good ML model isn’t a perfect
model!)</p>
<p>The major drawback of applying t-SNE to datasets is the large
computational requirement. Furthermore, hyper-parameter tuning of t-SNE
usually requires some trial and error to perfect.</p>
<p>Our example here is still a relatively simple example of 8x8 images
and not very typical of the modern problems that can now be solved in
the field of ML and DL. To account for even higher-order input data,
neural networks were developed to more accurately extract feature
information.</p>
<div id="exercise-working-in-three-dimensions" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="exercise-working-in-three-dimensions" class="callout-inner">
<h3 class="callout-title">Exercise: Working in three dimensions</h3>
<div class="callout-content">
<p>The above example has considered only two dimensions since humans can
visualize two dimensions very well. However, there can be cases where a
dataset requires more than two dimensions to be appropriately
decomposed. Modify the above programs to use three dimensions and create
appropriate plots. Do three dimensions allow one to better distinguish
between the digits?</p>
<div class="section level2">
<h2 id="solution-1">Solution<a class="anchor" aria-label="anchor" href="#solution-1"></a>
</h2>
<pre><code>from mpl_toolkits.mplot3d import Axes3D
# PCA
pca = decomposition.PCA(n_components=3)
pca.fit(features)
x_pca = pca.transform(features)
fig = plt.figure(1, figsize=(4, 4))
ax = fig.add_subplot(projection='3d')
ax.scatter(x_pca[:, 0], x_pca[:, 1], x_pca[:, 2], c=labels,
          cmap=plt.cm.nipy_spectral, s=9, lw=0)
plt.show()</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/pca_3d.svg" alt="Reduction to 3 components using pca" class="figure mx-auto d-block"><div class="figcaption">Reduction to 3 components using pca</div>
</figure><pre><code># t-SNE embedding
tsne = manifold.TSNE(n_components=3, init='pca',
        random_state = 0)
x_tsne = tsne.fit_transform(features)
fig = plt.figure(1, figsize=(4, 4))
ax = fig.add_subplot(projection='3d')
ax.scatter(x_tsne[:, 0], x_tsne[:, 1], x_tsne[:, 2], c=labels,
          cmap=plt.cm.nipy_spectral, s=9, lw=0)
plt.show()</code></pre>
<p>{: .language-python}</p>
<figure><img src="fig/tsne_3d.svg" alt="Reduction to 3 components using tsne" class="figure mx-auto d-block"><div class="figcaption">Reduction to 3 components using tsne</div>
</figure>
</div>
</div>
</div>
</div>
<div id="exercise-parameters" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="exercise-parameters" class="callout-inner">
<h3 class="callout-title">Exercise: Parameters</h3>
<div class="callout-content">
<p>Look up parameters that can be changed in PCA and t-SNE, and
experiment with these. How do they change your resulting plots? Might
the choice of parameters lead you to make different conclusions about
your data?</p>
</div>
</div>
</div>
<div id="exercise-other-algorithms" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="exercise-other-algorithms" class="callout-inner">
<h3 class="callout-title">Exercise: Other algorithms</h3>
<div class="callout-content">
<p>There are other algorithms that can be used for doing dimensionality
reduction (for example the Higher Order Singular Value Decomposition
(HOSVD)). Do an internet search for some of these and examine the
example data that they are used on. Are there cases where they do
poorly? What level of care might you need to use before applying such
methods for automation in critical scenarios? What about for interactive
data exploration?</p>
</div>
</div>
</div>
<p>{% include links.md %}</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>PCA is a linear dimensionality reduction technique for tabular
data</li>
<li>t-SNE is another dimensionality reduction technique for tabular data
that is more general than PCA</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div></section><section id="aio-07-neural-networks"><p>Content from <a href="07-neural-networks.html">Neural Networks</a></p>
<hr>
<p>Last updated on 2025-11-07 |

        <a href="https://github.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/edit/main/episodes/07-neural-networks.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are Neural Networks?</li>
<li>How can we classify images using a neural network?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the basic architecture of a perceptron.</li>
<li>Be able to create a perceptron to encode a simple function.</li>
<li>Understand that layers of perceptrons allow non-linear separable
problems to be solved.</li>
<li>Train a multi-layer perceptron using Scikit-Learn.</li>
<li>Evaluate the accuracy of a multi-layer perceptron using real input
data.</li>
<li>Understand that cross validation allows the entire data set to be
used in the training process.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="neural-networks">Neural networks<a class="anchor" aria-label="anchor" href="#neural-networks"></a>
</h1>
<p>Neural networks are a machine learning method inspired by how the
human brain works. They are particularly good at pattern recognition and
classification tasks, often using images as inputs. They are a
well-established machine learning technique, having been around since
the 1950s, but they’ve gone through several iterations to overcome
limitations in previous generations. Using state-of-the-art neural
networks is often referred to as ‘deep learning’.</p>
<div class="section level2">
<h2 id="perceptrons">Perceptrons<a class="anchor" aria-label="anchor" href="#perceptrons"></a>
</h2>
<p>Perceptrons are the building blocks of neural networks. They are an
artificial version of a single neuron in the brain. They typically have
one or more inputs and a single output. Each input will be multiplied by
a weight and the value of all the weighted inputs are then summed
together. Finally, the summed value is put through an activation
function which decides if the neuron “fires” a signal. In some cases,
this activation function is simply a threshold step function which
outputs zero below a certain input and one above it. Other designs of
neurons use other activation functions, but typically they have an
output between zero and one and are still step-like in their nature.</p>
<figure><img src="fig/perceptron.svg" alt="A diagram of a perceptron" class="figure mx-auto d-block"><div class="figcaption">A diagram of a perceptron</div>
</figure><div class="section level3">
<h3 id="coding-a-perceptron">Coding a perceptron<a class="anchor" aria-label="anchor" href="#coding-a-perceptron"></a>
</h3>
<p>Below is an example of a perceptron written as a Python function. The
function takes three parameters: <code>Inputs</code> is a list of input
values, <code>Weights</code> is a list of weight values and
<code>Threshold</code> is the activation threshold.</p>
<p>First we multiply each input by the corresponding weight. To do this
quickly and concisely, we will use the numpy multiply function which can
multiply each item in a list by a corresponding item in another
list.</p>
<p>We then take the sum of all the inputs multiplied by their weights.
Finally, if this value is less than the activation threshold, we output
zero, otherwise we output a one.</p>
<pre><code>import numpy as np
def perceptron(inputs, weights, threshold):

    assert len(inputs) == len(weights)

    # multiply the inputs and weights
    values = np.multiply(inputs,weights)

    # sum the results
    total = sum(values)

    # decide if we should activate the perceptron
    if total &lt; threshold:
        return 0
    else:
        return 1</code></pre>
<p>{: .language-python}</p>
</div>
<div class="section level3">
<h3 id="computing-with-a-perceptron">Computing with a perceptron<a class="anchor" aria-label="anchor" href="#computing-with-a-perceptron"></a>
</h3>
<p>A single perceptron can perform basic linear classification problems
such as computing the logical AND, OR, and NOT functions.</p>
<p>OR</p>
<table class="table">
<thead><tr class="header">
<th>Input 1</th>
<th>Input 2</th>
<th>Output</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>AND</p>
<table class="table">
<thead><tr class="header">
<th>Input 1</th>
<th>Input 2</th>
<th>Output</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>NOT</p>
<table class="table">
<thead><tr class="header">
<th>Input 1</th>
<th>Output</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>We can get a single perceptron to compute each of these functions</p>
<p>OR: ~~~ inputs = [[0.0,0.0],[1.0,0.0],[0.0,1.0],[1.0,1.0]] for input
in inputs: print(input,perceptron(input, [0.5,0.5], 0.5)) ~~~ {:
.language-python}</p>
<p>AND: ~~~ inputs = [[0.0,0.0],[1.0,0.0],[0.0,1.0],[1.0,1.0]] for input
in inputs: print(input,perceptron(input, [0.5,0.5], 1.0)) ~~~ {:
.language-python}</p>
<p>NOT:</p>
<p>The NOT function only has a single input. To make it work in the
perceptron we need to introduce a bias term which is always the same
value. In this example it is the second input. It has a weight of 1.0
while the weight on the real input is -1.0. ~~~ inputs =
[[0.0,1.0],[1.0,1.0]] for input in inputs: print(input,perceptron(input,
[-1.0,1.0], 1.0)) ~~~ {: .language-python}</p>
<p>A perceptron can be trained to compute any function which has linear
separability. A simple training algorithm called the perceptron learning
algorithm can be used to do this and Scikit-Learn has its own
implementation of it. We are going to skip over the perceptron learning
algorithm and move straight onto more powerful techniques. If you want
to learn more about it see <a href="https://computing.dcu.ie/~humphrys/Notes/Neural/single.neural.html" class="external-link">this
page</a> from Dublin City University.</p>
</div>
<div class="section level3">
<h3 id="perceptron-limitations">Perceptron limitations<a class="anchor" aria-label="anchor" href="#perceptron-limitations"></a>
</h3>
<p>A single perceptron cannot solve any function that is not linearly
separable, meaning that we need to be able to divide the classes of
inputs and outputs with a straight line. A common example of this is the
XOR function shown below:</p>
<table class="table">
<thead><tr class="header">
<th>Input 1</th>
<th>Input 2</th>
<th>Output</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>(Make a graph of this)</p>
<p>This function outputs a zero when all its inputs are one or zero and
its not possible to separate with a straight line. This is known as
linear separability. When this limitation was discovered in the 1960s it
effectively halted development of neural networks for over a decade in a
period known as the “AI Winter”.</p>
</div>
</div>
<div class="section level2">
<h2 id="multi-layer-perceptrons">Multi-layer perceptrons<a class="anchor" aria-label="anchor" href="#multi-layer-perceptrons"></a>
</h2>
<p>A single perceptron cannot be used to solve a non-linearly separable
function. For that, we need to use multiple perceptrons and typically
multiple layers of perceptrons. They are formed of networks of
artificial neurons which each take one or more inputs and typically have
a single output. The neurons are connected together in networks of 10s
to 1000s of neurons. Typically, networks are connected in layers with an
input layer, middle or hidden layer (or layers), and finally an output
layer.</p>
<figure><img src="fig/multilayer_perceptron.svg" alt="A multi-layer perceptron" class="figure mx-auto d-block"><div class="figcaption">A multi-layer perceptron</div>
</figure><div class="section level3">
<h3 id="training-multi-layer-perceptrons">Training multi-layer perceptrons<a class="anchor" aria-label="anchor" href="#training-multi-layer-perceptrons"></a>
</h3>
<p>Multi-layer perceptrons need to be trained by showing them a set of
training data and measuring the error between the network’s predicted
output and the true value. Training takes an iterative approach that
improves the network a little each time a new training example is
presented. There are a number of training algorithms available for a
neural network today, but we are going to use one of the best
established and well known, the backpropagation algorithm. This
algorithm is called back propagation because it takes the error
calculated between an output of the network and the true value and takes
it back through the network to update the weights. If you want to read
more about back propagation, please see <a href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf" class="external-link">this
chapter</a> from the book “Neural Networks - A Systematic
Introduction”.</p>
</div>
<div class="section level3">
<h3 id="multi-layer-perceptrons-in-scikit-learn">Multi-layer perceptrons in Scikit-Learn<a class="anchor" aria-label="anchor" href="#multi-layer-perceptrons-in-scikit-learn"></a>
</h3>
<p>We are going to build a multi-layer perceptron for recognising
handwriting from images. Scikit-Learn includes some example handwriting
data from the <a href="http://yann.lecun.com/exdb/mnist/" class="external-link">MNIST data
set</a>, which is a dataset containing 70,000 images of hand-written
digits. Each image is 28x28 pixels in size (784 pixels in total) and is
represented in grayscale with values between zero for fully black and
255 for fully white. This means we will need 784 perceptrons in our
input layer, each taking the input of one pixel and 10 perceptrons in
our output layer to represent each digit we might classify. If trained
correctly, only the perceptron in the output layer will “fire” to
represent the contents of the image (but this is a massive
oversimplification!).</p>
<p>We can import this dataset from <code>sklearn.datasets</code> then
load it into memory by calling the <code>fetch_openml</code>
function.</p>
<pre><code>import sklearn.datasets as skl_data
data, labels = skl_data.fetch_openml('mnist_784', version=1, return_X_y=True)</code></pre>
<p>{: .language-python}</p>
<p>This creates two arrays of data, one called <code>data</code> which
contains the image data and the other <code>labels</code> that contains
the labels for those images which will tell us which digit is in the
image. A common convention is to call the data <code>X</code> and the
labels <code>y</code>.</p>
<p>As neural networks typically want to work with data that ranges
between 0 and 1.0 we need to normalise our data to this range. Python
has a shortcut which lets us divide the entire data array by 255 and
store the result, we can simply do:</p>
<pre><code><span><span class="va">data</span> <span class="op">=</span> <span class="va">data</span> <span class="op">/</span> <span class="fl">255.0</span></span></code></pre>
<p>{: .language-python}</p>
<p>This is instead of writing a loop ourselves to divide every pixel by
255. Although the final result is the same and will take about the same
amount of computation (possibly a little less, it might do some clever
optimisations).</p>
<p>Now we need to initialise a neural network. Scikit-Learn has an
entire library for this (<code>sklearn.neural_network</code>) and the
<code>MLPClassifier</code> class handles multi-layer perceptrons. This
network takes a few parameters including the size of the hidden layer,
the maximum number of training iterations we’re going to allow, the
exact algorithm to use, whether or not we’d like verbose output about
what the training is doing, and the initial state of the random number
generator.</p>
<p>In scikit-learn’s <code>MLPClassifier</code>, the
<code>hidden_layer_sizes</code> parameter specifies the number and size
of hidden layers in the neural network. For example,
<code>hidden_layer_sizes=(50,)</code> creates a single hidden layer with
50 neurons, while <code>(100, 50)</code> creates two hidden layers with
100 and 50 neurons, respectively. It’s important to include the trailing
comma for a single hidden layer (e.g., <code>(50,)</code>) because
without it, <code>(50)</code> would be interpreted as an integer, not a
tuple, and cause an error. The example,
<code>MLPClassifier(hidden_layer_sizes=(50,), max_iter=50, verbose=1, random_state=1)</code>,
builds a neural network with one hidden layer containing 50 neurons,
runs for a maximum of 50 iterations, logs training progress, and ensures
reproducibility with <code>random_state=1</code>.</p>
<p>The max_iter parameter in MLPClassifier specifies the maximum number
of iterations, not epochs. Since MLPClassifier uses stochastic gradient
descent (or its variants), each iteration processes a small random
subset of the data (a batch), and the full dataset may not be seen in a
single iteration.</p>
<pre><code>import sklearn.neural_network as skl_nn
mlp = skl_nn.MLPClassifier(hidden_layer_sizes=(50,), max_iter=50, verbose=1, random_state=1)</code></pre>
<p>{: .language-python}</p>
<p>We now have a neural network but we have not trained it yet. Before
training, we will split our dataset into two parts: a training set which
we will use to train the classifier and a test set which we will use to
see how well the training is working. By using different data for the
two, we can avoid ‘over-fitting’, which is the creation of models which
do not “generalise” or work with data other than their training
data.</p>
<p>Typically, the majority of the data will be used as training data
(70-90%), to help avoid overfitting. Let us see how big our dataset is
to decide how many samples we want to train with.</p>
<pre><code><span><span class="va">data.shape</span></span></code></pre>
<p>{: .language-python}</p>
<p>This tells us we have 70,000 rows in the dataset.</p>
<pre><code>&lt;bound method NDFrame.describe of        pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  pixel781  pixel782  pixel783  pixel784
0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0
1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0
2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0
3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0
4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0
...       ...     ...     ...     ...     ...     ...     ...     ...     ...      ...  ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...
69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0
69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0
69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0
69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0
69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0

[70000 rows x 784 columns]&gt;</code></pre>
<p>{: .output}</p>
<p>Let us take 90% of the data for training and 10% for testing, so we
will use the first 63,000 samples in the dataset as the training data
and the last 7,000 as the test data.</p>
<pre><code>from sklearn.model_selection import train_test_split

# Assuming `data` is your feature matrix and `labels` is your target vector
X_train, X_test, y_train, y_test = train_test_split(
    data.values,        # Features
    labels.values,      # Labels
    test_size=0.1,      # Reserve 10% of data for testing
    random_state=42     # For reproducibility
)
X_train.shape</code></pre>
<p>{: .language-python}</p>
<p>Now lets train the network. This line will take about one minute to
run. We do this by calling the <code>fit</code> function inside the
<code>mlp</code> class instance. This needs two arguments: the data
itself, and the labels showing what class each item should be classified
to.</p>
<pre><code><span><span class="fu">mlp.fit</span><span class="op">(</span><span class="va">X_train</span>, <span class="va">y_train</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<p>Finally, we will score the accuracy of our network against both the
original training data and the test data. If the training had converged
to the point where each iteration of training was not improving the
accuracy, then the accuracy of the training data should be 1.0
(100%).</p>
<pre><code><span><span class="fu">print</span><span class="op">(</span><span class="st">"Training set score"</span>, <span class="fu">mlp.score</span><span class="op">(</span><span class="va">X_train</span>, <span class="va">y_train</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="st">"Testing set score"</span>, <span class="fu">mlp.score</span><span class="op">(</span><span class="va">X_test</span>, <span class="va">y_test</span><span class="op">)</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
</div>
<div class="section level3">
<h3 id="prediction-using-a-multi-layer-perceptron">Prediction using a multi-layer perceptron<a class="anchor" aria-label="anchor" href="#prediction-using-a-multi-layer-perceptron"></a>
</h3>
<p>Now that we have trained a multi-layer perceptron, we can give it
some input data and ask it to perform a prediction. In this case, our
input data is a 28x28 pixel image, which can also be represented as a
784-element list of data. The output will be a number between 0 and 9
telling us which digit the network thinks we have supplied. The
<code>predict</code> function in the <code>MLPClassifier</code> class
can be used to make a prediction. Lets use the first digit from our test
set as an example.</p>
<p>Before we can pass it to the predictor, we need to extract one of the
digits from the test set. We can use <code>iloc</code> on the dataframe
to get hold of the first element in the test set. In order to present it
to the predictor, we have to turn it into a numpy array which has the
dimensions of 1x784 instead of 28x28. We can then call the
<code>predict</code> function with this array as our parameter. This
will return an array of predictions (as it could have been given
multiple inputs), the first element of this will be the predicted digit.
You may get a warning stating “X does not have valid feature names”,
this is because we didn’t encode feature names into our X (digit images)
data.</p>
<pre><code>test_digit = X_test[0].reshape(1,784) # current shape is (784,)
test_digit_prediction = mlp.predict(test_digit)[0]
print("Predicted value",test_digit_prediction)</code></pre>
<p>{: .language-python}</p>
<p>We can now verify if the prediction is correct by looking at the
corresponding item in the <code>labels_test</code> array.</p>
<pre><code><span><span class="fu">print</span><span class="op">(</span><span class="st">"Actual value"</span>,<span class="va">y_test</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<p>This should be the same value which is being predicted.</p>
<div id="changing-the-learning-parameters" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="changing-the-learning-parameters" class="callout-inner">
<h3 class="callout-title">Changing the learning parameters</h3>
<div class="callout-content">
<p>There are several parameters which control the training of the data.
One of these is called the learning rate. Increasing this can reduce how
many learning iterations we need. But if this is too large you can end
up overshooting. Try tweaking this parameter by adding the parameter
<code>learning_rate_init</code> with a default value of 0.001. Try
increasing it to around 0.1.</p>
</div>
</div>
</div>
<div id="using-your-own-handwriting" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="using-your-own-handwriting" class="callout-inner">
<h3 class="callout-title">Using your own handwriting</h3>
<div class="callout-content">
<p>Create an image using Microsoft Paint, the GNU Image Manipulation
Project (GIMP) or <a href="https://jspaint.app/" class="external-link">jspaint</a>. The image
needs to be grayscale and 28 x 28 pixels.</p>
<p>Try to draw a digit (0-9) in the image and save it into your code
directory.</p>
<p>The code below loads the image (called digit.png, change to whatever
your file is called) using the OpenCV library. Some Anaconda
installations need this installed either through the package manager or
by running the command: <code>conda install -c conda-forge opencv</code>
from the anaconda terminal.</p>
<p>OpenCV assumes that images are 3 channel red, green, blue and we have
to convert to one channel grayscale with <code>cvtColor</code>.</p>
<p>We also need to normalise the image by dividing each pixel by
255.</p>
<p>To verify the image, we can plot it by using OpenCV’s
<code>imshow</code> function (we could also use Matplotlib’s
<code>matshow</code> function).</p>
<p>To check what digit it is, we can pass it into
<code>mlp.predict</code>, but we have to convert it from a 28x28 array
to a one dimensional 784-byte long array with the <code>reshape</code>
function.</p>
<p>Did it correctly classify your hand(mouse) writing? Try a few images.
If you have time try drawing images on a touch screen or taking a photo
of something you have really written by hand. Remember that you will
have to resize it to be 28x28 pixels. ~~~ import cv2 import
matplotlib.pyplot as plt digit = cv2.imread(“digit.png”) digit_gray =
cv2.cvtColor(digit, cv2.COLOR_BGR2GRAY) digit_norm = digit_gray/255.0
cv2.imshow(“Normalised Digit”,digit_norm) print(“Your digit
is”,mlp.predict(digit_norm.reshape(1,784))) ~~~ {: .language-python}</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="confusion-matrix">Confusion matrix<a class="anchor" aria-label="anchor" href="#confusion-matrix"></a>
</h3>
<p>We now know what percentage of images were correctly classified, but
we don’t know anything about the distribution of correct predictions
across our different classes (the digits 0 to 9 in this case). A more
powerful technique is known as a confusion matrix. Here we draw a grid
with each class along both the x and y axis. The x axis is the actual
number of items in each class and the y axis is the predicted number. In
a perfect classifier, there will be a diagonal line of values across the
grid moving from the top left to bottom right corresponding to the
number in each class, and all other cells will be zero. If any cell
outside of the diagonal is non-zero then it indicates a
miss-classification. Scikit-Learn has a function called
<code>confusion_matrix</code> in the <code>sklearn.metrics</code> class
which can display a confusion matrix for us. It will need two inputs:
arrays showing how many items were in each class for both the real data
and the classifications. We already have the real data in the
labels_test array, but we need to build it for the classifications by
classifying each image (in the same order as the real data) and storing
the result in another array.</p>
<pre><code><span><span class="co"># extract all test set predictions</span></span>
<span><span class="va">y_test_pred</span> <span class="op">=</span> <span class="fu">mlp.predict</span><span class="op">(</span><span class="va">X_test</span><span class="op">)</span></span>
<span><span class="va">y_test_pred</span></span></code></pre>
<p>{: .language-python}</p>
<p>The <code>ConfusionMatrixDisplay</code> class in the
<code>sklearn.metrics</code> package can create a graphical
representation of a confusion matrix with colour coding to highlight how
many items are in each cell. This colour coding can be useful when
working with very large numbers of classes. ~~~ import numpy as np from
sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_predictions(y_test,y_test_pred) ~~~ {:
.language-python}</p>
</div>
</div>
<div class="section level2">
<h2 id="cross-validation">Cross-validation<a class="anchor" aria-label="anchor" href="#cross-validation"></a>
</h2>
<p>Previously we split the data into training and test sets. But what if
the test set includes important features we want to train on that happen
to be missing in the training set? We are throwing away part of our data
to use it in the testing set.</p>
<p>Cross-validation runs the training/testing multiple times but splits
the data in a different way each time. This means all of the data gets
used both for training and testing. We can use multiple iterations of
training with different data in each set to eventually include the
entire dataset.</p>
<p>example list</p>
<p>[1,2,3,4,5,6,7,8]</p>
<p>train = 1,2,3,4,5,6 test = 7,8</p>
<p>train = 1,2,3,4,7,8 test = 5,6</p>
<p>train = 1,2,5,6,7,8 test = 3,4</p>
<p>train = 3,4,5,6,7,8 test = 1,2</p>
<p>(generate an image of this)</p>
<div class="section level3">
<h3 id="cross-validation-code-example">Cross-validation code example<a class="anchor" aria-label="anchor" href="#cross-validation-code-example"></a>
</h3>
<p>The <code>sklearn.model_selection</code> module provides support for
doing k-fold cross validation in Scikit-Learn. It can automatically
partition our data for cross validation.</p>
<p>Import this and call it <code>skl_msel</code></p>
<pre><code>import sklearn.model_selection as skl_msel</code></pre>
<p>{: .language-python}</p>
<p>Now we can choose how many ways we would like to split our data
(three or four are common choices).</p>
<pre><code><span><span class="va">kfold</span> <span class="op">=</span> <span class="fu">skl_msel.KFold</span><span class="op">(</span><span class="fl">4</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<p>Now we can loop through our data and test on each combination. The
<code>kfold.split</code> function returns two variables and we will have
our for loop work through both of them. The train variable will contain
a list of which items (by index number) we are currently using to train
and the test one will contain the list of which items we are going to
test on.</p>
<pre><code>for (train, test) in kfold.split(data):</code></pre>
<p>{: .language-python}</p>
<p>Now inside the loop, we can select the data with
<code>data_train = data.iloc[train]</code> and
<code>labels_train = labels.iloc[train]</code>. In some versions of
Python/Pandas/Scikit-Learn, you might be able to use
<code>data_train = data[train]</code> and
<code>labels_train = labels[train]</code>. This is a useful Python
shorthand which will use the list of indices from <code>train</code> to
select which items from <code>data</code> and <code>labels</code> we
use. We can repeat this process with the test set.</p>
<pre><code><span>    <span class="va">data_train</span> <span class="op">=</span> <span class="va">data.iloc</span><span class="op">[</span><span class="va">train</span><span class="op">]</span></span>
<span>    <span class="va">labels_train</span> <span class="op">=</span> <span class="va">labels.iloc</span><span class="op">[</span><span class="va">train</span><span class="op">]</span></span>
<span></span>
<span>    <span class="va">data_test</span> <span class="op">=</span> <span class="va">data.iloc</span><span class="op">[</span><span class="va">test</span><span class="op">]</span></span>
<span>    <span class="va">labels_test</span> <span class="op">=</span> <span class="va">labels.iloc</span><span class="op">[</span><span class="va">test</span><span class="op">]</span></span></code></pre>
<p>{: .language-python}</p>
<p>Finally, we need to train the classifier with the selected training
data and then score it against the test data. The scores for each set of
test data should be similar.</p>
<pre><code><span>    <span class="fu">mlp.fit</span><span class="op">(</span><span class="va">data_train</span>,<span class="va">labels_train</span><span class="op">)</span></span>
<span>    <span class="fu">print</span><span class="op">(</span><span class="st">"Testing set score"</span>, <span class="fu">mlp.score</span><span class="op">(</span><span class="va">data_test</span>, <span class="va">labels_test</span><span class="op">)</span><span class="op">)</span></span></code></pre>
<p>{: .language-python}</p>
<p>Once we have established that the cross validation was ok, we can go
ahead and train using the entire dataset by doing
<code>mlp.fit(data,labels)</code>.</p>
<p>Here is the entire example program:</p>
<pre><code>import matplotlib.pyplot as plt
import sklearn.datasets as skl_data
import sklearn.neural_network as skl_nn
import sklearn.model_selection as skl_msel

data, labels = skl_data.fetch_openml('mnist_784', version=1, return_X_y=True)
data = data / 255.0

mlp = skl_nn.MLPClassifier(hidden_layer_sizes=(50,), max_iter=50, random_state=1)

kfold = skl_msel.KFold(4)

for (train, test) in kfold.split(data):
    data_train = data.iloc[train]
    labels_train = labels.iloc[train]

    data_test = data.iloc[test]
    labels_test = labels.iloc[test]
    mlp.fit(data_train,labels_train)
    print("Training set score", mlp.score(data_train, labels_train))
    print("Testing set score", mlp.score(data_test, labels_test))
mlp.fit(data,labels)</code></pre>
<p>{: .language-python}</p>
</div>
</div>
<div class="section level2">
<h2 id="deep-learning">Deep learning<a class="anchor" aria-label="anchor" href="#deep-learning"></a>
</h2>
<p>Deep learning usually refers to newer neural network architectures
which use a special type of network known as a ‘convolutional network’.
Typically, these have many layers and thousands of neurons. They are
very good at tasks such as image recognition but take a long time to
train and run. They are often used with GPUs (Graphical Processing
Units) which are good at executing multiple operations simultaneously.
It is very common to use cloud computing or high performance computing
systems with multiple GPUs attached.</p>
<p>Scikit-Learn is not really setup for deep learning. We will have to
rely on other libraries. Common choices include Google’s TensorFlow,
Keras, (Py)Torch or Darknet. There is, however, an interface layer
between sklearn and tensorflow called skflow. A short example of using
this layer can be found at <a href="https://www.kdnuggets.com/2016/02/scikit-flow-easy-deep-learning-tensorflow-scikit-learn.html" class="external-link">https://www.kdnuggets.com/2016/02/scikit-flow-easy-deep-learning-tensorflow-scikit-learn.html</a>.</p>
<div class="section level3">
<h3 id="cloud-apis">Cloud APIs<a class="anchor" aria-label="anchor" href="#cloud-apis"></a>
</h3>
<p>Google, Microsoft, Amazon, and many other companys now have cloud
based Application Programming Interfaces (APIs) where you can upload an
image and have them return you the result. Most of these services rely
on a large pre-trained (and often proprietary) neural network.</p>
<div id="exercise-try-cloud-image-classification" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="exercise-try-cloud-image-classification" class="callout-inner">
<h3 class="callout-title">Exercise: Try cloud image classification</h3>
<div class="callout-content">
<p>Take a photo with your phone camera or find an image online of a
common daily scene. Upload it to Google’s Vision AI at <a href="https://cloud.google.com/vision/" class="external-link uri">https://cloud.google.com/vision/</a> How many objects has it
correctly classified? How many did it incorrectly classify? Try the same
image with Microsoft’s Computer Vision API at <a href="https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/" class="external-link uri">https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/</a>
Does it do any better/worse than Google?</p>
</div>
</div>
</div>
<p>{% include links.md %}</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Perceptrons are artificial neurons which build neural networks.</li>
<li>A perceptron takes multiple inputs, multiplies each by a weight
value and sums the weighted inputs. It then applies an activation
function to the sum.</li>
<li>A single perceptron can solve simple functions which are linearly
separable.</li>
<li>Multiple perceptrons can be combined to form a neural network which
can solve functions that aren’t linearly separable.</li>
<li>We can train a whole neural network with the back propagation
algorithm. Scikit-learn includes an implementation of this
algorithm.</li>
<li>Training a neural network requires some training data to show the
network examples of what to learn.</li>
<li>To validate our training we split the training data into a training
set and a test set.</li>
<li>To ensure the whole dataset can be used in training and testing we
can train multiple times with different subsets of the data acting as
training/testing data. This is called cross validation.</li>
<li>Deep learning neural networks are a very powerful modern machine
learning technique. Scikit-Learn does not support these but other
libraries like Tensorflow do.</li>
<li>Several companies now offer cloud APIs where we can train neural
networks on powerful computers.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div>
</div></section><section id="aio-08-ethics"><p>Content from <a href="08-ethics.html">Ethics and the Implications of Machine Learning</a></p>
<hr>
<p>Last updated on 2025-11-07 |

        <a href="https://github.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/edit/main/episodes/08-ethics.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are the ethical implications of using machine learning in
research?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Consider the ethical implications of machine learning, in general,
and in research.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="ethics-and-machine-learning">Ethics and machine learning<a class="anchor" aria-label="anchor" href="#ethics-and-machine-learning"></a>
</h1>
<p>As machine learning has risen in visibility, so to have concerns
around the ethics of using the technology to make predictions and
decisions that will affect people in everyday life. For example:</p>
<ul>
<li>The first death from a driverless car which failed to brake for a
pedestrian.<a href="https://www.forbes.com/sites/meriameberboucha/2018/05/28/uber-self-driving-car-crash-what-really-happened/" class="external-link"><span class="math display">\[1\]</span></a>
</li>
<li>Highly targetted advertising based around social media and internet
usage. <a href="https://www.wired.com/story/big-tech-can-use-ai-to-extract-many-more-ad-dollars-from-our-clicks/" class="external-link"><span class="math display">\[2\]</span></a>
</li>
<li>The outcomes of elections and referenda being influenced by highly
targetted social media posts. This is compounded by data being obtained
without the user’s consent. <a href="https://www.vox.com/policy-and-politics/2018/3/23/17151916/facebook-cambridge-analytica-trump-diagram" class="external-link"><span class="math display">\[3\]</span></a>
</li>
<li>The widespread use of facial recognition technologies. <a href="https://www.bbc.co.uk/news/technology-44089161" class="external-link"><span class="math display">\[4\]</span></a>
</li>
<li>The potential for autonomous military robots to be deployed in
combat. <a href="https://www.theverge.com/2021/6/3/22462840/killer-robot-autonomous-drone-attack-libya-un-report-context" class="external-link"><span class="math display">\[5\]</span></a>
</li>
</ul>
<div class="section level2">
<h2 id="problems-with-bias">Problems with bias<a class="anchor" aria-label="anchor" href="#problems-with-bias"></a>
</h2>
<p>Machine learning systems are often argued to be be fairer and more
impartial in their decision-making than human beings, who are argued to
be more emotional and biased, for example, when sentencing criminals or
deciding if someone should be granted bail. But there are an increasing
number of examples where machine learning systems have been exposed as
biased due to the data they were trained on. This can occur due to the
training data being unrepresentative or just under representing certain
cases or groups. For example, if you were trying to automatically screen
job candidates and your training data consisted only of people who were
previously hired by the company, then any biases in employment processes
would be reflected in the results of the machine learning.</p>
</div>
<div class="section level2">
<h2 id="problems-with-explaining-decisions">Problems with explaining decisions<a class="anchor" aria-label="anchor" href="#problems-with-explaining-decisions"></a>
</h2>
<p>Many machine learning systems (e.g. neural networks) can’t really
explain their decisions. Although the input and output are known, trying
to explain why the training caused the network to behave in a certain
way can be very difficult. When decisions are questioned by a human it’s
difficult to provide any rationale for how a decision was arrived
at.</p>
</div>
<div class="section level2">
<h2 id="problems-with-accuracy">Problems with accuracy<a class="anchor" aria-label="anchor" href="#problems-with-accuracy"></a>
</h2>
<p>No machine learning system is ever 100% accurate. Getting into the
high 90s is usually considered good. But when we’re evaluating millions
of data items this can translate into 100s of thousands of
mis-identifications. This would be an unacceptable margin of error if
the results were going to have major implications for people, such as
criminal sentencing decisions or structuring debt repayments.</p>
</div>
<div class="section level2">
<h2 id="energy-use">Energy use<a class="anchor" aria-label="anchor" href="#energy-use"></a>
</h2>
<p>Many machine learning systems (especially deep learning) need vast
amounts of computational power which in turn can consume vast amounts of
energy. Depending on the source of that energy this might account for
significant amounts of fossil fuels being burned. It is not uncommon for
a modern GPU-accelerated computer to use several kilowatts of power.
Running this system for one hour could easily use as much energy a
typical home in the OECD would use in an entire day. Energy use can be
particularly high when models are constantly being retrained or when
“parameter sweeps” are done to find the best set of parameters to train
with.</p>
</div>
</div>
<div class="section level1">
<h1 id="ethics-of-machine-learning-in-research">Ethics of machine learning in research<a class="anchor" aria-label="anchor" href="#ethics-of-machine-learning-in-research"></a>
</h1>
<p>Not all research using machine learning will have major ethical
implications. Many research projects don’t directly affect the lives of
other people, but this isn’t always the case.</p>
<p>Some questions you might want to ask yourself (and which an ethics
committee might also ask you):</p>
<ul>
<li>Will the results of your machine learning influence a decision that
will have a significant effect on a person’s life?</li>
<li>Will the results of your machine learning influence a decision that
will have a significant effect on an animial’s life?</li>
<li>Will you be using any people to create your training data, and if
so, will they have to look at any disturbing or traumatic material
during the training process?</li>
<li>Are there any inherent biases in the dataset(s) you’re using for
training?</li>
<li>How much energy will this computation use? Are there more efficient
ways to get the same answer?</li>
</ul>
<div id="exercise-ethical-implications-of-your-own-research" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="exercise-ethical-implications-of-your-own-research" class="callout-inner">
<h3 class="callout-title">Exercise: Ethical implications of your own
research</h3>
<div class="callout-content">
<p>Split into pairs or groups of three. Think of a use case for machine
learning in your research areas. What ethical implications (if any)
might there be from using machine learning in your research? Write down
your group’s answers in the etherpad.</p>
</div>
</div>
</div>
<p>{% include links.md %}</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>The results of machine learning reflect biases in the training and
input data.</li>
<li>Many machine learning algorithms can’t explain how they arrived at a
decision.</li>
<li>Machine learning can be used for unethical purposes.</li>
<li>Consider the implications of false positives and false
negatives.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div></section><section id="aio-09-learn-more"><p>Content from <a href="09-learn-more.html">Find out more</a></p>
<hr>
<p>Last updated on 2025-11-07 |

        <a href="https://github.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/edit/main/episodes/09-learn-more.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>Where can you find out more about machine learning?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Know where to go to learn more about machine learning</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="other-algorithms">Other algorithms<a class="anchor" aria-label="anchor" href="#other-algorithms"></a>
</h1>
<p>There are many other machine learning algorithms that might be
suitable for helping you to answer your research questions.</p>
<p>The Scikit-Learn <a href="https://scikit-learn.org/stable/index.html" class="external-link">webpage</a> has a good
overview of all the features available in the library.</p>
<div class="section level2">
<h2 id="genetic-algorithms">Genetic algorithms<a class="anchor" aria-label="anchor" href="#genetic-algorithms"></a>
</h2>
<p>Genetic algorithms are techniques which try to mimic biological
evolution. They will learn to solve a problem through a gradual process
of simulated evolution. Each generation is mutated slightly and then
evaluated with a fitness function. The fittest “genes” will then be
selected for the next generation. Sometimes this is combined with neural
networks to change the networks size structure.</p>
<p>This <a href="https://www.youtube.com/watch?v=qv6UVOQ0F44" class="external-link">video</a>
shows a genetic algorithm evolving neural networks to play a video
game.</p>
</div>
<div class="section level2">
<h2 id="useful-resources">Useful Resources<a class="anchor" aria-label="anchor" href="#useful-resources"></a>
</h2>
<ul>
<li><p><a href="https://vas3k.com/blog/machine_learning/" class="external-link">Machine
Learning for Everyone</a> - A useful overview of many different machine
learning techniques, all introduced in an easy to follow way.</p></li>
<li><p><a href="https://developers.google.com/machine-learning/crash-course/" class="external-link">Google
machine learning crash course</a> - A quick course from Google on how to
use some of their machine learning products.</p></li>
<li><p><a href="https://research.fb.com/the-facebook-field-guide-to-machine-learning-video-series/" class="external-link">Facebook
Field Guide to Machine Learning</a> - A good introduction to machine
learning concepts from Facebook.</p></li>
<li><p><a href="https://docs.aws.amazon.com/machine-learning/latest/dg/amazon-machine-learning-key-concepts.html" class="external-link">Amazon
Machine Learning guide</a> - An introduction to the key concepts in
machine learning from Amazon.</p></li>
<li><p><a href="https://azure.microsoft.com/en-gb/overview/ai-platform/" class="external-link">Azure
AI</a> - Microsoft’s Cloud based AI platform.</p></li>
</ul>
<p>{% include links.md %}</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>This course has only touched on a few areas of machine learning and
is designed to teach you just enough to do something useful.</li>
<li>Machine learning is a rapidly evolving field and new tools and
techniques are constantly appearing.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/" class="external-link">Source</a></p>
				<p><a href="https://github.com/UW-Madison-DataScience/machine-learning-novice-sklearn-v2/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:endemann@wisc.edu">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://UW-Madison-DataScience.github.io/machine-learning-novice-sklearn-v2/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://UW-Madison-DataScience.github.io/machine-learning-novice-sklearn-v2/aio.html",
  "identifier": "https://UW-Madison-DataScience.github.io/machine-learning-novice-sklearn-v2/aio.html",
  "dateCreated": "2013-07-13",
  "dateModified": "2025-11-07",
  "datePublished": "2025-11-07"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

